{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3088ffc3104424fb8fd5365b285c76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a67174e6ec9410f9cb1047cad3d0277",
              "IPY_MODEL_8985e5a8e8ce49149f70730571821072",
              "IPY_MODEL_90f23944dc4c42c58bee36a5392fe691"
            ],
            "layout": "IPY_MODEL_d560e7d9b3174a789659810763920317"
          }
        },
        "2a67174e6ec9410f9cb1047cad3d0277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c93ae2ab677c4380b0c97f7fc0169242",
            "placeholder": "​",
            "style": "IPY_MODEL_728e2ea8302647bfb81aab5677c41911",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "8985e5a8e8ce49149f70730571821072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb24afc8a77a48a7945bc7c027df9951",
            "max": 52448,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a99d9b688cf8484a8eebdcc822ea3cab",
            "value": 52448
          }
        },
        "90f23944dc4c42c58bee36a5392fe691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89a5b9677738400eb10e57f9ce325f4e",
            "placeholder": "​",
            "style": "IPY_MODEL_4515bff6e72d436d91651962dd954652",
            "value": " 424k/? [00:00&lt;00:00, 28.3MB/s]"
          }
        },
        "d560e7d9b3174a789659810763920317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c93ae2ab677c4380b0c97f7fc0169242": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "728e2ea8302647bfb81aab5677c41911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb24afc8a77a48a7945bc7c027df9951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a99d9b688cf8484a8eebdcc822ea3cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89a5b9677738400eb10e57f9ce325f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4515bff6e72d436d91651962dd954652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4651a422fb64d74ba0dbaa661f19bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1a2ec427dad422897d1a1f15073d7c4",
              "IPY_MODEL_af7da63785834bd8ba3a70f366cd3f70",
              "IPY_MODEL_1f7c1fb53bb244fea43ee852ee37b81d"
            ],
            "layout": "IPY_MODEL_35dedb31d85d4962893ccb12692da191"
          }
        },
        "d1a2ec427dad422897d1a1f15073d7c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e2ba2899e974d67a756bf79a0ac9e74",
            "placeholder": "​",
            "style": "IPY_MODEL_d4b2550ef48045309a229ad2c724f9c1",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "af7da63785834bd8ba3a70f366cd3f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f48f661090649bf8ad1b9d57e6e2c9c",
            "max": 52448,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3a5a3d3903546109deca6fb3264c1a1",
            "value": 52448
          }
        },
        "1f7c1fb53bb244fea43ee852ee37b81d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffb7c74d8a7d4436a97d945b6848a8a9",
            "placeholder": "​",
            "style": "IPY_MODEL_0ee9f8b5c9284d66bf935e97133fc2ad",
            "value": " 424k/? [00:00&lt;00:00, 28.0MB/s]"
          }
        },
        "35dedb31d85d4962893ccb12692da191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e2ba2899e974d67a756bf79a0ac9e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b2550ef48045309a229ad2c724f9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f48f661090649bf8ad1b9d57e6e2c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a5a3d3903546109deca6fb3264c1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ffb7c74d8a7d4436a97d945b6848a8a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ee9f8b5c9284d66bf935e97133fc2ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zi-JBqaijhJa",
        "outputId": "fe7d7069-c0b0-40e0-c729-d3ef31138391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtext==0.6.0) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.6.0) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchtext==0.6.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchtext==0.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchtext==0.6.0) (3.0.2)\n",
            "Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchtext-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.6.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi[unidic-lite]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG7aKPj-jvBA",
        "outputId": "5bcc71a0-34b7-4716-94df-bf4c600a8b9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fugashi[unidic-lite]\n",
            "  Downloading fugashi-1.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting unidic-lite (from fugashi[unidic-lite])\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading fugashi-1.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (698 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.0/698.0 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=2cc60f380ab043649915bfb08113f25c6350b4c382c9ffa075dd7790b8036104\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/fd/e9/ea4459b868e6d2902e8d80e82dbacb6203e05b3b3a58c64966\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite, fugashi\n",
            "Successfully installed fugashi-1.4.0 unidic-lite-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "docXAqchjx1Q",
        "outputId": "67d4aed9-ba21-4377-ddfc-3e2a823d7a98"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: blis, thinc, spacy\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.7.1 requires spacy<3.8.0,>=3.7.2, but you have spacy 3.8.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.2.0 spacy-3.8.4 thinc-8.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#english\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOW1odrOk9oa",
        "outputId": "3b61ce10-3227-417b-a54f-3dec06bf456a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.7.1\n",
            "    Uninstalling en-core-web-sm-3.7.1:\n",
            "      Successfully uninstalled en-core-web-sm-3.7.1\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#japanese\n",
        "!python -m spacy download ja_core_news_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jaoczx2lAoG",
        "outputId": "35ae4026-cbd3-4b89-b96b-5f8883f45772"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ja-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ja_core_news_sm-3.8.0/ja_core_news_sm-3.8.0-py3-none-any.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sudachipy!=0.6.1,>=0.5.2 (from ja-core-news-sm==3.8.0)\n",
            "  Downloading SudachiPy-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting sudachidict-core>=20211220 (from ja-core-news-sm==3.8.0)\n",
            "  Downloading SudachiDict_core-20250129-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading SudachiDict_core-20250129-py3-none-any.whl (72.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SudachiPy-0.6.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sudachipy, sudachidict-core, ja-core-news-sm\n",
            "Successfully installed ja-core-news-sm-3.8.0 sudachidict-core-20250129 sudachipy-0.6.10\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ja_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chinese,arabic,russian\n",
        "!python -m spacy download zh_core_web_sm\n",
        "!python -m spacy download xx_ent_wiki_sm\n",
        "!python -m spacy download ru_core_news_sm\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEAWjx9lmnFg",
        "outputId": "013ad104-94cd-4112-e86a-9b519bf1caa7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting zh-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.8.0/zh_core_web_sm-3.8.0-py3-none-any.whl (48.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-pkuseg<2.0.0,>=1.0.0 (from zh-core-web-sm==3.8.0)\n",
            "  Downloading spacy_pkuseg-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0) (2.5.1)\n",
            "Collecting numpy<3.0.0,>=2.0.0 (from spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0)\n",
            "  Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from srsly<3.0.0,>=2.3.0->spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0) (2.0.10)\n",
            "Downloading spacy_pkuseg-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, spacy-pkuseg, zh-core-web-sm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.3.17 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.3 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.3 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.3 which is incompatible.\n",
            "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.3 spacy-pkuseg-1.0.0 zh-core-web-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('zh_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting xx-ent-wiki-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.8.0/xx_ent_wiki_sm-3.8.0-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xx-ent-wiki-sm\n",
            "Successfully installed xx-ent-wiki-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_ent_wiki_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Downloading pymorphy3-2.0.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg-python-0.7.2 pymorphy3-2.0.2 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset pth\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "path = \"/content/normalization_assesment_dataset_10k.csv\"\n",
        "df = pd.read_csv(path, usecols=[0,1], header=0)"
      ],
      "metadata": {
        "id": "LdDXQGyAlKRW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import spacy\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset"
      ],
      "metadata": {
        "id": "ahfy6uX9ndM4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torchtext.data import Field, TabularDataset"
      ],
      "metadata": {
        "id": "CHPTPqIFsSIn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "ZzhwUFJXsUyc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tokenization function\n",
        "def tokenize_text(text):\n",
        "    #tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "HyMrWc4Hslxk"
      },
      "execution_count": 360,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating fields\n",
        "SRC = Field(tokenize=tokenize_text,\n",
        "            init_token='<sos>',\n",
        "            eos_token='<eos>',\n",
        "            lower=False)\n",
        "\n",
        "TRG = Field(tokenize=tokenize_text,\n",
        "            init_token='<sos>',\n",
        "            eos_token='<eos>',\n",
        "            lower=False)"
      ],
      "metadata": {
        "id": "gt_n_aWBtLrt"
      },
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_fields = [('src', SRC), ('trg', TRG)]"
      ],
      "metadata": {
        "id": "WSeiUGoMtf8V"
      },
      "execution_count": 362,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV file\n",
        "input_file = '/content/normalization_assesment_dataset_10k.csv'  # Replace with the path to your input file\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Ensure the dataframe has the correct number of rows (10,000)\n",
        "assert len(df) == 10000, \"The CSV file must have exactly 10,000 rows\"\n",
        "\n",
        "# Split the dataframe into the three parts\n",
        "train_df = df.iloc[:5000]\n",
        "test_df = df.iloc[5000:7500]\n",
        "valid_df = df.iloc[7500:10000]\n",
        "\n",
        "# Save each part as a new CSV\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "test_df.to_csv('test.csv', index=False)\n",
        "valid_df.to_csv('valid.csv', index=False)\n",
        "\n",
        "print(\"CSV files split and saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H96aJuqpubh8",
        "outputId": "b27f8d70-0816-4e43-aaae-b79975fd02d4"
      },
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV files split and saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import nltk\n",
        " nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX1P5P30vmkk",
        "outputId": "74f4d459-cdf0-4c40-d0ac-3efe2a6f748c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into train, validation, and test sets\n",
        "train, val, test = TabularDataset.splits(\n",
        "    path='./',\n",
        "    train='train.csv',\n",
        "    validation='valid.csv',\n",
        "    test='test.csv',\n",
        "    format='csv',\n",
        "    fields=data_fields,  # Specify the fields mapping\n",
        "    skip_header=True\n",
        ")\n",
        "\n",
        "# Check the first example in the training data\n",
        "print(vars(train.examples[0]))\n",
        "\n",
        "# Check the length of the datasets\n",
        "print(f\"Train dataset size: {len(train.examples)}\")\n",
        "print(f\"Validation dataset size: {len(val.examples)}\")\n",
        "print(f\"Test dataset size: {len(test.examples)}\")\n",
        "\n",
        "# Build vocabulary for SRC and TRG using the training dataset\n",
        "SRC.build_vocab(train, min_freq=2)  # Set min_freq as needed to filter rare tokens\n",
        "TRG.build_vocab(train, min_freq=2)\n",
        "\n",
        "# Print the vocabulary sizes\n",
        "print(f\"Source vocabulary size: {len(SRC.vocab)}\")\n",
        "print(f\"Target vocabulary size: {len(TRG.vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syNCezKEtna9",
        "outputId": "f7394512-a2f8-4cbb-8105-8c0da8369c2e"
      },
      "execution_count": 364,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'src': ['Jordan', 'Riley/Adam', 'Argyle/Martin', 'Brammer'], 'trg': ['Jordan', 'Riley/Adam', 'Argyle/Martin', 'Brammer']}\n",
            "Train dataset size: 5000\n",
            "Validation dataset size: 2500\n",
            "Test dataset size: 2500\n",
            "Source vocabulary size: 1894\n",
            "Target vocabulary size: 1243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.examples[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LedXJGtv4VM",
        "outputId": "960ef014-fa57-43f6-dab8-a980c47309dd"
      },
      "execution_count": 365,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torchtext.data.example.Example object at 0x7da258e65b90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train.examples), len(val.examples), len(test.examples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7fRKjf2v8Bj",
        "outputId": "ba6bcd17-0002-47c5-d621-f34496f6f8cd"
      },
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 2500, 2500)"
            ]
          },
          "metadata": {},
          "execution_count": 366
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC.build_vocab(train, min_freq = 2)\n",
        "TRG.build_vocab(train, min_freq = 2)"
      ],
      "metadata": {
        "id": "KhwFRboRwTQi"
      },
      "execution_count": 367,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(SRC.vocab))\n",
        "print(len(TRG.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkIeur0yyF1l",
        "outputId": "2c9c6ed2-19d5-4ede-a1fe-9858590e7134"
      },
      "execution_count": 368,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1894\n",
            "1243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "aAD5WdItyJ6y"
      },
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 20\n",
        "\n",
        "train_iter = BucketIterator(\n",
        "    train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device = device\n",
        ")\n",
        "\n",
        "valid_iter = BucketIterator(\n",
        "    val,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device = device\n",
        ")\n",
        "\n",
        "test_iter = BucketIterator(\n",
        "    test,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device\n",
        ")"
      ],
      "metadata": {
        "id": "O5q8vTnmyTlF"
      },
      "execution_count": 370,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder of seq2seq model\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        #src = [src len, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "hZG_8gN-yX7C"
      },
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder of the seq2seq model\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "      #input = [1, batch size]\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        #prediction = [batch size, output dim]\n",
        "        return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "i6MvKiFuycTT"
      },
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "nKQf6UQAy2sa"
      },
      "execution_count": 373,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "metadata": {
        "id": "oDRgYTzIzEdC"
      },
      "execution_count": 374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAoT5JR8zKa7",
        "outputId": "54e8ed56-4d91-437c-fe01-e1425d8b1dd4"
      },
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(1894, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(1243, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=1243, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 375
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzGTTsQ8zPC5",
        "outputId": "4b3d5c3f-1644-4c2d-86d0-c2d7b524d09c"
      },
      "execution_count": 376,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 8,797,147 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "FTn7cmeBzSdx"
      },
      "execution_count": 377,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "6zvBIZnb0V9B"
      },
      "execution_count": 378,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip =1):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "\n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "2VAqZ_tz0aGR"
      },
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src   #src!\n",
        "            trg = batch.trg    #trg!\n",
        "            output = model(src, trg, 0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "i1zyeLmZ0jwC"
      },
      "execution_count": 380,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "1fKjG0yf23ko"
      },
      "execution_count": 381,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 15  # train for 15 epochs\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model,\n",
        "              train_iter,\n",
        "              optimizer,\n",
        "              criterion)\n",
        "    valid_loss = evaluate(model,\n",
        "              valid_iter,\n",
        "              criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk5Xb7TU28g4",
        "outputId": "44e71774-4776-4087-9685-89fa8a359ab3"
      },
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 6s\n",
            "\tTrain Loss: 3.017\n",
            "\t Val. Loss: 2.222\n",
            "Epoch: 02 | Time: 0m 6s\n",
            "\tTrain Loss: 2.572\n",
            "\t Val. Loss: 1.904\n",
            "Epoch: 03 | Time: 0m 6s\n",
            "\tTrain Loss: 2.261\n",
            "\t Val. Loss: 1.767\n",
            "Epoch: 04 | Time: 0m 6s\n",
            "\tTrain Loss: 2.087\n",
            "\t Val. Loss: 1.674\n",
            "Epoch: 05 | Time: 0m 6s\n",
            "\tTrain Loss: 1.958\n",
            "\t Val. Loss: 1.649\n",
            "Epoch: 06 | Time: 0m 6s\n",
            "\tTrain Loss: 1.855\n",
            "\t Val. Loss: 1.614\n",
            "Epoch: 07 | Time: 0m 6s\n",
            "\tTrain Loss: 1.756\n",
            "\t Val. Loss: 1.594\n",
            "Epoch: 08 | Time: 0m 6s\n",
            "\tTrain Loss: 1.657\n",
            "\t Val. Loss: 1.583\n",
            "Epoch: 09 | Time: 0m 6s\n",
            "\tTrain Loss: 1.574\n",
            "\t Val. Loss: 1.562\n",
            "Epoch: 10 | Time: 0m 6s\n",
            "\tTrain Loss: 1.477\n",
            "\t Val. Loss: 1.533\n",
            "Epoch: 11 | Time: 0m 6s\n",
            "\tTrain Loss: 1.380\n",
            "\t Val. Loss: 1.515\n",
            "Epoch: 12 | Time: 0m 6s\n",
            "\tTrain Loss: 1.282\n",
            "\t Val. Loss: 1.498\n",
            "Epoch: 13 | Time: 0m 6s\n",
            "\tTrain Loss: 1.193\n",
            "\t Val. Loss: 1.476\n",
            "Epoch: 14 | Time: 0m 6s\n",
            "\tTrain Loss: 1.102\n",
            "\t Val. Loss: 1.463\n",
            "Epoch: 15 | Time: 0m 6s\n",
            "\tTrain Loss: 1.016\n",
            "\t Val. Loss: 1.462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgAH4kov32cA",
        "outputId": "6d529664-7b22-4b9c-ab77-42693b23b61e"
      },
      "execution_count": 383,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-383-e971942e4ccb>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('tut1-model.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 1.365 | Test PPL:   3.916 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaN7TV2l665n",
        "outputId": "7a02ad7a-518c-4d3f-9f54-1048d9d7f322"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1v1os327EG-",
        "outputId": "1aaf8720-c740-4cec-b768-507b1f35a001"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (2.2.3)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (4.25.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
            "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.14.1 stanza-1.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import spacy\n",
        "import langdetect  # To detect language\n",
        "import jieba  # For Chinese tokenization\n",
        "import stanza  # For Russian, Arabic, and other languages\n",
        "\n",
        "# Load spaCy models for different languages\n",
        "spacy_en = spacy.load('en_core_web_sm')  # English tokenizer\n",
        "spacy_zh = spacy.load('zh_core_web_sm')  # Chinese tokenizer\n",
        "spacy_ar = spacy.load('xx_ent_wiki_sm')  # A multilingual tokenizer for Arabic, etc.\n",
        "# For Russian and other languages\n",
        "stanza_ru = stanza.Pipeline(lang='ru', processors='tokenize')\n",
        "stanza_ar = stanza.Pipeline(lang='ar', processors='tokenize')\n",
        "\n",
        "# Function to detect language\n",
        "def detect_language(text):\n",
        "    return langdetect.detect(text)\n",
        "\n",
        "def normalize_sentence(sentence, src_field, trg_field, model, device, max_len=50):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # If the input sentence is a list of tokens, join them into a single string\n",
        "    if isinstance(sentence, list):\n",
        "        sentence = ' '.join(sentence)  # Join the list of tokens into a string\n",
        "\n",
        "    # Detect the language of the input sentence (now it's a string)\n",
        "    language = detect_language(sentence)\n",
        "\n",
        "    # Tokenize based on language\n",
        "    if language == 'en':  # English\n",
        "        nlp = spacy_en\n",
        "        tokens = [token.text for token in nlp(sentence)]\n",
        "    elif language == 'zh':  # Chinese\n",
        "        tokens = list(jieba.cut(sentence))\n",
        "    elif language == 'ru':  # Russian\n",
        "        doc = stanza_ru(sentence)\n",
        "        tokens = [word.text for sent in doc.sentences for word in sent.words]\n",
        "    elif language == 'ar':  # Arabic\n",
        "        doc = stanza_ar(sentence)\n",
        "        tokens = [word.text for sent in doc.sentences for word in sent.words]\n",
        "    else:  # Other languages\n",
        "        nlp = spacy_zh  # Using a multilingual model for unsupported languages\n",
        "        tokens = [token.text for token in nlp(sentence)]\n",
        "\n",
        "    # Debugging step: print tokenized sentence\n",
        "    print(f\"Tokenized sentence: {tokens}\")\n",
        "\n",
        "    # Add start and end tokens\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
        "\n",
        "    # Convert tokens to their index representations in the vocabulary\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    # Pass the source tensor through the encoder\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "    # Initialize the target sequence with the <sos> token\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "    # Generate output sequence step by step\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "\n",
        "        # Get the predicted token (index)\n",
        "        pred_token = output.argmax(1).item()\n",
        "\n",
        "        # Append the predicted token to the target sequence\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        # Stop if we reach the <eos> token\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "            break\n",
        "\n",
        "    # Convert the indices back to tokens (the predicted normalized text)\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "    # Return the normalized sentence, removing <sos> and <eos> tokens\n",
        "    return trg_tokens[1:-1]  # Removing <sos> and <eos> when returning the output\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660,
          "referenced_widgets": [
            "e3088ffc3104424fb8fd5365b285c76c",
            "2a67174e6ec9410f9cb1047cad3d0277",
            "8985e5a8e8ce49149f70730571821072",
            "90f23944dc4c42c58bee36a5392fe691",
            "d560e7d9b3174a789659810763920317",
            "c93ae2ab677c4380b0c97f7fc0169242",
            "728e2ea8302647bfb81aab5677c41911",
            "fb24afc8a77a48a7945bc7c027df9951",
            "a99d9b688cf8484a8eebdcc822ea3cab",
            "89a5b9677738400eb10e57f9ce325f4e",
            "4515bff6e72d436d91651962dd954652",
            "f4651a422fb64d74ba0dbaa661f19bfe",
            "d1a2ec427dad422897d1a1f15073d7c4",
            "af7da63785834bd8ba3a70f366cd3f70",
            "1f7c1fb53bb244fea43ee852ee37b81d",
            "35dedb31d85d4962893ccb12692da191",
            "6e2ba2899e974d67a756bf79a0ac9e74",
            "d4b2550ef48045309a229ad2c724f9c1",
            "9f48f661090649bf8ad1b9d57e6e2c9c",
            "f3a5a3d3903546109deca6fb3264c1a1",
            "ffb7c74d8a7d4436a97d945b6848a8a9",
            "0ee9f8b5c9284d66bf935e97133fc2ad"
          ]
        },
        "id": "OxWULE5HmdPc",
        "outputId": "0f52ba64-e7b0-4da9-d0bb-d8ac538217ec"
      },
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3088ffc3104424fb8fd5365b285c76c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: ru (Russian):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | syntagrus |\n",
            "=========================\n",
            "\n",
            "INFO:stanza:Using device: cuda\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Done loading processors!\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4651a422fb64d74ba0dbaa661f19bfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "WARNING:stanza:Language ar package default expects mwt, which has been added\n",
            "INFO:stanza:Loading these models for language: ar (Arabic):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | padt    |\n",
            "| mwt       | padt    |\n",
            "=======================\n",
            "\n",
            "INFO:stanza:Using device: cuda\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
        "\n",
        "    trgs = []\n",
        "    pred_trgs = []\n",
        "\n",
        "    for datum in data:\n",
        "\n",
        "        src = vars(datum)['src']\n",
        "        trg = vars(datum)['trg']\n",
        "\n",
        "        pred_trg = normalize_sentence(src, src_field, trg_field, model, device, max_len)\n",
        "\n",
        "        #cut off <eos> token\n",
        "        #pred_trg = pred_trg[:-1]\n",
        "\n",
        "        pred_trgs.append(pred_trg)\n",
        "        trgs.append([trg])\n",
        "\n",
        "    return bleu_score(pred_trgs, trgs)"
      ],
      "metadata": {
        "id": "UATCxTmp7Sk_"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_value = calculate_bleu(test, SRC, TRG, model, device)\n",
        "\n",
        "print(f'BLEU score = {bleu_value*100:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "68VSGbZeOteY",
        "outputId": "18b78ec8-e4e1-4ea3-f452-eb877a945325"
      },
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentence: ['<', 'Unknown', '>', '/Mccarver', ',', 'William']\n",
            "Tokenized sentence: ['目黒', '将', '司', '/', 'Benjamin', 'Franklin']\n",
            "Tokenized sentence: ['Kalash/MIT', '/', 'Hatik']\n",
            "Tokenized sentence: ['Les', 'frères', 'Déjean']\n",
            "Tokenized sentence: ['Govana']\n",
            "Tokenized sentence: ['Natalia', 'María', 'Serna', 'Salazar']\n",
            "Tokenized sentence: ['Russell', 'Eric', 'Metoyer']\n",
            "Tokenized sentence: ['神前', '暁']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Tomosoiu', ',', 'Bogdan', 'Ioan']\n",
            "Tokenized sentence: ['Cheb', 'mimoun', 'el', 'oujdi']\n",
            "Tokenized sentence: ['Colin', 'Leadbetter', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Roy', 'Harper']\n",
            "Tokenized sentence: ['Tremendo', '/', 'Griffi']\n",
            "Tokenized sentence: ['Hotei', '/', 'Takeshi', 'Fujii/Yukinojo', 'Mori']\n",
            "Tokenized sentence: ['Huỳnh', 'Anh/Huy', 'ền', 'Thanh']\n",
            "Tokenized sentence: ['Billy', 'Joel']\n",
            "Tokenized sentence: ['Eric', 'Mendoza', '&', 'Christian', 'Moreno']\n",
            "Tokenized sentence: ['Madeen', 'Sk', '&', 'Kamal', 'Eslavath']\n",
            "Tokenized sentence: ['Mariah', 'Carey', 'and', 'Walter', 'Afanasieff']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Snider', ',', 'Chanel', 'Orion']\n",
            "Tokenized sentence: ['Charmaine', 'Wells', '&', 'Werner', 'Carrasco']\n",
            "Tokenized sentence: ['Woahkeii']\n",
            "Tokenized sentence: ['bem', '/', 'Siah', '/', 'K', 'yo', 'to']\n",
            "Tokenized sentence: ['Paul', 'Youdell', '/', 'Nathan', 'Hudson']\n",
            "Tokenized sentence: ['Dino', 'Cazares', '/', 'Travis', 'Neal', '/', 'Tim', 'Yeung', '/', 'Joe', 'Payne', '/', 'Logan', 'Mader', '/', 'Lucas', 'Banker']\n",
            "Tokenized sentence: ['Aida', 'Pohlhammer']\n",
            "Tokenized sentence: ['Pepe', 'Guí', 'zar']\n",
            "Tokenized sentence: ['Alex', 'Gabriel', 'Quezada']\n",
            "Tokenized sentence: ['芳賀', '敬', '太']\n",
            "Tokenized sentence: ['Addy', 'Flor', '-', 'Gil']\n",
            "Tokenized sentence: ['Dave', 'Southwick']\n",
            "Tokenized sentence: ['P.', 'Glasby', '/', 'F', '.', 'Del', 'Moro', '/', 'Tidy', 'Publishing', '/', 'remix', 'by', 'M.', 'Johnson', '/', 'Additional', 'production']\n",
            "Tokenized sentence: ['Artem', 'Gribov']\n",
            "Tokenized sentence: ['Richard', 'Wright', '/', 'Roger', 'Waters']\n",
            "Tokenized sentence: ['Freddie', 'Mercury']\n",
            "Tokenized sentence: ['Joe', 'Satriani']\n",
            "Tokenized sentence: ['Miguel', 'Ángel', 'Segura', ',', 'Eric', 'Fernan', 'do', 'Martinez', '&', 'Rober', 'to', 'Valle']\n",
            "Tokenized sentence: ['Dan', 'Couch', ',', 'Elvie', 'Shane', '&', 'Oscar', 'Charles']\n",
            "Tokenized sentence: ['つん', 'く♂']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Mincey', ',', 'Jeremy']\n",
            "Tokenized sentence: ['All', 'Members']\n",
            "Tokenized sentence: ['Frankie', 'Lopez', ',', 'Matthew', 'Cole', '&', 'Dante', 'Garcia', 'II']\n",
            "Tokenized sentence: ['Ashton', 'Graham', '&', 'Quentin', 'Taylor']\n",
            "Tokenized sentence: ['Rochester', 'Ezell']\n",
            "Tokenized sentence: ['Anderson', 'Freire']\n",
            "Tokenized sentence: ['Xuân', 'Quang']\n",
            "Tokenized sentence: ['Willie', 'Maxwell', 'II', '/', 'Angel', 'Luis', 'Cosme', '/', 'Salik', 'Singletary']\n",
            "Tokenized sentence: ['Nolan', 'Lambroza/Michael', 'Pollack', '/', 'Nguy', 'ễn', 'Anh', 'Vũ', '/', 'Alec', 'Shane', 'Benjamin']\n",
            "Tokenized sentence: ['Tim', 'Alexander']\n",
            "Tokenized sentence: ['Robert', 'Arthur']\n",
            "Tokenized sentence: ['David', 'Matthews']\n",
            "Tokenized sentence: ['$', 'LOTHBO', 'I', ',', 'Razegod']\n",
            "Tokenized sentence: ['Public', 'Domain']\n",
            "Tokenized sentence: ['Ley', 'Mart', 'ín']\n",
            "Tokenized sentence: ['Egor', 'Letov']\n",
            "Tokenized sentence: ['Charles', 'Ray', 'Rhodes']\n",
            "Tokenized sentence: ['Moccasin', 'Creek', '/', 'Jeff', 'McCool']\n",
            "Tokenized sentence: ['Joshua', 'Toala', '/', 'Stefan', 'Otto', 'Norris', '/', 'Divided', 'Mind', 'Music', '(', 'BMI', ')', '/The', 'Menace', 'Movement', 'Publishing', '(', 'ASCAP', ')']\n",
            "Tokenized sentence: ['Theodore', 'Presser']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Demrey', ',', 'Sebastian']\n",
            "Tokenized sentence: ['Swapnil', 'Sharma']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Smith', ',', 'Brian']\n",
            "Tokenized sentence: ['Benjamin', 'Braxton', '/', 'Neja']\n",
            "Tokenized sentence: ['Victor', 'Reyes']\n",
            "Tokenized sentence: ['Marc', 'Urréa/Alain', 'Llorca', '/', 'Francis', 'Delmas/Bernard', 'Mazauric', '/', 'Etienne', 'Salvador', '/', 'Lucien', 'Crémadès', '/', 'É', 'mile', 'Wandelmer', 'Santisteban', '/', 'Gold', '/', 'M', '.', 'Urréa', '/', 'B', '.', 'Mazauric']\n",
            "Tokenized sentence: ['Ronaldo', 'Billings']\n",
            "Tokenized sentence: ['Bernardo', 'Guevara', 'Barros']\n",
            "Tokenized sentence: ['Gus', 'Kahn', '/', 'Dave', 'Pell', '/', 'Edward', 'Eliscu', '/', 'Vincent', 'Youmans']\n",
            "Tokenized sentence: ['Deborah', 'Gibson', '/', 'Debbie', 'Gibson', '/', 'Evan', 'Rogers']\n",
            "Tokenized sentence: ['J', '.', 'Ferdy', '/', 'Mark', 'Wirt', 'z']\n",
            "Tokenized sentence: ['Björn', 'J', ':', 'son', 'Lindh']\n",
            "Tokenized sentence: ['Egber', 'to', 'Gismonti']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Krikes', ',', 'George', 'Arthur']\n",
            "Tokenized sentence: ['SoFay', 'go']\n",
            "Tokenized sentence: ['CAMPDERRICH', 'DELHORT', 'NIL', '/', 'ESTIVILL', 'NONELL', 'JORDI', '/', 'GARCIA', 'VERDASCO', 'ALEJANDRO', '/', 'RIOL', 'FERNANDEZ', 'DIDAC']\n",
            "Tokenized sentence: ['Daaku', '/', 'Deep', 'Chahal']\n",
            "Tokenized sentence: ['Ashraf', 'Babu']\n",
            "Tokenized sentence: ['BlauDis', 'S']\n",
            "Tokenized sentence: ['Sudhakar', 'Sharma/Himesh', 'Reshammiya']\n",
            "Tokenized sentence: ['Jonathan', 'Ong']\n",
            "Tokenized sentence: ['Fixi', '/', 'R', '.', 'Wan']\n",
            "Tokenized sentence: ['Terry', 'Finch']\n",
            "Tokenized sentence: ['Stew', 'Walker', '&', 'Victoria', 'Zeoli']\n",
            "Tokenized sentence: ['Howard', 'Jones']\n",
            "Tokenized sentence: ['Eric', 'Larios']\n",
            "Tokenized sentence: ['P', 'Yungin']\n",
            "Tokenized sentence: ['Moon', 'Kissed']\n",
            "Tokenized sentence: ['暗杠']\n",
            "Tokenized sentence: ['Fred', 'Weyrich', '(', 'dtsch', '.', ')', '/', 'Gary', 'Unwin', '/', 'Patti', 'Unwin']\n",
            "Tokenized sentence: ['Sultaan']\n",
            "Tokenized sentence: ['Spirit', 'Garden', '/', '近藤', '世真', '（', 'Elements', 'Garden', '）']\n",
            "Tokenized sentence: ['Claude', 'Vasori']\n",
            "Tokenized sentence: ['Amin', 'Rostami']\n",
            "Tokenized sentence: ['Frank', 'Madero/Jos', 'é', 'Javier', 'Puerta', '/', 'Ramón', 'Garriga']\n",
            "Tokenized sentence: ['Rudolf', 'Schenker', '/', 'Klaus', 'Meine']\n",
            "Tokenized sentence: ['Vince', 'Clarke']\n",
            "Tokenized sentence: ['IN', 'SOO', 'SHIN', '/', 'Sung', 'Eun', 'Lim']\n",
            "Tokenized sentence: ['KVA', 'RFORTH', 'NIKLAS']\n",
            "Tokenized sentence: ['Gabriel', 'Guerrero']\n",
            "Tokenized sentence: ['CA', 'COMPOSER', 'UNKNOWN', '/', 'CA', 'WILLIAMS', 'CHRISTOPHER', '/', 'PA', 'CHUUWEE', '&', 'TRIZZ']\n",
            "Tokenized sentence: ['Eliza', 'Legzdina', '&', 'Kaveh', 'Ayati']\n",
            "Tokenized sentence: ['Ulf', 'Reinhard']\n",
            "Tokenized sentence: ['Bucks']\n",
            "Tokenized sentence: ['James', 'DeVito', '/', 'Luke', 'Silas', '/', 'Ary', 'Warnaar', '/', 'Peter', 'Berkman']\n",
            "Tokenized sentence: ['Jason', 'Corbett']\n",
            "Tokenized sentence: ['Michael', 'Bruce']\n",
            "Tokenized sentence: ['Mer', 'v', 'Pepler']\n",
            "Tokenized sentence: ['Kapone', 'James']\n",
            "Tokenized sentence: ['Béla', 'Bart', 'ók']\n",
            "Tokenized sentence: ['夏代', '孝明', '(', 'Natsushiro', 'Takaaki', ')']\n",
            "Tokenized sentence: ['Rich', 'Fowler']\n",
            "Tokenized sentence: ['Gurvan', 'Liard', '/', 'Jérôme', 'Ettinger', '/', 'Benjamin', 'Bouton', '/', 'Sylvain', 'Corbard', '/', 'Jean', '-', 'Yves', 'Redor', '/', 'Sébastien', 'Delaunay']\n",
            "Tokenized sentence: ['PJ', 'Harvey']\n",
            "Tokenized sentence: ['Domino', 'Publishing']\n",
            "Tokenized sentence: ['Sam', 'Hollander', '/', 'Garrett', 'Dut', 'ton', '/', 'Robert', 'Carranza']\n",
            "Tokenized sentence: ['Singh', '&', 'Singh']\n",
            "Tokenized sentence: ['Prod', 'by', 'ninez', '/', 'Jamie', 'Turner', '/', 'Damanie', 'Harvey', '-', 'Campbell']\n",
            "Tokenized sentence: ['Berto', 'Perez']\n",
            "Tokenized sentence: ['Trad', '.', 'Arr', '.', 'Fabian', 'Holland']\n",
            "Tokenized sentence: ['JJ', 'Doom']\n",
            "Tokenized sentence: ['Steve', 'Porcaro', ',', 'John', 'Bettis', ',', 'TONK', 'Wit', 'Tha', 'Gift', ',', 'YNW', 'Melly']\n",
            "Tokenized sentence: ['John', 'Wesley']\n",
            "Tokenized sentence: ['Bradley', 'David', 'Parsons']\n",
            "Tokenized sentence: ['CA', 'GUTIERREZ', 'STEVEN', 'ALEXANDER', 'RENGIFO', '/', 'CA', 'MORENO', 'LEROY', 'JR', '/', 'PA', 'STEVE', 'DEKAY', '&', 'LEROY', 'MORENO']\n",
            "Tokenized sentence: ['Joshua', 'Klein']\n",
            "Tokenized sentence: ['Hernán', 'De', 'Horta']\n",
            "Tokenized sentence: ['Labi', 'Siffre']\n",
            "Tokenized sentence: ['Han', 'All', '/', 'In', 'Young', 'Jung']\n",
            "Tokenized sentence: ['Olli', '(', 'Funk', ')']\n",
            "Tokenized sentence: ['Natalie', 'Stafford', '&', 'Jean', '-', 'Odlin', 'Jean', '-', 'Noel']\n",
            "Tokenized sentence: ['Igor', 'Stravinsky']\n",
            "Tokenized sentence: ['Hardstyle', 'Ger', 'many']\n",
            "Tokenized sentence: ['FER', 'N', 'A', 'N', 'D', 'O', 'AUGUS', 'TO', '/', 'FER', 'N', 'A', 'N', 'D', 'O', 'MENDES']\n",
            "Tokenized sentence: ['Harry', 'Chang', '/', 'JJ', 'Lin']\n",
            "Tokenized sentence: ['C', 'LAUGESEN', 'HENRIK', '/', 'PA', 'LAUGE', '&', 'BABA', 'GNOHM']\n",
            "Tokenized sentence: ['Derechos', 'De', 'Autor', 'Reservados', '/', 'Héctor', 'Serafín', 'Bojór', 'quez', 'Leyva']\n",
            "Tokenized sentence: ['Phạm', 'Duy', '/', 'Phạm', 'Thanh', 'Bình']\n",
            "Tokenized sentence: ['Stephen', 'Rippy']\n",
            "Tokenized sentence: ['Robert', 'Woods', '/', 'Pyot', 'r', 'Ilyich', 'Tchaikovsky']\n",
            "Tokenized sentence: ['Axel', 'Breitung', '/', 'Alex', 'Trime', '/', 'René', 'Baumann', '/', 'Sven', '``', 'Delgado', \"''\", 'Jordan']\n",
            "Tokenized sentence: ['Frank', 'Churchill', '/', 'Larry', 'Morey']\n",
            "Tokenized sentence: ['Samuel', 'Selkridge']\n",
            "Tokenized sentence: ['Ice', 'Money']\n",
            "Tokenized sentence: ['BAO', 'THACH']\n",
            "Tokenized sentence: ['DHRUV', 'A']\n",
            "Tokenized sentence: ['JohnJoseph', 'Giurano']\n",
            "Tokenized sentence: ['Đạt', 'Long', 'Vinh']\n",
            "Tokenized sentence: ['LUC', 'K', 'I', ',', 'Yung', 'I', 'cey']\n",
            "Tokenized sentence: ['Marco', 'Carmelo', 'Gange', 'mi', '/', 'Francesco', 'Spampinato']\n",
            "Tokenized sentence: ['John', 'Lennon']\n",
            "Tokenized sentence: ['Enthic', '/', 'L', '.', 'Bundil', '/', 'Petr', 'Lexa', '/', 'Pam', 'Rabbit']\n",
            "Tokenized sentence: ['CONTROL', 'COPYRIGHT', '/', 'HUGO', 'DE', 'FARIA', 'GONCALVES', 'VICTOR']\n",
            "Tokenized sentence: ['森悠', '也']\n",
            "Tokenized sentence: ['Kevin', 'Gates']\n",
            "Tokenized sentence: ['Ben', 'Hayslip/', 'Rhett', 'Akins', '/', 'Brantley', 'Gilbert']\n",
            "Tokenized sentence: ['Dr', '.', 'Alban', '/', 'Sascha', 'Lappessen', '/', 'Ralf', 'Kappmeier', '/', 'Thomas', 'Alisson']\n",
            "Tokenized sentence: ['Annie', 'Lennox', '/', 'Dave', 'Stewart']\n",
            "Tokenized sentence: ['CA', 'BONNER', 'EVERTON', '/', 'CA', 'CHARLES', 'DENNIS', '/', 'CA', 'CORNEILLE', '/', 'CA', 'HADDAD', '/', 'CA', 'TAYLOR', 'JOHN', 'CHRISTOPHER', '/', 'PA', 'CHAKA', 'DEMUS', '&', 'PLIERS']\n",
            "Tokenized sentence: ['Michal', 'Ormand', 'ík', '/', 'Hisohkah']\n",
            "Tokenized sentence: ['Subhash', 'Jena']\n",
            "Tokenized sentence: ['پر', 'وی', 'ز', 'قد', 'رخ', 'ان', 'ی', '/', 'حم', 'ید', 'رض', 'ا', 'رح', 'یم', 'ی']\n",
            "Tokenized sentence: ['Ryan', 'Robinette']\n",
            "Tokenized sentence: ['Pryme', '/', 'Xtofa', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Jhoan', 'Varon', 'Santiago', '&', 'Abraham', 'Lopez', 'G']\n",
            "Tokenized sentence: ['raleon', 'deshon', 'Moore']\n",
            "Tokenized sentence: ['Wood', 'Baptiste']\n",
            "Tokenized sentence: ['Terrell', 'Kennedy']\n",
            "Tokenized sentence: ['Louis', 'St', 'Mary', 'III']\n",
            "Tokenized sentence: ['Martin', 'Tungevaag/Harper', '/', 'Henning', 'Olerud', '/', 'Caroline', 'Elisabeth', 'Hartung', 'Sjoelshagen']\n",
            "Tokenized sentence: ['MALIN', '(', 'GB', '1', ')', 'D', 'A', 'V', 'ID', '/', 'N', 'IC', 'K', 'HOGARTH']\n",
            "Tokenized sentence: ['Kenneth', 'Bryant']\n",
            "Tokenized sentence: ['Buddah', 'Bless', '/', 'Tyron', 'Douglas', '/', 'Miles', 'Parks', 'McCollum']\n",
            "Tokenized sentence: ['Stephen', 'Morris/Bernard', 'Sumner', '/', 'Ian', 'Curtis', '/', 'Peter', 'Hook']\n",
            "Tokenized sentence: ['Daniel', 'Ritter']\n",
            "Tokenized sentence: ['Nikk', 'i', 'Sixx', '/', 'Dave', 'Darling']\n",
            "Tokenized sentence: ['Luca', 'Francini']\n",
            "Tokenized sentence: ['David', 'Lynch', '/', 'Zola', 'Taylor']\n",
            "Tokenized sentence: ['Norus', 'Padidar', '/', 'Emile', 'Hartkamp', '/', 'Manfred', 'Jongenelis', '/', 'Smaragd', 'Publishing', '/', 'Goldstar', 'Music', 'Edition']\n",
            "Tokenized sentence: ['Mark', 'Knight', '/', 'Michael', 'Gray', '/', 'Andreya', 'Triana']\n",
            "Tokenized sentence: ['Malu', 'Anna', '&', 'SM', 'music', 'SUN', 'I', 'L']\n",
            "Tokenized sentence: ['Sam', 'Burton']\n",
            "Tokenized sentence: ['Thurston', 'Moore', '/', 'Sonic', 'Youth']\n",
            "Tokenized sentence: ['Hugo', 'Ruiz']\n",
            "Tokenized sentence: ['Daniel', 'Lee', 'Williams', 'jr']\n",
            "Tokenized sentence: ['Bruno', '/', 'Ed', 'Reis', '/', 'MV', 'Music', '/', 'Jo', 'ão', 'Victor']\n",
            "Tokenized sentence: ['Wendt', '/', 'Lundh']\n",
            "Tokenized sentence: ['Hironobu', 'Kageyama', '(', 'PK', 'A', 'Hironobu', 'Kag', '...', ')']\n",
            "Tokenized sentence: ['Angel', 'YUmbai']\n",
            "Tokenized sentence: ['Niall', 'Stenson']\n",
            "Tokenized sentence: ['James', 'A', 'Manno', '&', 'Steve', 'Bloom']\n",
            "Tokenized sentence: ['Ricardo', 'Palacin', '/', 'Domingo', 'Edjang', 'Moreno']\n",
            "Tokenized sentence: ['Jay', 'Newland', '/', 'Soren', 'Bigum/Benjamin', 'Traeru', 'p/', 'Indra', 'Rios', '-', 'Moore']\n",
            "Tokenized sentence: ['CA', 'A', 'L', 'D', 'A', 'N', 'M', 'AZ', 'PE', 'TER', '/', 'CA', 'CAPLAN', 'LEIGH', '/', 'P', 'A', 'QU', 'ADRANT', '&', 'NC', '-', '17']\n",
            "Tokenized sentence: ['Defsoul', '/', 'Yugyeom', '/', 'Stainboys', '/', 'Park', 'Seulgi', '/', 'Go', 'Younghwan']\n",
            "Tokenized sentence: ['Mikael', 'Tariverdiev', '&', 'Andrei', 'Voznesensky']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Jackson', ',', 'Shawn', 'Westly']\n",
            "Tokenized sentence: ['DALE', 'PAYNE', '/', 'LETITIA', 'CAYE', 'MEEKS', '/', 'YUNG', 'DREW']\n",
            "Tokenized sentence: ['organ', 'mixt']\n",
            "Tokenized sentence: ['植松伸夫']\n",
            "Tokenized sentence: ['Allen', 'Touissant', '/', 'W', '.', 'Lee']\n",
            "Tokenized sentence: ['Philip', 'Glass']\n",
            "Tokenized sentence: ['Ben', 'Lingen']\n",
            "Tokenized sentence: ['Sony', 'Assla', '/', 'V', '.', 'Kainth']\n",
            "Tokenized sentence: ['Megan', 'Thee', 'Stallion']\n",
            "Tokenized sentence: ['Ив', 'ан', 'Го', 'мз', 'ик', 'ов']\n",
            "Tokenized sentence: ['Josh', 'Kerr', '/', 'BMG', 'Platinum', 'Songs', '/', 'Kailey', \"'s\", 'Dream', '(', 'BMI', ')', '/Blue', 'Corolla', 'Oklahoma', '/', 'Feel', 'Your', 'Creative', 'Pulse', 'Music', '(', 'SESAC', ')', '/Warner', '-', 'Tamerlane', 'Publishing', 'Corp.', '(', 'BMI', ')', ',', 'Songs', 'of', 'Home', 'Team', 'Music', '(', 'BMI', ')', ',', 'and', 'Tunes', 'of', 'TrailerParker', '(', 'BMI', ')']\n",
            "Tokenized sentence: ['Elliot', 'Chiprut']\n",
            "Tokenized sentence: ['Slow', 'Pain']\n",
            "Tokenized sentence: ['Ellery', 'Eskelin']\n",
            "Tokenized sentence: ['Hannah', 'Cutt', ',', 'Sean', 'Hurwitz', '&', 'Gregg', 'Cash']\n",
            "Tokenized sentence: ['Сергей', 'Васильевич', 'Рахманинов']\n",
            "Tokenized sentence: ['Bruce', 'Fingers', '/', 'Dylan', 'Thomas', 'Price', '/', 'Billie', 'Ray', 'Fingers']\n",
            "Tokenized sentence: ['Emilly', 'Silva', 'Lins']\n",
            "Tokenized sentence: ['Huda', 'Hudia', '&', 'DJ', 'Fixx']\n",
            "Tokenized sentence: ['Das', ',', 'Dayal', '&', 'Girdher']\n",
            "Tokenized sentence: ['BiS', '&', 'どりみ', '/', '松隈', 'ケン', 'タ']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Antoine', ',', 'Katoto', 'Luhembe']\n",
            "Tokenized sentence: ['A', '.', 'Zuleta', '/', 'L', '.', 'Garcia', '/', 'Rigober', 'to', 'Tovar', 'Garcia']\n",
            "Tokenized sentence: ['PETIT', 'ALEX', 'SR', '(', 'MR', ')', '/', 'THIRSTON', 'SAD', 'IK', 'I']\n",
            "Tokenized sentence: ['Shlomo', 'Artzi', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Didik', 'Sucahy', 'o/', 'Remy', 'Soetansyah']\n",
            "Tokenized sentence: ['Adam', 'Zouitina']\n",
            "Tokenized sentence: ['Megan', 'Pete', '/', 'Austin', 'Owens', '/', 'Jordan', 'Thorpe', '/', 'James', 'Foye', 'III', '/', 'Frank', 'Rodriguez', '/', 'Belcalis', 'Almanzar']\n",
            "Tokenized sentence: ['Milkyy', 'Melodies']\n",
            "Tokenized sentence: ['Steven', 'Fritsch', '/', 'Marcel', 'Stephan', '/', 'Vitali', 'Zestovskih', '/', 'Jan', 'Niklas', 'Simonsen/Berislaw', 'Audenaerd']\n",
            "Tokenized sentence: ['D.', 'Morrison', '/', 'G', '.', 'Cherchia', '/', 'Vicious', 'Grooves', '/', 'A', '.', 'Van', 'Dorsselaer', '/', 'Dean', 'Morrison', '/', 'Gustavo', 'Cherchia', '/', 'Adam', 'Van', 'Dorsselaer']\n",
            "Tokenized sentence: ['Barry', 'White']\n",
            "Tokenized sentence: ['Peezy', ',', 'Brielle', 'Lesley']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Hansen', ',', 'Ivar']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Kay', ',', 'Will']\n",
            "Tokenized sentence: ['Carlos', 'Arturo', 'Brito']\n",
            "Tokenized sentence: ['Daniel', 'Heredero']\n",
            "Tokenized sentence: ['WB', 'Music']\n",
            "Tokenized sentence: ['Avinash', '-', 'Vishwajeet', '/', 'Ashwini', 'Shende']\n",
            "Tokenized sentence: ['Kim', 'McMechan']\n",
            "Tokenized sentence: ['Johnny', 'Andrews', '/', 'Jared', 'Wallace', '/', 'Michael', 'Weldon']\n",
            "Tokenized sentence: ['Nick', 'Phoenix', '/', 'Thomas', 'Bergersen']\n",
            "Tokenized sentence: ['Stas', 'Mikhaylov']\n",
            "Tokenized sentence: ['Snoop', 'Dogg', ',', 'Hug', ',', 'Dr', '.', 'Dre']\n",
            "Tokenized sentence: ['THOMPSON', 'FRAZIER', 'O', 'III', '/', 'WOODS', 'KEVIN', 'LEE']\n",
            "Tokenized sentence: ['Torence', 'Hatch']\n",
            "Tokenized sentence: ['Paul', 'Heyse', '/', 'Candidus', '/', 'John', 'Henry', 'Mackay', '/', 'Ludwig', 'Uhland', '/', 'Richard', 'Dehmel/Richard', 'Strauss/Hermann', 'Von', 'Gilm', '/', 'Johannes', 'Brahms', '/', 'Heinrich', 'Heine', '/', 'Friedrich', 'Daumer', '/', 'Ot', 'to', 'Julius', 'Bier', 'baum', '/', 'R', '.', 'Reinick', '/', 'Karl', 'Henckell/Eduard', 'Mörike', '/', 'Heinrich', 'Hart', '/', 'Volkslied']\n",
            "Tokenized sentence: ['Kobalt', '/', 'Mark', 'Crew/Spirit', '/', 'b', '-', 'unique', '(', 'P', 'R', 'S', 'CAE', ':', '00716588904', ')']\n",
            "Tokenized sentence: ['Trad']\n",
            "Tokenized sentence: ['J', '.', 'P.', 'De', 'Brum', 'Paula/', 'M', '.', 'Vorwerk', '/', 'J', '.', 'P.', 'S', '.', 'S', '.', 'De', 'Rosario']\n",
            "Tokenized sentence: ['Thomas', 'Padilla']\n",
            "Tokenized sentence: ['Lil', 'Tracy']\n",
            "Tokenized sentence: ['Sarah', 'She', 'eva']\n",
            "Tokenized sentence: ['Mata', 'Jones']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Reese', ',', 'Christina']\n",
            "Tokenized sentence: ['Takak', 'i', 'Horigome/Yasuyuk', 'i', 'Horigome']\n",
            "Tokenized sentence: ['Neal', 'Morse']\n",
            "Tokenized sentence: ['Yugo', 'Kanno']\n",
            "Tokenized sentence: ['Ivy', 'Joe', 'Hunter', '/', 'William', 'Mickey', 'Stevenson']\n",
            "Tokenized sentence: ['Hillel', 'Slovak', '/', 'Jack', 'Irons', '/', 'Anthony', 'Kiedis', '/', 'Flea']\n",
            "Tokenized sentence: ['Lelê', '/', 'Allefy', '/', 'Guga', 'Nandes', '/', 'Bruno', 'Cardoso']\n",
            "Tokenized sentence: ['George', 'W', '.', 'Cooke']\n",
            "Tokenized sentence: ['손민', '수', '(', 'DEV', 'SIST', 'ERS', ')']\n",
            "Tokenized sentence: ['I', 'shmael', 'Azeez']\n",
            "Tokenized sentence: ['\\u200biTownGameplay', ',', 'Zarcort']\n",
            "Tokenized sentence: ['Jay', 'B']\n",
            "Tokenized sentence: ['Joni', 'Mitchell']\n",
            "Tokenized sentence: ['Vic', 'Chesnutt', '/', 'Rob', 'Veal']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Moeller', ',', 'Kim']\n",
            "Tokenized sentence: ['Mikael', 'Bolyos']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/', 'zenny', ',', 'joseph', 'junior']\n",
            "Tokenized sentence: ['P.', 'Clavi/', 'Mieczyslaw', 'Wojnick', 'i']\n",
            "Tokenized sentence: ['Kris', 'Kristofferson']\n",
            "Tokenized sentence: ['May', 'a', 'Yepes', '(', 'Duque', 'Osorio', ',', 'Flore', 'z', '&', 'Vasque', 'z', ')']\n",
            "Tokenized sentence: ['Benjamin', 'James', 'Azzi', '/', 'Benjamin', 'Jon', 'Ringel', '/', 'David', 'Michael', 'Supica', '/', 'Gregory', 'Michael', 'Hommert', '/', 'Jonathan', 'Darrell', 'Shaw']\n",
            "Tokenized sentence: ['René', 'de', 'Bruijn', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Kirk', 'Monteux']\n",
            "Tokenized sentence: ['Danny', 'White', '/', 'Darren', 'White', '/', 'B', '.', 'Trzetrzelewska']\n",
            "Tokenized sentence: ['Abel', 'Baer', ',', 'Anita', 'Leonard', ',', 'Barbara', 'Belle', ',', 'Bill', 'Crandall', ',', 'Bill', 'Dalton', ',', 'Bob', 'Crewe', ',', 'Bob', 'Gaudio', ',', 'Curtis', 'Williams', ',', 'Dorothy', 'Fields', ',', 'Frank', 'Slay', ',', 'Gaynel', 'Hodge', ',', 'James', 'Moody', ',', 'Jesse', 'Belvin', ',', 'Jimmy', 'McHugh', ',', 'Louis', 'Prima', ',', 'Louis', 'Wolfe', 'Gilbert', ',', 'Otis', 'Blackwell', ',', 'Stan', 'Rhodes', ',', 'Thomas', 'Austin', '&', 'Tom', 'Austin']\n",
            "Tokenized sentence: ['Manuel', 'de', 'Falla']\n",
            "Tokenized sentence: ['El', 'Mayor', 'Clásico']\n",
            "Tokenized sentence: ['Tom', 'Baxter', '/', 'R', 'I', 'T', 'U', 'A', 'L', '/', 'Adam', 'Midgley', '/', 'Gerard', \"O'Connell\"]\n",
            "Tokenized sentence: ['Shevchenko', 'Aleksander', 'Aleksandrovich', '/', 'Dunaevskiy', 'Maksim', 'I', 'saakovich']\n",
            "Tokenized sentence: ['Edgar', 'Barrera/Christian', 'Nodal', '/', 'Jaime', 'Gon', 'zález', '/', 'René', 'Humber', 'to', 'Lau', 'Ibarra']\n",
            "Tokenized sentence: ['Tjahjadi', 'Djanat', 'a', ',', 'I', 'shak', '&', 'Tjahjadi', 'Djajanat', 'a']\n",
            "Tokenized sentence: ['Joakim', 'Brodén', '/', 'Tommy', 'Johansson']\n",
            "Tokenized sentence: ['Rian', 'Ekky', 'Pradipta', '/', 'Ardiansyah', '/', 'Gea']\n",
            "Tokenized sentence: ['yesPer', './', 'Jesper', 'Krieg']\n",
            "Tokenized sentence: ['Jaideep', 'Sahni', '/', 'Salim', '-', 'Sulaiman']\n",
            "Tokenized sentence: ['Sneha', 'Khanwalkar']\n",
            "Tokenized sentence: ['Bryce', 'Owen', 'Hayes', '&', 'Donny', 'McDougal']\n",
            "Tokenized sentence: ['Ijeom', 'a', 'Anastasia', 'Igweat', 'u']\n",
            "Tokenized sentence: ['Phạm', 'Duy', '/', 'Phạm', 'Thanh', 'Bình']\n",
            "Tokenized sentence: ['Michael', 'Norman', 'Sorrells']\n",
            "Tokenized sentence: ['SHEHAB', 'ELSAYED']\n",
            "Tokenized sentence: ['M.Ashraf', '&', 'A.Hameed']\n",
            "Tokenized sentence: ['La', 'Familia', 'Valera', 'Miranda']\n",
            "Tokenized sentence: ['AR', 'BROWNE', 'JACKSON', '/', 'AR', 'LUNNY', 'DONAL', '/', 'AR', 'SHANNON', 'SHARON', 'TERESA', '/', 'CA', 'TRAD', '/', 'PA', 'JACKSON', 'BROWNE', '&', 'SHARON', 'SHANNON']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Godbolt', ',', 'Thomas']\n",
            "Tokenized sentence: ['Boosie', 'Bada', 'zz']\n",
            "Tokenized sentence: ['Milton', 'Turell']\n",
            "Tokenized sentence: ['Otávio', 'A', '.']\n",
            "Tokenized sentence: ['Kash', 'Bang/Bad', 'Tree']\n",
            "Tokenized sentence: ['Lullaby', 'Music', '&', 'kroh', 'music']\n",
            "Tokenized sentence: ['김희', '진']\n",
            "Tokenized sentence: ['Fabio', 'Clemente', '/', 'Davide', 'Petrella', '/', 'Jacopo', 'Lazzarini/Alessandro', 'Merli', '/', 'Emanuele', 'Palumbo']\n",
            "Tokenized sentence: ['3g_kon', '/', '\\\\n']\n",
            "Tokenized sentence: ['Noriyasu', 'Agematsu']\n",
            "Tokenized sentence: ['Sahir', 'Ludhian', 'vi', '/', 'Khayyam']\n",
            "Tokenized sentence: ['Roy', 'Engel', '/', 'Nitzan', 'Schachaf']\n",
            "Tokenized sentence: ['Cherry', 'Filter']\n",
            "Tokenized sentence: ['Amit', 'Trivedi', '/', 'Amitabh', 'Bhattacharya']\n",
            "Tokenized sentence: ['Dan', 'zo', ',', 'Veigh']\n",
            "Tokenized sentence: ['Jens', 'Thele', '/', 'Andy', 'Bown', '/', 'H', '.', 'P.', 'Baxxter', '/', 'Rick', 'Parfitt', '/', 'Rick', 'J.', 'Jordan', '/', 'Michael', 'Simon']\n",
            "Tokenized sentence: ['Jan', 'Kazda', '/', 'Christofer', 'Johnsson', '/', 'Thomas', 'Karlsson', '/', 'Therion']\n",
            "Tokenized sentence: ['Almanac/Bad', 'Neighborhood']\n",
            "Tokenized sentence: ['Austin', 'Reynolds', '/', 'Bryan', 'Akers']\n",
            "Tokenized sentence: ['Ric', 'Marlow', '/', 'Bobby', 'Scott']\n",
            "Tokenized sentence: ['Tay', 'Global', '/', 'Arthur', 'T.', 'Kelley']\n",
            "Tokenized sentence: ['R', '.', 'Weston']\n",
            "Tokenized sentence: ['Mau', 'Chih', 'Fang']\n",
            "Tokenized sentence: ['PJ', 'Voorhees']\n",
            "Tokenized sentence: ['Circa', '242', '/', 'Mick', 'Coogan', '/', 'Scott', 'Dittrich', '/', 'Prescription', 'Songs', '/', 'Michael', \"'\", 'Omega', \"'\", 'Fonseca']\n",
            "Tokenized sentence: ['Fran', 'z', 'Doelle', '/', 'Fritz', 'Rotter', '/', 'Berndt', 'Carlberg']\n",
            "Tokenized sentence: ['Rick', 'Springfield']\n",
            "Tokenized sentence: ['Ruben', 'Mende', 'z', 'Del', 'Castillo']\n",
            "Tokenized sentence: ['Lisa', 'Schonberg/Allan', 'Wilson']\n",
            "Tokenized sentence: ['Showmain', '/', 'Hoop', 'Records', '/', 'Chiu', 'Ying', 'Hsun']\n",
            "Tokenized sentence: ['Kris', 'Barras']\n",
            "Tokenized sentence: ['Roger', 'Gustave', 'Samyn', '/', 'Françoise', 'Hardy']\n",
            "Tokenized sentence: ['Colonnello/Testa']\n",
            "Tokenized sentence: ['Limit']\n",
            "Tokenized sentence: ['Ravi', 'Basrur', '/', 'Shabbir', 'Ahmed', '/', 'Zee', 'Music', 'Company']\n",
            "Tokenized sentence: ['Brian', 'Lee', 'White', '/', 'Brian', 'Trifon']\n",
            "Tokenized sentence: ['Ben', 'Raleigh', '/', 'David', 'Mook']\n",
            "Tokenized sentence: ['Griffin', 'Sherry', '/', 'Maxwell', 'Davis', '/', 'Sean', 'McCarthy']\n",
            "Tokenized sentence: ['Tom', 'MacDonald']\n",
            "Tokenized sentence: ['Jimi', 'Hendrix']\n",
            "Tokenized sentence: ['Gioachino', 'Rossini', '/', 'Giovanni', 'Federico', 'Schmidt']\n",
            "Tokenized sentence: ['Ingrid', 'Michaelson']\n",
            "Tokenized sentence: ['Steven', 'Jansen', '/', 'Jack', 'Gourlay', '/', 'Joe', 'Taylor', '/', 'Lucas', 'de', 'Wert', '/', 'Victor', 'Leicher', '/', 'Stephan', 'Leicher']\n",
            "Tokenized sentence: ['Dane', 'Anders', 'Benson']\n",
            "Tokenized sentence: ['Beas', 'Flores', '&', 'Castellanos', 'Higared', 'a']\n",
            "Tokenized sentence: ['Kenichi', 'Maeyamada']\n",
            "Tokenized sentence: ['AMP', 'Chris']\n",
            "Tokenized sentence: ['MAKAVEL', 'IGO', 'DD']\n",
            "Tokenized sentence: ['Kun', 'zevebe', '/', 'Kartner', '/', 'Ida', 'From', '/', 'Bent', 'From']\n",
            "Tokenized sentence: ['Alejandro', 'Maximilian', 'o', 'Espinoza']\n",
            "Tokenized sentence: ['Hiromu', 'Akita']\n",
            "Tokenized sentence: ['Truva/Malte', 'Kuhn', '/', 'Benjamin', 'Asare', '/', 'Jens', 'Schneider', '/', 'Farsad', 'Zoroofchi', '/', 'Jules', 'Kal', 'mbacher']\n",
            "Tokenized sentence: ['A', 'A', 'REIN', 'I', '/', 'C', 'A', 'REINI', '/', 'P', 'A', 'ANTT', 'I', 'REIN', 'I', '&', 'MAAPALLON', 'MAITOLAITUR', 'IT']\n",
            "Tokenized sentence: ['Dorothy', 'Heyward', '/', 'George', 'Gershwin', '/', 'Ira', 'Gershwin', '/', 'DuBose', 'Heyward']\n",
            "Tokenized sentence: ['Chris', 'Cheney']\n",
            "Tokenized sentence: ['CA', 'KOTTEN', 'LEVI', '/', 'CA', 'LINNEY', 'CAITLIN', 'DONERLY', '/', 'CA', 'REGAN', 'LEENA', '/', 'CA', 'RUTTE', 'STIJN', '/', 'PA', 'AVAO', '&', 'LINNEY']\n",
            "Tokenized sentence: ['Bobby', 'Plater', '/', 'Edward', 'Johnson', '/', 'Buddy', 'Feyne', '/', 'Tiny', 'Bradshaw']\n",
            "Tokenized sentence: ['Serapio', 'Ramirez']\n",
            "Tokenized sentence: ['Anand', 'Bakshi/Rahul', 'Dev', 'Burman']\n",
            "Tokenized sentence: ['Amedeo', 'Minghi', '/', 'Franco', 'Califano/Rober', 'to', 'Conrado', '/', 'Edoardo', 'Vianello']\n",
            "Tokenized sentence: ['Victor', 'Font']\n",
            "Tokenized sentence: ['Shiro', 'Sagisu']\n",
            "Tokenized sentence: ['Map', 'Style', '/', 'Gift', 'Colyer']\n",
            "Tokenized sentence: ['Nick', 'Phoenix', '/', 'Thomas', 'Bergersen', '/', 'Petr', 'Pololán', 'ík']\n",
            "Tokenized sentence: ['Alexandre', 'Abescat', '/', 'Tom', 'Mokrane', '/', 'Jeonghyeon', 'Kim']\n",
            "Tokenized sentence: ['Sergio', 'Mendoza', '/', 'Sean', 'Rogers', '/', 'Quetzal', 'Guerrero']\n",
            "Tokenized sentence: ['Olaf', 'Dubber', '/', 'Copyright', 'Control', '/', 'Alexander', 'Grosskord']\n",
            "Tokenized sentence: ['Parrish', 'Smith', '/', 'Jimmy', 'Urine', '/', 'MC', 'Lyte']\n",
            "Tokenized sentence: ['Hebert', 'Vargas']\n",
            "Tokenized sentence: ['CA', 'JON', 'G', 'DE', 'OS', 'CAR', 'R', '/', 'CA', 'K', 'N', 'E', 'PPER', 'S', 'MARKUS', 'P', 'MAR', 'K', '/', 'CA', 'OLAYIWOLA', 'ALEXAND', 'ER', '/', 'CA', 'PLUG', 'WIM', '/', 'P', 'A', 'K', 'R', 'A', 'A', 'K', '&', 'SMAA', 'K']\n",
            "Tokenized sentence: ['Paul', 'Ablaze/Jonathan', 'Lefrancois', '-', 'Leduc']\n",
            "Tokenized sentence: ['Anübix']\n",
            "Tokenized sentence: ['Gilber', 'to', 'Hernande', 'z']\n",
            "Tokenized sentence: ['Kaabil']\n",
            "Tokenized sentence: ['Juan', 'Gay', 'tán', '/', 'Rafael', 'Carrión']\n",
            "Tokenized sentence: ['Amadeo', 'Mariscal', 'Lopez']\n",
            "Tokenized sentence: ['S.K', '.', 'Hine', '/', 'Michael', 'Casey', '/', 'Bill', 'Somerville', '-', 'Large']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/quiles', ',', 'anthony']\n",
            "Tokenized sentence: ['Jheynner', '/', 'Amina', 'Sahli', '/', 'Morgan', 'Avenue', '/', 'Copyright', 'Control', '/', 'Cloud', '9', 'Publishing', '/', 'Matthijs', 'de', 'Ronden', '/', 'Morien', 'van', 'der', 'Tang', '/', 'Jheynner', 'Argote', 'Frias', '/', 'Maktub', 'Music', 'Publishing']\n",
            "Tokenized sentence: ['Sihle', 'Nkosi']\n",
            "Tokenized sentence: ['Paulina', 'Villarreal', ',', 'Daniela', 'Villarreal', ',', 'Alejandra', 'Villarreal']\n",
            "Tokenized sentence: ['Chopsquad', 'DJ', ',', 'NLE', 'Choppa', ',', 'YNW', 'BSlime']\n",
            "Tokenized sentence: ['REEVES', 'TINA', '/', 'WHITE', 'CHRIS', '(', 'US', '2', '-', 'CNTRY', ')']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Davila', ',', 'Joseph']\n",
            "Tokenized sentence: ['Yuno', 'Miles']\n",
            "Tokenized sentence: ['Ron', 'Alan', 'Steele', 'IPI', '#', '335529165', '(', 'ASCAP', ')']\n",
            "Tokenized sentence: ['Pappu', 'Ravidas', '&', 'Sikandar', 'Kumar']\n",
            "Tokenized sentence: ['Phạm', 'Đình', 'Chương']\n",
            "Tokenized sentence: ['Troy', 'McDonald', '/', 'Devon', 'Charles', 'Thagard']\n",
            "Tokenized sentence: ['Aman', 'Tone']\n",
            "Tokenized sentence: ['MARTIN', 'EZ', 'D', 'A', 'NN', 'Y', 'JR']\n",
            "Tokenized sentence: ['Joggeswari', '&', 'Suman']\n",
            "Tokenized sentence: ['Cs', 'Hits', '/', 'A', 'Tur', 'ma', 'da', 'Tibatinha']\n",
            "Tokenized sentence: ['Lynn', 'Ahrens', '/', 'Jay', 'David', 'Saks', '/', 'Stephen', 'Flaherty']\n",
            "Tokenized sentence: ['James', 'Newton', 'Howard']\n",
            "Tokenized sentence: ['Thrilla', 'Montana', '/', 'Curtis', 'Carter']\n",
            "Tokenized sentence: ['Christopher', 'Peyton', '/', 'Benjamin', 'Pedersen']\n",
            "Tokenized sentence: ['พิ', 'ชญ', '์ส', 'ิน', 'ี', 'วี', 'ระ', 'สุ', 'ทธ', 'ิม', 'าศ']\n",
            "Tokenized sentence: ['Garrison', 'Keillor']\n",
            "Tokenized sentence: ['Raheem', 'Bowes']\n",
            "Tokenized sentence: ['Lee', 'Payne']\n",
            "Tokenized sentence: ['Alejandro', 'Medina']\n",
            "Tokenized sentence: ['Clelia', 'Raquel', 'Dondoglio/Luigi', 'Albertelli', '/', 'Enrico', 'Ric', 'cardi']\n",
            "Tokenized sentence: ['嘉垛', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Michael', 'Stipe/Mike', 'Mills', '/', 'Peter', 'Buck']\n",
            "Tokenized sentence: ['Rahul', 'Dev', 'Burman', '/', 'Anand', 'Bakshi']\n",
            "Tokenized sentence: ['James', 'Towlson', ',', 'Melina', 'Katherine', 'Bertsekas', ',', 'Nyla', 'Mosby', '&', 'Adrian', 'Acosta']\n",
            "Tokenized sentence: ['Chris', 'Webby']\n",
            "Tokenized sentence: ['Dmitry', 'Lysenko']\n",
            "Tokenized sentence: ['Talia', 'Billig', '/', 'Jos', 'é', 'James']\n",
            "Tokenized sentence: ['Matti', 'Vehmas']\n",
            "Tokenized sentence: ['Darrien', 'Sao']\n",
            "Tokenized sentence: ['Dinesh', 'Silgava', '&', 'Sita', 'Mali']\n",
            "Tokenized sentence: ['Darontez', 'May', 'o', ',', 'JAYSTONBEATS', ',', 'Mike', 'Mvjor', ',', 'MVAbeats', ',', 'Jaggerwerks', ',', 'King', 'Von', ',', 'Lil', 'Durk', ',', 'Booka', '600']\n",
            "Tokenized sentence: ['Соловьев', 'Вадим']\n",
            "Tokenized sentence: ['Jimmy', 'Javier', 'Castillo', 'Urbina']\n",
            "Tokenized sentence: ['RON', 'GOO', 'DWIN']\n",
            "Tokenized sentence: ['Cris', 'Velasco/Òscar', 'Senén']\n",
            "Tokenized sentence: ['Omo', 'Frenchie']\n",
            "Tokenized sentence: ['Mearge', 'Abate', '/', 'Monkhouse', '/', 'Ian', 'Dixon', '/', 'Peter', 'Harper']\n",
            "Tokenized sentence: ['ME', '&', 'M', 'Y', 'TOOTHBRUS', 'H']\n",
            "Tokenized sentence: ['Giulio', 'Caccini']\n",
            "Tokenized sentence: ['I', 'chiro', 'Yamazaki']\n",
            "Tokenized sentence: ['Witold', 'Lutosławski']\n",
            "Tokenized sentence: ['Sdot', 'Go']\n",
            "Tokenized sentence: ['Graham', 'Butt', '/', 'Allen', 'Adams']\n",
            "Tokenized sentence: ['Bart', 'Peeters']\n",
            "Tokenized sentence: ['Alain', 'Souchon']\n",
            "Tokenized sentence: ['W', 'I', 'L', 'L', 'BAKER', ',', 'CHRISTOPHER', 'S', 'KELLY', ',', 'PETE', 'WOODRU', 'FF']\n",
            "Tokenized sentence: ['Tory', 'Grace', 'Beridon', ',', 'Natalia', 'Taylar', '&', 'Caleb', 'Oczkowski']\n",
            "Tokenized sentence: ['Johann', 'Strauss']\n",
            "Tokenized sentence: ['Rockit', 'Music']\n",
            "Tokenized sentence: ['勝', '又', '隆', '一']\n",
            "Tokenized sentence: ['Lil', '30']\n",
            "Tokenized sentence: ['Phạm', 'Duy']\n",
            "Tokenized sentence: ['Joshua', 'Saric']\n",
            "Tokenized sentence: ['Niack']\n",
            "Tokenized sentence: ['Wim', 'Mertens']\n",
            "Tokenized sentence: ['Andy', 'Riley', '/', 'Jason', 'Cowley', '/', 'Gavin', 'Belton', '/', 'Steve', 'Walker', '/', 'Laurence', 'Ritchie']\n",
            "Tokenized sentence: ['Brenda', 'McMorrow', '/', 'B', 'McMorrow']\n",
            "Tokenized sentence: ['Ernie', 'Kovacs']\n",
            "Tokenized sentence: ['Романов', 'А', '.']\n",
            "Tokenized sentence: ['Enver', 'Sadinlija', '/', 'Zoran', 'Star', 'cevic']\n",
            "Tokenized sentence: ['Dave', 'Clark', '/', 'Howard', 'Lemon']\n",
            "Tokenized sentence: ['David', 'Gordon', '/', 'Steve', 'Gordon']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Pichasaca', ',', 'Jose']\n",
            "Tokenized sentence: ['Anh', 'Bằng']\n",
            "Tokenized sentence: ['奥井雅美', '/', 'Monta']\n",
            "Tokenized sentence: ['Pavel', 'Grecesqui']\n",
            "Tokenized sentence: ['Carl', 'Fischer', '/', 'Bill', 'William', 'Carey']\n",
            "Tokenized sentence: ['黃', '偉文', '/', '周柏豪']\n",
            "Tokenized sentence: ['A', 'BOERSTA', 'D', 'V', 'IDA', 'R', '/', 'A', 'RAC', 'S', 'GUN', 'TARS', '/', 'C', 'PAULS', 'RAI', 'MOND', 'S', '/', 'P', 'A', 'WALTER', 'S', '&', 'KAZHA']\n",
            "Tokenized sentence: ['Chris', 'Shern', '/', 'Ryan', 'Worthy', '/', 'Jeff', 'Goluszka', '/', 'Ryan', 'Caldwell', '/', 'Justin', 'Birchard']\n",
            "Tokenized sentence: ['Lucas', 'Ing', ',', 'Eliabe', 'Quexin', ',', 'Felipe', 'Vian', 'a', '&', 'Mathe', 'us', 'di', 'Padua']\n",
            "Tokenized sentence: ['GELES', 'SUAREZ', 'OMA', 'R', 'ANTON', 'IO']\n",
            "Tokenized sentence: ['Max', 'Oude', 'Weernink', '/', 'Leo', 'Roelandschap', '/', 'Jim', 'Taihut', 'tu', '/', 'Nils', 'Rondhuis']\n",
            "Tokenized sentence: ['Armando', 'Madeira']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Carpenter', ',', 'Jonny']\n",
            "Tokenized sentence: ['Michele', 'Lobaccaro', '/', 'Nabil', 'Salameh', '/', 'Alessandro', 'Pipino']\n",
            "Tokenized sentence: ['Brandon', 'Ellis']\n",
            "Tokenized sentence: ['Antelmo', 'Chávez', 'Valdovinos']\n",
            "Tokenized sentence: ['Disean', 'Victor']\n",
            "Tokenized sentence: ['Alber', 'to', 'Escáme', 'z', 'López']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Pennance', ',', 'Andy']\n",
            "Tokenized sentence: ['YPC', 'Samad/Trapalot', 'G5']\n",
            "Tokenized sentence: ['J3', '/', 'ID', 'Cleff', '/', 'Joshua', 'Roland']\n",
            "Tokenized sentence: ['Anly', 'Panouvong']\n",
            "Tokenized sentence: ['K', 'I', 'NG', '3', 'L', 'D', 'K']\n",
            "Tokenized sentence: ['Yeldri', 'Jimenez']\n",
            "Tokenized sentence: ['한경', '록']\n",
            "Tokenized sentence: ['Ira', 'Gershwin', '/', 'George', 'Gershwin']\n",
            "Tokenized sentence: ['Locho', 'Crack']\n",
            "Tokenized sentence: ['Tim', 'Janis']\n",
            "Tokenized sentence: ['نجيب', 'حنكش']\n",
            "Tokenized sentence: ['Ronnie', 'Urini', '/', 'Erich', 'Schindl', '/', 'Gary', 'Danner', '/', 'Christian', 'M', '.', 'Mayer']\n",
            "Tokenized sentence: ['Jordyn', 'Kane', '/', 'Cameron', 'Marygold', '/', 'Brayden', 'Deskins', '/', 'Colton', 'Fisher', '/', 'Benjamin', 'Roberts']\n",
            "Tokenized sentence: ['S', '.', 'Bruner', '/', 'D', '.', 'Albarn', '/', 'G', '.', 'Kurstin']\n",
            "Tokenized sentence: ['CA', 'RICCI', 'JASON', '/', 'PA', 'JASON', 'RICCI', '&', 'NEW', 'BLOOD']\n",
            "Tokenized sentence: ['Jules', 'De', 'Martino', '/', 'Katie', 'White']\n",
            "Tokenized sentence: ['Bobby', 'Sandimanie', 'III']\n",
            "Tokenized sentence: ['赵真']\n",
            "Tokenized sentence: ['Hanan', 'Gorenshtin']\n",
            "Tokenized sentence: ['Inaujee', 'Ison', '&', 'Justice', 'Gray']\n",
            "Tokenized sentence: ['Mitchell', 'Kinkade']\n",
            "Tokenized sentence: ['\\u200bhotaru']\n",
            "Tokenized sentence: ['Jake', 'Shears', ',', 'Michael', 'Cheever', ',', 'Boys', 'Noize']\n",
            "Tokenized sentence: ['Frank', 'Loesser']\n",
            "Tokenized sentence: ['Arturo', 'Venegas', 'Villa/Ricardo', 'Gutierrez', 'Alvare', 'z', '/', 'Victor', 'Gabriel', 'Rodarte', 'Cordova']\n",
            "Tokenized sentence: ['TRUE', 'JULIE']\n",
            "Tokenized sentence: ['Matthew', 'Jackson']\n",
            "Tokenized sentence: ['Tom', 'Brier']\n",
            "Tokenized sentence: ['Sterlin', 'Talley']\n",
            "Tokenized sentence: ['Jos', 'é', 'Luis', 'Moctezuma']\n",
            "Tokenized sentence: ['A', 'K', 'M', 'A', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Sergej', 'Vladimirovich', 'Terent', \"'ev\"]\n",
            "Tokenized sentence: ['Sean', 'Price']\n",
            "Tokenized sentence: ['Leroy', 'Junior', 'Russell', '&', 'Leah', 'Antoinette']\n",
            "Tokenized sentence: ['YoungBoy', 'Never', 'Broke', 'Again']\n",
            "Tokenized sentence: ['Joshua', 'Dixon']\n",
            "Tokenized sentence: ['Lê', 'Minh', 'Bằng']\n",
            "Tokenized sentence: ['Scorey']\n",
            "Tokenized sentence: ['Christian', 'Lindberg']\n",
            "Tokenized sentence: ['Jos', 'é', 'Luis', 'Garibay']\n",
            "Tokenized sentence: ['Lil', 'Agony', '&', 'Braver', 'Beats']\n",
            "Tokenized sentence: ['A', 'RASKE', 'PEN', 'GE', '/', 'CA', 'A', 'NDER', 'SEN', 'CHRIST', 'IAN', '/', 'P', 'A', 'KLUMBEN', '&', 'RASKE', 'PENGE']\n",
            "Tokenized sentence: ['Aleksandr', 'Shamaluev']\n",
            "Tokenized sentence: ['Rashe', 'Houston']\n",
            "Tokenized sentence: ['Karl', 'Frierson', '/', 'Pit', 'Baumgartner']\n",
            "Tokenized sentence: ['Dahlback', '(', 'Lunde', '&', 'Lunde', ')']\n",
            "Tokenized sentence: ['Alexander', 'Zurita', 'Sahuanga']\n",
            "Tokenized sentence: ['twin', 'janky', '800']\n",
            "Tokenized sentence: ['Касаткин', 'Ярослав', 'Олегович']\n",
            "Tokenized sentence: ['Moises', 'Santiago', 'López']\n",
            "Tokenized sentence: ['Jorge', 'Ibarra/Jenifer', 'Ibarra']\n",
            "Tokenized sentence: ['Rober', 'to', 'Valentin']\n",
            "Tokenized sentence: ['Daoud', '/', 'Rasool', 'Diaz', '/', 'Sool', 'Got', 'Hits', '/', 'Cordae', 'Dunston', '/', 'Raphael', 'Saadiq', '/', 'Terrace', 'Martin', '/', 'Daoud', 'Ayodele', 'Miles', 'Anthony']\n",
            "Tokenized sentence: ['2UGLi', '/', 'The', 'Premonist', '/', 'UGLi', 'Business', '/', 'Jacy', 'Chantiles']\n",
            "Tokenized sentence: ['Brittney', 'Mills', '&', 'Vick', 'i', 'Vox']\n",
            "Tokenized sentence: ['V', '.', 'Cartier']\n",
            "Tokenized sentence: ['-', '-/', 'שי', 'בכ', 'ר']\n",
            "Tokenized sentence: ['Big', 'Sad', '1900']\n",
            "Tokenized sentence: ['Atom', 'Heart', '/', 'Tetsu', 'Inoue', '/', 'Bill', 'Laswell']\n",
            "Tokenized sentence: ['Patrik', 'Jensen']\n",
            "Tokenized sentence: ['Herbainous', 'Music', '(', 'ASCAP', ')']\n",
            "Tokenized sentence: ['Christian', 'Blas', 'Rios']\n",
            "Tokenized sentence: ['Michael', 'David', 'Manring']\n",
            "Tokenized sentence: ['Chopsquad', 'DJ', ',', 'King', 'Von']\n",
            "Tokenized sentence: ['Shahin', 'Music', '/', 'Hadis', 'Dehghan', '/', 'Mostafa', 'Momeni']\n",
            "Tokenized sentence: ['Herby', 'Fontal']\n",
            "Tokenized sentence: ['Sebastien', 'Tellier']\n",
            "Tokenized sentence: ['Slimeroni', ',', 'A.R', '.', 'The', 'Mermaid']\n",
            "Tokenized sentence: ['Hans', '-', 'Joachim', 'Hespos']\n",
            "Tokenized sentence: ['Ünsal', 'Tüz', 'ün']\n",
            "Tokenized sentence: ['BAUS', 'S', 'CHRISTOPH/COLLAZO', 'MARIO', '/', 'LUV', 'I', 'NER', 'VIN', 'CENT']\n",
            "Tokenized sentence: ['Matrioshka', 'Brain', '/', 'Aleph', 'Oratorio', '/', 'Cain', 'Geist']\n",
            "Tokenized sentence: ['Willy', 'William']\n",
            "Tokenized sentence: ['A', '.', 'Ramlee', '/', 'Ezra', 'Kong']\n",
            "Tokenized sentence: ['Funwi']\n",
            "Tokenized sentence: ['Antonio', 'Vega']\n",
            "Tokenized sentence: ['GlockBoy', 'BoBo']\n",
            "Tokenized sentence: ['Les', 'Hurdle', '/', 'Madeline', 'Bell', '/', 'Kathleen', 'Poppy', '/', 'Themes', 'International', 'Music', 'Ltd', '.']\n",
            "Tokenized sentence: ['Johann', 'Sebastian', 'Bach']\n",
            "Tokenized sentence: ['Miky', 'Mendozza', ',', 'Bolela', '&', 'Oscar', 'Mont']\n",
            "Tokenized sentence: ['Mambo/Hurriganes', '/', 'Jim', 'Kovalzskij']\n",
            "Tokenized sentence: ['Laurent', 'Ey', 'quem']\n",
            "Tokenized sentence: ['[', 'traditional', ']', '/John', 'Rutter']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Thomas', ',', 'Christian', 'Allen']\n",
            "Tokenized sentence: ['Son', 'Seong', 'Rak']\n",
            "Tokenized sentence: [\"chAN's\", '/', 'chAN', '’', 's', '/', 'earattack', '/', 'Don', 'Baller', '/', 'LEE', 'DAE', 'HWI', '/', 'James', '‘', 'Boy', 'Matthews', '’', 'Norton']\n",
            "Tokenized sentence: ['Teej', '/', 'Objectiv']\n",
            "Tokenized sentence: ['Magnus', 'Langenohl/Philipp', 'Monney']\n",
            "Tokenized sentence: ['Dylan', 'Chenfeld', '/', 'Noah', 'Chenfeld']\n",
            "Tokenized sentence: ['AR', 'HAAVISTO', 'JANNE', 'PEKKA', '/', 'AR', 'LANKINEN', 'MIKKO', 'JUHANI', '/', 'AR', 'NYMAN', 'TOM', 'THEODOR', '/', 'AR', 'PITSINKI', 'MATTI', 'KAARLO', '/', 'C', 'LANKINEN', 'MIKKO', 'JUHANI', '/', 'PA', 'ROCK', 'YHTYE', 'LAIKA', '&', 'THE', 'COSMONA']\n",
            "Tokenized sentence: ['مسلم', 'الوائلي']\n",
            "Tokenized sentence: ['Jürgen', 'Engler', '/', 'Ralf', 'Dörper', '/', 'Lee', 'Altus']\n",
            "Tokenized sentence: ['Artur', 'Mena']\n",
            "Tokenized sentence: ['Oxius', '/', 'Incurzion', 'Audio']\n",
            "Tokenized sentence: ['Kory', 'Brown', ',', 'Cory', 'Polizzi', '&', 'Nathaniel', 'Welch']\n",
            "Tokenized sentence: ['Bee', 'Gees', '/', 'Barry', 'Gibb', '/', 'Robin', 'Gibb']\n",
            "Tokenized sentence: ['THAI', 'KHANG']\n",
            "Tokenized sentence: ['GUN', 'SHXT', 'GUN', 'SHXT']\n",
            "Tokenized sentence: ['Amy', 'Koïta']\n",
            "Tokenized sentence: ['Roger', 'Wanamo/Hirok', 'i', 'Kikut', 'a', '/', 'Nobuo', 'Uematsu', '/', 'Susanna', 'Pölt', '/', 'Y', 'oko', 'Shimomura/Yasunori', 'Mitsuda']\n",
            "Tokenized sentence: ['mol', '$', 'mol', '$']\n",
            "Tokenized sentence: ['Cochise']\n",
            "Tokenized sentence: ['Johnny', 'Cash']\n",
            "Tokenized sentence: ['Sammy', 'Serious']\n",
            "Tokenized sentence: ['Adel', 'Osaki/Tibeau', 'Denamur']\n",
            "Tokenized sentence: ['Minit', '/', 'AVOK', 'ID']\n",
            "Tokenized sentence: ['Lil', 'Nuu']\n",
            "Tokenized sentence: ['Tego', 'Calder', 'ón']\n",
            "Tokenized sentence: ['Tony', 'Hendrik', '/', 'Bernd', 'Meinunger', '/', 'Andreas', 'Martin', 'Krause', '/', 'Radio', '-', 'Tele', '-', 'Music', 'GmbH', '/', 'Coconut', 'Music', 'Limited', '&', 'Co.', 'KG']\n",
            "Tokenized sentence: ['PHIL', 'SEUN', 'G', 'BUL', 'P', 'A', 'E']\n",
            "Tokenized sentence: ['Anthony', 'Tubbs']\n",
            "Tokenized sentence: ['Domo', 'Genesis', ',', 'Action', 'Bronson', ',', 'Earl', 'Sweatshirt', ',', 'SpaceGhostPurrp', ',', 'The', 'Alchemist']\n",
            "Tokenized sentence: ['Shi-/', 'Angelo', 'Taylor']\n",
            "Tokenized sentence: ['Joel', 'Trujillo', ',', 'Brandon', 'Trejo', '&', 'Anthony', 'Valdez']\n",
            "Tokenized sentence: ['Samuel', 'Santos']\n",
            "Tokenized sentence: ['Bruce', 'Bickerton']\n",
            "Tokenized sentence: ['Manuel', 'S', '.', 'Acuna']\n",
            "Tokenized sentence: ['Charlie', 'Puth', '/', 'Jake', 'Torrey', '/', 'Jacob', 'Kasher', '/', 'Blake', 'Slatkin']\n",
            "Tokenized sentence: ['C.', 'BERMAN', '/', 'F', '.', 'BERMAN', '/', 'A', '.', 'CREMERS', '/', 'Whey', 'Cooler', '/', 'MICHEL', 'CYWIE', '/', 'Gema', 'Obo', 'Gema', '/', 'DIDIER', 'BARBELIVIEN', '/', 'Music', 'Sales', 'Corporation', 'Obo', 'Premier', 'Music', '/', 'Warner', '-', 'Tamerlane', 'Pub', 'Corp.', 'Obo', 'Edition', 'Shark', 'Media', 'Songs']\n",
            "Tokenized sentence: ['Faheem', 'Najm', '/', 'J', '.', 'Robinson', '/', 'Dee', 'Jay', 'Dana', '/', 'Thomas', 'Tom', 'Cat', 'Bennett']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Glueckauf', ',', 'Aaron']\n",
            "Tokenized sentence: ['Sun', 'Drug']\n",
            "Tokenized sentence: ['Anh', 'Quân']\n",
            "Tokenized sentence: ['David', 'Machalicky', '/', 'Tom', 'áš', 'Rothschedl/Damian', 'Kucera', '/', 'Jan', 'Kucera']\n",
            "Tokenized sentence: ['Jason', 'Fed', 'ze', '&', 'Fed', 'ze']\n",
            "Tokenized sentence: ['Мамедов', 'Бахтияр', 'Гусейнулы', ',', 'Сатыбалдиев', 'Тамерлан', '&', 'Алексеев', 'Анатолий']\n",
            "Tokenized sentence: ['James', 'Hamley', '/', 'Hugh', 'Lake', '/', 'Brae', 'Luafalealo']\n",
            "Tokenized sentence: ['Ani', 'DiFranco']\n",
            "Tokenized sentence: ['Sirpy']\n",
            "Tokenized sentence: ['Shankar', '–', 'Ehsaan–Loy', '/', 'Javed', 'Akhtar']\n",
            "Tokenized sentence: ['Brasse', 'Brännström', '/', 'Magnus', 'Härenstam']\n",
            "Tokenized sentence: ['Locic', '&', 'INgra']\n",
            "Tokenized sentence: ['Bruce', 'Springsteen', '/', 'Warren', 'Zevon']\n",
            "Tokenized sentence: ['David', 'Line', '/', 'Kevin', 'Hendrick', '/', 'Charles', 'Macleod', '/', 'Caroline', 'Banks']\n",
            "Tokenized sentence: ['Harmonize']\n",
            "Tokenized sentence: ['Pfive', 'México', '/', 'Pfive', 'Entertainmet']\n",
            "Tokenized sentence: ['Fernan', 'do', 'Bejarano', 'Pazos']\n",
            "Tokenized sentence: ['Blxck', '/', 'Marcos', 'Aurelio', 'Miranda', 'Nogueira', 'Neto']\n",
            "Tokenized sentence: ['O', '.', 'Farres']\n",
            "Tokenized sentence: ['Eddie', 'Hult', 'q', 'vist']\n",
            "Tokenized sentence: ['Twisty', 'P']\n",
            "Tokenized sentence: ['ALLEN', 'R', 'E', 'Y', 'NO', 'LDS']\n",
            "Tokenized sentence: ['Gordon', 'Mote']\n",
            "Tokenized sentence: ['Georges', 'Delerue']\n",
            "Tokenized sentence: ['Sean', 'Price']\n",
            "Tokenized sentence: ['Jesse', 'Keeler', '/', 'Alex', 'Puod', 'ziukas']\n",
            "Tokenized sentence: ['B.', 'Mason', '/', 'L', '.', 'Reed']\n",
            "Tokenized sentence: ['GAR', 'R', 'Y', 'R', 'REED']\n",
            "Tokenized sentence: ['Freddie', 'Dredd', ',', 'Haarper']\n",
            "Tokenized sentence: ['Keith', 'Kenniff/Hollie', 'Kenniff']\n",
            "Tokenized sentence: ['Tim', 'Wilke', '/', 'The', 'Cratez', '/', 'David', 'Kraft', '/', 'Said', 'Eroglu', '/', 'Saven', 'Mus', 'iq', '/', 'Davut', 'Althundal']\n",
            "Tokenized sentence: ['J.', 'Hershey', '/', 'D', '.', 'Swander']\n",
            "Tokenized sentence: ['BAZLI', 'UNIC', '/', 'COPYRIGHT', 'CONTROL']\n",
            "Tokenized sentence: ['Brad', 'Carroll', '/', 'Peter', 'Sham']\n",
            "Tokenized sentence: ['Gunna', ',', 'Florian', '“', 'Flo', '”', 'Ongonga', ',', 'Angelo', 'Ferraro']\n",
            "Tokenized sentence: ['Erin', 'Hannah', 'Fein', '/', 'Brett', 'Mat', 'thew', 'Sanderson', '/', 'Tristan', 'Alexander', 'Wraight']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Portis', ',', 'Deundraeus']\n",
            "Tokenized sentence: ['Screamin', '’', 'Jay', 'Hawkins']\n",
            "Tokenized sentence: ['John', 'Barry', '/', 'Anthony', 'Newley', '/', 'Leslie', 'Bricusse']\n",
            "Tokenized sentence: ['CA', 'SIMMONDS', 'MAURICE', 'NATHAN', '/', 'CA', 'SMITH', 'LEON', '/', 'CA', 'TURNER', 'BOBBY', 'BERNARD', 'JR', '/', 'PA', 'CAP', '1', '&', 'VERSE', 'SIMMONDS']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Hill', ',', 'Brandon', 'GM']\n",
            "Tokenized sentence: ['WEST', 'GROUN', 'D', '/', 'minatok', 'u']\n",
            "Tokenized sentence: ['YOUM', 'A', 'N']\n",
            "Tokenized sentence: ['Francesco', 'Romei']\n",
            "Tokenized sentence: ['Tracy', 'Lipp', '/', 'Mr', '.', 'Lordi']\n",
            "Tokenized sentence: ['Mad', 'Mix', '&', 'JP']\n",
            "Tokenized sentence: ['Eric', 'Freeman', '&', 'Chris', 'Clark']\n",
            "Tokenized sentence: ['Tahir', 'Tekin', '/', 'K', 'I', 'NGS', 'MEN', 'GROU', 'P', 'LLC']\n",
            "Tokenized sentence: ['Adrian', 'o', 'Banchier', 'i']\n",
            "Tokenized sentence: ['Dream', 'On', 'Dreamer']\n",
            "Tokenized sentence: ['Marco', 'Carpentier', 'i']\n",
            "Tokenized sentence: ['Liam', 'McCay']\n",
            "Tokenized sentence: ['Joe', 'Leytrick', '&', 'Mathew', 'McGuire']\n",
            "Tokenized sentence: ['Viju', 'Shah', '/', 'Sameer']\n",
            "Tokenized sentence: ['Sylvain', 'Tack', '/', 'Paul', 'Severs', '/', 'Johan', 'de', 'Graeve']\n",
            "Tokenized sentence: ['Nox', 'Beatz', '/', 'Chris', 'Webby', '/', 'Jordan', 'Devin', '/', 'JP', 'On', 'Da', 'Track', '/', 'Brian', 'Joseph', 'Eisner', '/', 'Jake', 'Anthony', 'Procanik', '/', 'Dashorn', 'Andrew', 'Whitehead', '/', 'Kyle', 'Robert', 'Crosby', 'English']\n",
            "Tokenized sentence: ['Txus', 'di', 'Fellatio/', 'Manuel', 'Seoane']\n",
            "Tokenized sentence: ['Jose', 'Luis', 'Cruz', 'Cruz']\n",
            "Tokenized sentence: ['Ge', 'of', 'frey', 'Ell', 'wood']\n",
            "Tokenized sentence: ['Chester', 'Bennington', '/', 'Rob', 'Bourdon', '/', 'Brad', 'Delson', '/', 'Dave', 'Farrell', '/', 'Mike', 'Shinoda']\n",
            "Tokenized sentence: ['David', 'Travis', 'Edwards', '/', '古川毅']\n",
            "Tokenized sentence: ['Kamar', 'Forrest']\n",
            "Tokenized sentence: ['Mohamed', 'El', 'Kayaty', '-', 'مح', 'مد', 'ال', 'قا', 'يا', 'تي']\n",
            "Tokenized sentence: ['Skull', 'Diver', '&', 'Thomas', 'Gardell']\n",
            "Tokenized sentence: ['コト', 'リン', 'ゴ', '/', 'あい', 'はら', 'ひろ', 'ゆき']\n",
            "Tokenized sentence: ['Rhomar', 'Jessy', ',', 'Kyle', 'Yumang', ',', 'Aiden', 'Torres', '&', 'J.Raul', 'Garcia']\n",
            "Tokenized sentence: ['Paul', 'Gam', '/', 'Ezequiel', 'Espino']\n",
            "Tokenized sentence: ['Marcell', 'Ulysse']\n",
            "Tokenized sentence: ['Wanchaloem', 'Suepain']\n",
            "Tokenized sentence: ['Tom', 'Cardy']\n",
            "Tokenized sentence: ['Ness', 'Beats']\n",
            "Tokenized sentence: ['beatsbyblg1', '/', 'MoneyManLegit']\n",
            "Tokenized sentence: ['FN', 'DaDealer', ',', 'Lil', 'ManMan', ',', 'Slimelife', 'Shawty']\n",
            "Tokenized sentence: ['David', 'Said']\n",
            "Tokenized sentence: ['Ма', 'йс', 'тр', 'ук', 'Ма', 'кс', 'им']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Jenkins', ',', 'Kendall', 'Damonta']\n",
            "Tokenized sentence: ['DJ', 'Intan', 'Novela', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Skully', 'Wit', 'Da', 'k', '&', 'Pdg', 'Jugg']\n",
            "Tokenized sentence: ['Michael', 'Scholz', '&', 'Detlef', 'Wiedeke']\n",
            "Tokenized sentence: ['HOLLAND', 'NIC', 'K']\n",
            "Tokenized sentence: ['Sebastian', 'Levermann', '&', 'Niels', 'Löffler']\n",
            "Tokenized sentence: ['Son', 'Jichang', '/', 'Lee', 'Taeseop', '/', 'Kim', 'Seongmyeon']\n",
            "Tokenized sentence: ['Matthías', 'Jochumsson', '/', 'Sveinbjörn', 'Sveinbjörnsson']\n",
            "Tokenized sentence: ['Daniel', 'Gutierrez', '/', 'Jacobo', 'Vélez', 'Mesa', '/', 'Juan', 'Carlos', 'Arrechea']\n",
            "Tokenized sentence: ['Gerry', '&', 'Michael', 'Thompson']\n",
            "Tokenized sentence: ['Christian', 'Marsac', '/', 'Addie', 'Brik']\n",
            "Tokenized sentence: ['Morgan', 'Heritage']\n",
            "Tokenized sentence: ['Nawaraj', 'Lamsal', '/', 'Surendra', 'Man', 'Singh']\n",
            "Tokenized sentence: ['Bauhaus']\n",
            "Tokenized sentence: ['Irving', 'Batalla', 'Batalla']\n",
            "Tokenized sentence: ['Surinder', 'Sehaj', '/', 'Dinesh', 'Deepak']\n",
            "Tokenized sentence: ['BART', 'BUTLER', '/', 'JON', 'PARDI', '/', 'LUKE', 'LAIRD', '/', 'RHETT', 'AKINS']\n",
            "Tokenized sentence: ['Georgian', 'a', 'Lobont']\n",
            "Tokenized sentence: ['Mart', 'ín', 'Rodriguez']\n",
            "Tokenized sentence: ['BEGGI', 'NS', 'ALEXANDER', 'JAMES', '/', 'W', 'ILSON', 'KELSEY', 'ERIN']\n",
            "Tokenized sentence: ['Smatt', 'Sertified', ',', 'Aaron', 'Melloul', '&', 'Alexander', 'Parsegov']\n",
            "Tokenized sentence: ['Sébastien', 'Gourseyrol']\n",
            "Tokenized sentence: ['Albert', 'React']\n",
            "Tokenized sentence: ['Byron', 'Stingily', '/', 'Marshall', 'Jefferson', '/', 'Eric', 'Welton']\n",
            "Tokenized sentence: ['Dominik', 'Przyby', 'łek']\n",
            "Tokenized sentence: ['leeg', 'of']\n",
            "Tokenized sentence: ['Josh', 'Homme']\n",
            "Tokenized sentence: ['前', '田克樹', '/', '藤林', '聖子']\n",
            "Tokenized sentence: ['Sir', 'Edward', 'Elgar', ',', 'Fran', 'z', 'Joseph', 'Haydn', '&', 'DMK', '(', 'DAE-MYUN', 'G', 'K', 'A', 'NG', ')']\n",
            "Tokenized sentence: ['Nguy', 'ễn', 'Duy', 'An']\n",
            "Tokenized sentence: ['CA', 'GRAN', 'BERG', 'MARCUS', 'VILHELM', '/', 'CA', 'N', 'Y', 'BERG', 'TOMA', 'S', '/', 'P', 'A', 'ARNE', 'ALLIGA', 'TOR', '&', 'DJUNGELTRUM', 'MA']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Ault', ',', 'Daniel']\n",
            "Tokenized sentence: ['Markus', 'Mustonen', '/', 'Joakim', 'Berg', '/', 'Sami', 'Sirviö', '/', 'Harri', 'Mänty', '/', 'Martin', 'Sköld']\n",
            "Tokenized sentence: ['Felipe', 'Lima', '/', 'Gustavo', 'Carvalho/Caelu']\n",
            "Tokenized sentence: ['ScrumbleMan', ',', 'BabyTron']\n",
            "Tokenized sentence: ['Connor', 'Donovan', '/', 'Vasant', 'Sundaresan']\n",
            "Tokenized sentence: ['أحمد', 'خضر', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Jaleen', 'Mathis']\n",
            "Tokenized sentence: ['RdRay']\n",
            "Tokenized sentence: ['Charlie', 'Starr']\n",
            "Tokenized sentence: ['Pepe', 'Castillo']\n",
            "Tokenized sentence: ['Hale', 'Reeves']\n",
            "Tokenized sentence: ['DIR', 'EN', 'GREY', '/', '京']\n",
            "Tokenized sentence: ['Ian', 'Gillan', '/', 'Ritchie', 'Blackmore', '/', 'Roger', 'Glover']\n",
            "Tokenized sentence: ['John', 'Dirne/Gyo', 'Kretz', '/', 'Catalina', 'Schweighauser', '/', 'Tom', 'Dekkers']\n",
            "Tokenized sentence: ['Simon', 'Phoumsavan']\n",
            "Tokenized sentence: ['Тк', 'ач', 'О', './', 'Taisia', 'Povaliy', '/', 'Бу', 'де', 'йч', 'ук', 'В', '.', 'П', '.']\n",
            "Tokenized sentence: ['John', 'Farrar']\n",
            "Tokenized sentence: ['Manuela', 'Various', '/', 'Thomas', 'De', 'Quincey']\n",
            "Tokenized sentence: ['Juan', 'Bautista', 'Leonardo']\n",
            "Tokenized sentence: ['D', '’', 'Ban', 'j', ',', 'Wande', 'Coal']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Blac', ',', 'Scoota']\n",
            "Tokenized sentence: ['Greg', 'Phillips', '/', 'Jesse', 'Robinson', '/', 'Kishaun', 'Bailey']\n",
            "Tokenized sentence: ['Chris', 'Martin', ',', 'Guy', 'Berryman', ',', 'Jonny', 'Buckland', ',', 'Will', 'Champion', ',', 'Seum', 'Dero', ',', 'Marin', 'Hoxha']\n",
            "Tokenized sentence: ['Pete', 'Townshend']\n",
            "Tokenized sentence: ['DJ', 'YASBI', 'RIMEX', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Marc', 'Van', 'DeKeere']\n",
            "Tokenized sentence: ['Appu', '&', 'Traditional']\n",
            "Tokenized sentence: ['Nam', 'Je', 'Hoon', '/', 'Kim', 'Yoon', 'Myung', '/', 'JUN', 'IK']\n",
            "Tokenized sentence: ['Swede', '/', 'Lee', 'Hunter', '/', 'Marvin', 'Flowberg', '/', 'Ulterium', 'Records', '/', 'Mike', 'Cameron', 'Force']\n",
            "Tokenized sentence: ['D.', 'Sardy', '/', 'Andy', 'Bews', '/', 'Colin', 'Doran', '/', 'Andy', 'Gilmour', '/', 'Paul', 'Townsend', '/', 'Lawrence', 'Hibbitt']\n",
            "Tokenized sentence: ['Stella', 'Jang', '/', 'Grizzly']\n",
            "Tokenized sentence: ['Jean', '-', 'Philippe', 'Rameau']\n",
            "Tokenized sentence: ['Mad', 'Solutions', 'LLC', '/', 'Songtrust', '/', 'Paul', 'Nonso', 'Okoye', 't', '/', 'as', 'Rudeboy', '/', 'Christopher', 'Anthony', 'Eko', 't', '/', 'as', 'Chrisstrings']\n",
            "Tokenized sentence: ['Joe', 'Palmer']\n",
            "Tokenized sentence: ['Joseph', 'Kosma/Johnny', 'Mercer', '/', 'Jacques', 'Prévert']\n",
            "Tokenized sentence: ['Victor', 'Hobbs', '&', 'Alexander', 'Couret']\n",
            "Tokenized sentence: ['A', 'SPRUD', 'ZS', 'VALTER', 'S', '/', 'AR', 'EIHVALDS', 'UG', 'IS', '/', 'AR', 'LACIS', 'JANIS', '/', 'AR', 'SPRUD', 'ZS', 'VALTER', 'S', '/', 'AR', 'ZV', 'IED', 'RIS', 'ATIS', '/', 'C', 'SPRUD', 'ZS', 'VALTER', 'S', '/', 'P', 'A', 'FLAME', '&', 'THE', 'ROLLTONE', 'S']\n",
            "Tokenized sentence: ['Daryl', 'Hall', '/', 'Sara', 'Allen', '/', 'John', 'Oates']\n",
            "Tokenized sentence: ['Rub', 'én', 'Hidalgo']\n",
            "Tokenized sentence: ['Giovanni', 'Pacini']\n",
            "Tokenized sentence: ['Randle', 'whisler']\n",
            "Tokenized sentence: ['DEREK', 'MURPH', 'Y', '/', 'MARCUS', 'ALBALADEJ', 'O', '/', 'MARIO', 'ALFRED', 'O', 'COLLAZO', '/', 'UNA', 'FFILIATED', 'WR', 'I', 'TER', '#', '3']\n",
            "Tokenized sentence: ['Dario', 'Engels']\n",
            "Tokenized sentence: ['Luigi', 'Denza', '/', '田中', '星児']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Robinson', ',', 'Christopher', 'CODY']\n",
            "Tokenized sentence: ['SMK', 'EXCLSV', ',', 'Y', 'N', 'W', 'Melly']\n",
            "Tokenized sentence: ['Ennio', 'Morricone']\n",
            "Tokenized sentence: ['A', 'GEORGE', 'STEFAN', '/', 'C', 'SCHOENBERG', 'ARNOLD', '/', 'PA', 'COPYRIGHT', 'UNTIL', 'END', '2015']\n",
            "Tokenized sentence: ['Кирилл', 'Игоревич', 'Незборецкий']\n",
            "Tokenized sentence: ['CA', 'BURTON', 'CATHY', 'LOUISE', '/', 'CA', 'GERASIMOV', 'KONSTANTIN', '/', 'CA', 'NITZAN', 'RAZ', '/', 'PA', 'COSTA', '&', 'CATHY', 'BURTON']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Harris', ',', 'David', '/', 'Qualls', ',', 'David']\n",
            "Tokenized sentence: ['BRYAN', '(', 'NM', ')', 'KONG', '/', 'BRYCE', '(', 'NM', ')', 'ZIALCITA', '/', 'ISA', '(', 'NM', ')', 'GARCIA', '/', 'SARAH', '(', 'NM', ')', 'MARCO', '/', 'SIOPAO', '(', 'NM', ')', 'CHUA']\n",
            "Tokenized sentence: ['Chevyano', '/', 'Marvelous', 'Jay']\n",
            "Tokenized sentence: ['Angel', 'Yerai', 'Ramirez', 'Rodriguez']\n",
            "Tokenized sentence: ['Patrik', 'Jarlestam']\n",
            "Tokenized sentence: ['SANCHEZ', 'CUCO']\n",
            "Tokenized sentence: ['Raju', 'Punjabi', '/', 'Vikas', 'Gaana', 'Bajana']\n",
            "Tokenized sentence: ['Derek', 'T', '.', 'Triplett']\n",
            "Tokenized sentence: ['Kingsley', 'O', '.']\n",
            "Tokenized sentence: ['Sion', 'Thomas']\n",
            "Tokenized sentence: ['Thomas', 'Joseph', 'Terry', '/', 'Bryan', 'L.', 'Winchester', '/', 'Courtland', 'Urbano', '/', 'Jason', 'Emmanuel', 'Petty']\n",
            "Tokenized sentence: ['Slan', 'k']\n",
            "Tokenized sentence: ['Adam', 'Williams', '-', 'Walters']\n",
            "Tokenized sentence: ['Kameron', 'Michael', 'Fair']\n",
            "Tokenized sentence: ['Simon', 'Neale']\n",
            "Tokenized sentence: ['Tyler', 'Joseph']\n",
            "Tokenized sentence: ['Arnold', 'Schönberg/Hermann', 'von', 'Lingg']\n",
            "Tokenized sentence: ['Ritchie', 'Blackmore', '/', 'Candice', 'Night']\n",
            "Tokenized sentence: ['Davydenko', 'Dmitry', '/', 'Zhelnov', 'Sergey', 'Vladimirovich', '/', 'Egor', 'Bashkov']\n",
            "Tokenized sentence: ['Angelo', 'Luis', 'S', 'del', 'Carmen']\n",
            "Tokenized sentence: ['Robert', 'Villanueva', ',', 'Austin', 'Mahone']\n",
            "Tokenized sentence: ['PARTYNEXTDOOR', ',', 'DJ', 'Candlestick', ',', 'OG', 'Ron', 'C']\n",
            "Tokenized sentence: ['Samir', 'Sadaoui']\n",
            "Tokenized sentence: ['Begena']\n",
            "Tokenized sentence: ['Danny', 'Berrios', ',', 'Dylan', 'Conrique', ',', 'Sarah', 'Emily', 'Berrios']\n",
            "Tokenized sentence: ['Andrey', 'Babaev', '&', 'İslam', 'Seferli']\n",
            "Tokenized sentence: ['강우', '경', '/', '박현', '암']\n",
            "Tokenized sentence: ['John', 'Jenkins']\n",
            "Tokenized sentence: ['Harvey', 'Christo', '/', 'Steven', 'Wirawan', '/', 'Juan', 'Mandagie']\n",
            "Tokenized sentence: ['Sinne', 'Eeg']\n",
            "Tokenized sentence: ['Nicola', 'Francesco', 'Hay', 'm', '/', 'George', 'Frideric', 'Handel']\n",
            "Tokenized sentence: ['TENAGLIA', 'DANIEL']\n",
            "Tokenized sentence: ['Bjørn', 'Torske/Ole', 'Mjøs', '/', 'Rune', 'Lindb', 'æk']\n",
            "Tokenized sentence: ['Stanislaw', 'Soyka', '/', 'William', 'Shakespeare']\n",
            "Tokenized sentence: ['Gur', 'meet', 'Singh', '&', 'Happy', 'Raikoti']\n",
            "Tokenized sentence: ['Rados', '&', '#', '322', ';', 'aw', 'Merta']\n",
            "Tokenized sentence: ['Hustinder']\n",
            "Tokenized sentence: ['Bo', 'Peep']\n",
            "Tokenized sentence: ['Mani', 'Bhawanigarh']\n",
            "Tokenized sentence: ['Surapun', 'Chanwichananun']\n",
            "Tokenized sentence: ['卯花', 'ロク', '(', 'Ukaroku', ')']\n",
            "Tokenized sentence: ['Ian', 'Vanek']\n",
            "Tokenized sentence: ['Philip', 'Strand', '/', 'Yaroslav', 'Polikarpov', '/', 'Victoria', 'Alkin', '/', 'Aino', 'Jawo', '/', 'Caroline', 'Hjelt', '/', 'Emelie', 'Eriksson', '/', 'Louice', 'Leveau']\n",
            "Tokenized sentence: ['Filipe', 'Ret', ',', 'MC', 'Cabelinho', ',', 'MC', 'Maneirinho', ',', 'Ian', 'Gir', 'ão']\n",
            "Tokenized sentence: ['Nimesh', 'S.', 'Patel']\n",
            "Tokenized sentence: ['Javiielo', ',', 'Anubiis', ',', 'Nekxum']\n",
            "Tokenized sentence: ['Jean', '-', 'Paul', 'Genr', 'é']\n",
            "Tokenized sentence: ['Chico', 'Amaral/Samuel', 'Ros', 'a']\n",
            "Tokenized sentence: ['William', 'Lincoln', 'Knauer', '/', 'Jonathan', 'Hase', 'James', '/', 'Philip', 'Abram', 'Dickey']\n",
            "Tokenized sentence: ['Aloy', ',', 'Josias', '(', 'NLD', ')', ',', 'Young', 'T', '(', 'Producer', ')', ',', '24Heavy', ',', 'YTB', 'Trench', ',', 'Future', ',', 'Young', 'Thug', ',', 'Unfoonk']\n",
            "Tokenized sentence: ['KiSondrea', 'Bradford']\n",
            "Tokenized sentence: ['Max', 'Raabe/Achim', 'Hagemann', '/', 'Johannes', 'Ernst']\n",
            "Tokenized sentence: ['David', 'Clewett', '/', 'Savan', 'Kotecha', '/', 'Ivar', 'Lisinski']\n",
            "Tokenized sentence: ['Stephan', 'Haeri', '/', 'Charles', 'Dumont', '/', 'Télépopmusik', '/', 'Angela', 'McCluskey', '/', 'Christophe', 'Hetier']\n",
            "Tokenized sentence: ['Adjustor', 'BG', '18']\n",
            "Tokenized sentence: ['Jon', 'Maguire', '/', 'Corey', 'Sanders', '/', 'Calum', 'Scott']\n",
            "Tokenized sentence: ['Squirt', 'Kelly']\n",
            "Tokenized sentence: ['Desmond', 'Drummond']\n",
            "Tokenized sentence: ['CA', 'CLAESSEN', 'BART', 'J', 'H', '/', 'PA', 'COPYRIGHT', 'CONTROL']\n",
            "Tokenized sentence: ['Disharmonic', 'Orchestra']\n",
            "Tokenized sentence: ['樫田', 'レオ', '/', 'PMM', 'K']\n",
            "Tokenized sentence: ['梶浦', '由', '記']\n",
            "Tokenized sentence: ['E', '.', 'Harris']\n",
            "Tokenized sentence: ['A', 'ViVA', ',', 'Jacob', 'Skinkle']\n",
            "Tokenized sentence: ['Karanvir', 'Singh', ',', 'Simar', 'Panag', '&', 'Kulvir', 'Singh', 'Hans']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Livingston', ',', 'Damion', 'Dwayne']\n",
            "Tokenized sentence: ['Bryan', 'Merasty']\n",
            "Tokenized sentence: ['巫啟', '賢']\n",
            "Tokenized sentence: ['Andy', 'Quin']\n",
            "Tokenized sentence: ['Felix', 'Mendelssohn-Bartholdy']\n",
            "Tokenized sentence: ['Lee', 'SooHyun']\n",
            "Tokenized sentence: ['Cruz', 'de', 'Jes', 'ús', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Anthony', 'McGhee']\n",
            "Tokenized sentence: ['Synthwave', 'Goose']\n",
            "Tokenized sentence: ['Sdot', 'Go', ',', 'Jay', 'Hound']\n",
            "Tokenized sentence: ['Ezequiel', 'Espino', '/', 'Paul', 'Gam']\n",
            "Tokenized sentence: ['Ngân', 'Giang/Anh', 'Bằng']\n",
            "Tokenized sentence: ['Ng', 'ô', 'Thụy', 'Miên']\n",
            "Tokenized sentence: ['Kenneth', 'Manns']\n",
            "Tokenized sentence: ['Javier', 'Varela']\n",
            "Tokenized sentence: ['六', 'ツ', '見', '純代', '/', '菊田', '大介']\n",
            "Tokenized sentence: ['William', 'P.', 'MacKay', '&', 'John', 'J', '.', 'Husband']\n",
            "Tokenized sentence: ['Natalia', 'Ger', 'manou']\n",
            "Tokenized sentence: ['Anne‐', 'Marie', '/', 'Nat', 'Dunn', '/', 'Marshmello']\n",
            "Tokenized sentence: ['Majzoub', 'Tabrizi']\n",
            "Tokenized sentence: ['Dotan', 'Harpenau', '/', 'Neil', 'Ormandy', '/', 'Martin', 'Wave']\n",
            "Tokenized sentence: ['Andrew', 'Lloyd', 'Webber', '/', 'Tim', 'Rice']\n",
            "Tokenized sentence: ['Shinpei', 'Nakayama', '/', 'Uj', 'yo', 'Noguchi']\n",
            "Tokenized sentence: ['Noreiga', 'Jonathan']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Jimenez', ',', 'Andrea']\n",
            "Tokenized sentence: ['Mark', 'Hockings', '/', 'Richard', 'Silverthorn', '/', 'Neil', 'Taylor']\n",
            "Tokenized sentence: ['Derecho', 'Del', 'Autor', 'Reservado', '&', 'Rodolfo', 'Olivares']\n",
            "Tokenized sentence: ['Juan', 'Gay', 'tan']\n",
            "Tokenized sentence: ['Ty', 'Nitty', '/', 'The', 'Alchemist', '/', 'Big', 'Twins', '/', 'G.O.D', '.', 'Pt', '.', 'III', '/', 'Chinky']\n",
            "Tokenized sentence: ['Delete']\n",
            "Tokenized sentence: ['Werner', 'Taut', 'z']\n",
            "Tokenized sentence: ['I', 'CON', 'I', 'X']\n",
            "Tokenized sentence: ['Toivo', 'Kärki']\n",
            "Tokenized sentence: ['Josh', 'Best', ',', 'Patrick', 'Blackie', ',', 'Glenn', 'Arseneau', ',', 'Dave', 'Mackenzie', ',', 'Kellan', 'Menhennett', '&', 'Peter', 'Arseneau']\n",
            "Tokenized sentence: ['A', 'LEUNG', 'BENJAMIN', '/', 'C', 'BERMUDEZ', 'JENNIFER', 'RENE', '/', 'PA', 'RENE', 'SOMNA', '&', 'JENNIFER']\n",
            "Tokenized sentence: ['Steven', 'Jackson', '&', 'Ceasare', 'Willis']\n",
            "Tokenized sentence: ['Greta', 'Carroll', '/', 'Mikhail', 'Makarenko', '/', 'Kaja', 'Chomiczuk', '/', 'Andrew', 'Christopher', 'Hillock']\n",
            "Tokenized sentence: ['小蟲']\n",
            "Tokenized sentence: ['Michael', 'David', '/', 'Tyler', 'Blake']\n",
            "Tokenized sentence: ['SatKirin', '&', 'Thomas']\n",
            "Tokenized sentence: ['CA', 'BRUCE', 'JOSH', 'DWIGHT', '/', 'CA', 'ELIZA', 'PAIGE', '/', 'CA', 'HAYWARD', 'CRAIG', 'IAN', '/', 'PA', 'BRU', '-', 'C', 'FEAT', '.', 'PAIGE', 'ELIZA', '&', 'SHAPES']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Barett', ',', 'Mahmoud']\n",
            "Tokenized sentence: ['Camille', 'Saint', '‐', 'Saëns']\n",
            "Tokenized sentence: ['Hasan', '&', 'Husan', 'Holboevlar']\n",
            "Tokenized sentence: ['Nguy', 'ễn', 'Nhất', 'Huy']\n",
            "Tokenized sentence: ['Mario', 'Marchetti', '/', '유영진', '/', 'Sophie', 'Curtis', '/', 'Adam', 'McInnis']\n",
            "Tokenized sentence: ['Mozart']\n",
            "Tokenized sentence: ['Raymond', 'Watts']\n",
            "Tokenized sentence: ['Tor', 'björn', 'Wassenius', '/', 'Jonas', 'Liberg/Johan', 'Sahlén', '/', 'Claes', 'Andreasson', '/', 'Britt', 'Viberg', '/', 'Vibeke', 'Dueholm']\n",
            "Tokenized sentence: ['datahead']\n",
            "Tokenized sentence: ['Orlan', 'do', 'di', 'Lasso']\n",
            "Tokenized sentence: ['Lil', 'Raider']\n",
            "Tokenized sentence: ['C.Bringtown']\n",
            "Tokenized sentence: ['Gérard', 'Manset']\n",
            "Tokenized sentence: ['Jaisin', 'Patel']\n",
            "Tokenized sentence: ['Vincent', 'Cannady', '/', 'Tyler', 'Harrison', 'Mead', '/', 'J', '.', 'Hart']\n",
            "Tokenized sentence: ['Sybersound', '/', 'Finneas', \"O'Connell\", '/', 'Billie', 'Eilish', \"O'Connell\"]\n",
            "Tokenized sentence: ['Paul', 'Epworth', '/', 'J.A.C', '.', 'Redford', '/', 'Adele']\n",
            "Tokenized sentence: ['Mauricio', 'Carlakoski']\n",
            "Tokenized sentence: ['Hoàng', 'Hoa/Th', 'ảo', 'Trang']\n",
            "Tokenized sentence: ['Kieran', 'Connolly']\n",
            "Tokenized sentence: ['DEFRA', '/', 'Isaac', 'Rojas', '/', 'Pablo', 'Benjamin', 'Betancourth', 'Franco']\n",
            "Tokenized sentence: ['El', 'Fara', 'Ps']\n",
            "Tokenized sentence: ['Christina', 'Kostopoulos']\n",
            "Tokenized sentence: ['ALFRED', 'LLOYD', 'CAMPBELL']\n",
            "Tokenized sentence: ['Glenn', 'Danzig']\n",
            "Tokenized sentence: ['Kony', '/', 'Pikers', '/', 'Aetherboy', '1', '/', 'Piot', 'r', 'Olbert', '/', 'Marcin', 'Kosiorek', '/', 'Piot', 'r', 'Rutkowski', '/', 'Rafał', 'Ostrowski', '/', 'Daniel', 'Wójtowicz', '/', 'Łukas', 'z', 'Bułat', '-', 'Mironowicz']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Antoine', ',', 'Katoto', 'Luhembe']\n",
            "Tokenized sentence: ['Anibal', 'Tejed', 'a']\n",
            "Tokenized sentence: ['devyn', 'michael', 'walker']\n",
            "Tokenized sentence: ['Dubose', 'Heyward', '/', 'George', 'Gershwin', '/', 'Traditional', '/', 'Ira', 'Gershwin', '/', 'N', '/', 'A', '-', 'See', 'Medley', 'Segments']\n",
            "Tokenized sentence: ['Roland', 'Binder']\n",
            "Tokenized sentence: ['Vileta', 'Parra/Violeta', 'Parra']\n",
            "Tokenized sentence: ['Terpomo11', '/', 'Nagi', '/', 'AetherStar']\n",
            "Tokenized sentence: ['X', '-', 'Zone', 'Music', '/', 'Youth', 'Friends', 'Of', 'Jesus', '&', 'Deacon', 'Vincent']\n",
            "Tokenized sentence: ['John', 'Reis']\n",
            "Tokenized sentence: ['Capitán', 'Chinaco']\n",
            "Tokenized sentence: ['Wayne', 'Henderson', '/', 'A.L.S', '.', '&', 'C.', 'McCrary']\n",
            "Tokenized sentence: ['David', 'San', 'Andrés', 'Fernández', '/', 'Diego', 'Muñoz', 'Fajardo']\n",
            "Tokenized sentence: ['Giovanni', 'Pierluigi', 'da', 'Palestrina']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Lopez', ',', 'Joel']\n",
            "Tokenized sentence: ['Lonnie', 'rainey']\n",
            "Tokenized sentence: ['Rodrigo', 'Vieira/', 'Marcos', 'Carnaval']\n",
            "Tokenized sentence: ['Robert', 'Hunter', '/', 'Jerry', 'Garcia']\n",
            "Tokenized sentence: ['Grant', 'McDowall', ',', 'Will', 'Heggie', ',', 'Stuart', 'Everest', ',', 'Craig', 'Lorentson']\n",
            "Tokenized sentence: ['Rodney', 'Mashundure']\n",
            "Tokenized sentence: ['Pizko', 'MC']\n",
            "Tokenized sentence: ['Yasunori', 'Mitsuda']\n",
            "Tokenized sentence: ['Anne', '-', 'Sophie', 'Versnaeyen', ',', 'Nicolas', 'Bedos']\n",
            "Tokenized sentence: ['Mark', 'Kelly', '/', 'Steve', 'Hogarth', '/', 'Ian', 'Mosley', '/', 'Pete', 'Trewavas', '/', 'Steve', 'Rothery']\n",
            "Tokenized sentence: ['Ernes', 'to', 'E', '.', 'Perez', '/', 'Humber', 'to', 'Eugene', 'Diaz', '/', 'Michael', 'Alexis', 'Castro/Monte', 'Hess', '/', 'William', 'Dwight', 'McKelvy']\n",
            "Tokenized sentence: ['Huw', 'Gower', '/', 'Rod', 'Goodway', '/', 'Adrian', 'Shaw']\n",
            "Tokenized sentence: ['Zelski', ',', 'Hendelberg', ',', 'Keane', ',', 'Wright', ',', 'Woolley', ',', '&', 'Pridgen']\n",
            "Tokenized sentence: ['Antonio', 'Tamark', 'i', 'Baldwin']\n",
            "Tokenized sentence: ['伊藤', '和', '馬', '/', '森本', '練']\n",
            "Tokenized sentence: ['Laurent', 'Coulondre']\n",
            "Tokenized sentence: ['Jay', 'Winborn', '/', 'Nifa', 'Spraggins', '/', 'Marc', 'Pomeroy']\n",
            "Tokenized sentence: ['土屋', '憲', '一']\n",
            "Tokenized sentence: ['Tewodros', 'Tadesse']\n",
            "Tokenized sentence: ['Tommy', 'Ill']\n",
            "Tokenized sentence: ['Mark', 'Reilly', '/', 'Mark', 'Fisher']\n",
            "Tokenized sentence: ['Malola', 'Kannan', '&', 'J', '.', 'Bhakthavatsalam']\n",
            "Tokenized sentence: ['Romero', 'Os', 'by']\n",
            "Tokenized sentence: ['Lilli', 'Lewis']\n",
            "Tokenized sentence: ['Alejandro', '“', 'Kid', 'Fresco', '”', 'Jimene', 'z', '/', 'Maria', 'Gracia', 'Córdova', '/', 'Javier', '“', 'Noreh', '”', 'Triviño', '/', 'Giovanny', 'Fernández', '/', 'Gustavo', '“', 'Mangus', '”', 'Ovalles', '/', 'Jorge', 'Nehme']\n",
            "Tokenized sentence: ['Rob', 'Knox', ',', 'Brandyn', 'Burnette', ',', 'Molly', 'Moore', ',', 'Jesse', 'McCartney']\n",
            "Tokenized sentence: ['Yamil', 'Rezc']\n",
            "Tokenized sentence: ['Jai', 'dev', 'Kumar', '/', 'Kumaar']\n",
            "Tokenized sentence: ['James', 'Womack']\n",
            "Tokenized sentence: ['EFFY', '/', 'Shunryu']\n",
            "Tokenized sentence: ['GLUCK', 'CHRISTOPH', 'WILLIBALD', '/', 'LEON', 'CRAIG']\n",
            "Tokenized sentence: ['Adella', '/', 'Catherine', 'Steinberg']\n",
            "Tokenized sentence: ['Ibeth', 'Yarely', 'Sosa', 'Corral']\n",
            "Tokenized sentence: ['Desi', 'Crew']\n",
            "Tokenized sentence: ['Matt', 'Redman', '/', 'Jesse', 'Reeves', '/', 'Jonas', 'Myrin', '/', 'Chris', 'Tomlin']\n",
            "Tokenized sentence: ['Graham', 'Nash', '/', 'Allan', 'Clarke', '/', 'Tony', 'Hicks']\n",
            "Tokenized sentence: ['Brown', '/', 'Mallett', '/', 'Gibson', '/', 'JOHNSON']\n",
            "Tokenized sentence: ['Mira', '(', 'Critchlow', '&', 'Morgan', ')']\n",
            "Tokenized sentence: ['D', '.', 'A', '.', 'R', '.']\n",
            "Tokenized sentence: ['Pablo', 'Manuel', 'Martinez', ',', 'Aldana', 'Canale', ',', 'Padre', 'Jota', ',', 'Tomas', 'Romero', '&', 'Felipe', 'CB']\n",
            "Tokenized sentence: ['Reol', '/', 'ギガ', 'P']\n",
            "Tokenized sentence: ['卢小娟']\n",
            "Tokenized sentence: ['Samuel', 'Back', 'of']\n",
            "Tokenized sentence: ['D', '.', 'A', '.', 'R', '.']\n",
            "Tokenized sentence: ['Lynden', 'Williams']\n",
            "Tokenized sentence: ['Ringo', 'Starr']\n",
            "Tokenized sentence: ['Atlus']\n",
            "Tokenized sentence: ['DJ', 'ain', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Will', 'Henry', 'Monk', '/', 'Henry', 'Francis', 'Lyte']\n",
            "Tokenized sentence: ['Mikhail', 'Makarenko', '/', 'Kaja', 'Chomiczuk', '/', 'Andrew', 'Christopher', 'Hillock', '/', 'Greta', 'Carroll']\n",
            "Tokenized sentence: ['Josh', 'Tapen', '/', 'Janik', 'Riegert', '/', 'Niles', 'Hollowel', '-', 'Dhar', '/', 'Marc', 'Buhr']\n",
            "Tokenized sentence: ['Annie', 'Lennox', '/', 'Copyright', 'Control', '/', 'David', 'Allan', 'Stewart']\n",
            "Tokenized sentence: ['Jordi', 'San', 'z', 'Mompó', '&', 'Joan', 'San', 'z', 'Mompó']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Vicenti', ',', 'Paolo', 'Orion']\n",
            "Tokenized sentence: ['CA', 'BU', 'SKER', 'STEFANUS', 'F', 'M', 'STEFAN', '/', 'P', 'A', 'N', '-', 'VIT', 'R', 'A', 'L', 'FT', 'SOVEREI', 'GN', 'K', 'I', 'NG', '&', 'LAST', 'WORD']\n",
            "Tokenized sentence: ['Barton', 'Palmer']\n",
            "Tokenized sentence: ['Basistov', 'Semyon']\n",
            "Tokenized sentence: ['Trewhitt']\n",
            "Tokenized sentence: ['Javier', 'Tenenbaum']\n",
            "Tokenized sentence: ['出口', 'たか', 'し']\n",
            "Tokenized sentence: ['Srk', 'Sumon', 'Roy', '&', 'Giasuddin', 'Taheri']\n",
            "Tokenized sentence: ['Jennifer', 'Decilveo', ',', 'Andra', 'Day']\n",
            "Tokenized sentence: ['Jordi', 'Riba', 'Rivera/Tremendo', '/', 'Eddy', 'Drammeh', '/', 'N', 'é', 'stor', 'Gemar', 'García']\n",
            "Tokenized sentence: ['Inez', 'Janiak', '/', 'Mateus', 'z', 'Przybylski']\n",
            "Tokenized sentence: ['Vinnie', 'Paz', '/', 'Buckwild', '/', 'Celph', 'Titled']\n",
            "Tokenized sentence: ['James', 'Smith', '(', 'Underoath', ')', ',', 'Grant', 'Brandell', ',', 'Spencer', 'Chamberlain', ',', 'Aaron', 'Gillespie', ',', 'Christopher', 'Dudley', ',', 'Timothy', 'McTague']\n",
            "Tokenized sentence: ['Bryan', 'Adams', '/', 'Jim', 'Vallance', '/', 'Bob', 'Clearmountain']\n",
            "Tokenized sentence: ['Tetsuya', 'Komuro']\n",
            "Tokenized sentence: ['Erik', 'Ron', '/', 'Beau', 'Bokan', '/', 'Jared', 'Warth', '/', 'Eric', 'Lambert', '/', 'Joey', 'Sturgis', '/', 'Matt', 'Traynor', '/', 'Elliott', 'Gruenberg', '/', 'Blessthefall', '/', 'Copyright', 'Control', '/', 'BLESSTHEFALL', 'LLC', '-', 'BMI', '/', 'Touch', 'Down', 'Browns', 'Music']\n",
            "Tokenized sentence: ['Keep', 'The', 'Dream', 'Studio']\n",
            "Tokenized sentence: ['Michael', 'Cale', 'Parks']\n",
            "Tokenized sentence: ['Vadim', 'Nikolaenko']\n",
            "Tokenized sentence: ['Kevin', 'Savigar', '/', 'Jim', 'Cregan', '/', 'Rod', 'Stewart', '/', 'Gary', 'Grainger', '/', 'Phil', 'Chen']\n",
            "Tokenized sentence: ['camilo', 'joaquin', 'villarruel']\n",
            "Tokenized sentence: ['Fran', 'z', 'Lehár']\n",
            "Tokenized sentence: ['Pandit', 'G', '/', 'Afjal', 'Miah', '/', 'Aktar', 'Ahmed', '/', 'Lord', 'Camacho', '/', 'Sanjay', 'Tailor', '/', 'Dr', '.', 'Das', '/', 'Steve', 'Chandra', 'Savale']\n",
            "Tokenized sentence: ['Sickboyrari', ',', 'SosMula', ',', 'ZillaKami']\n",
            "Tokenized sentence: ['Khathadath']\n",
            "Tokenized sentence: ['Will', 'Gregory', '/', 'Alison', 'Goldfrapp']\n",
            "Tokenized sentence: ['Keondre', 'Taylor']\n",
            "Tokenized sentence: ['Napalm', \"El'Diablo\"]\n",
            "Tokenized sentence: ['Erik', 'Canales', '/', 'David', 'Merino', 'Velasco/Rober', 'to', 'Carlos', 'Castrillo', 'Sanchez', '/', 'Daniel', 'Alcover']\n",
            "Tokenized sentence: ['Hay', 'k', 'Zardaryan', '&', 'Movses', 'Margaryan']\n",
            "Tokenized sentence: ['Joey', 'Fatts']\n",
            "Tokenized sentence: ['Teófilo', 'Guerrero']\n",
            "Tokenized sentence: ['A', '.', 'Petrov']\n",
            "Tokenized sentence: ['Gus', 'Kahn', '/', 'Wilbur', 'Schwandt', '/', 'Fabian', 'Andr', 'é']\n",
            "Tokenized sentence: ['Matt', 'Easton', '/', 'Jeff', 'Arenson']\n",
            "Tokenized sentence: ['David', 'McWilliams']\n",
            "Tokenized sentence: ['Sefat', 'Ullah', 'Sefuda/Md', 'Ehsanul', 'Habib', 'Onik']\n",
            "Tokenized sentence: ['Vangelis']\n",
            "Tokenized sentence: ['Nick', 'Hakim', '/', 'Andrew', 'Sarlo', '/', 'Jesse', 'and', 'Forever', '/', 'Michael', 'J', 'Thomas', 'III']\n",
            "Tokenized sentence: ['Emile', 'Hoogenhout', '/', 'Copyright', 'Control', '/', 'Edition', 'Get', 'Physical', '/', 'Budde', 'Music']\n",
            "Tokenized sentence: ['Vesa', 'Ranta', '/', 'Sami', 'Lopakka', '/', 'Taneli', 'Jarva/Miika', 'Tenkula', '/', 'Not', 'Documented']\n",
            "Tokenized sentence: ['Iron', 'Avantgarde', 'Publishing', '/', 'Brittney', 'Hayes', '/', 'Scott', 'Buchanan', '/', 'Andrew', 'Saunders', '/', 'Grant', 'Truesdell', '/', 'Nikko', 'Whitworth', '/', 'Unleash', 'The', 'Archers']\n",
            "Tokenized sentence: ['Prabhat', 'Barot', '&', 'Traditional']\n",
            "Tokenized sentence: ['CA', 'PRYOR', 'RICHARD', 'F']\n",
            "Tokenized sentence: ['THERESA', 'JONES', '/', 'UKNOWN', 'WRITER']\n",
            "Tokenized sentence: ['Dmitri', 'Shostakovich']\n",
            "Tokenized sentence: ['Tyler', 'Halverson']\n",
            "Tokenized sentence: ['Roderick', 'Story']\n",
            "Tokenized sentence: ['Nariak', 'i']\n",
            "Tokenized sentence: ['WILLIAMS', 'JOHN', 'T']\n",
            "Tokenized sentence: ['Graham', 'Parker', '/', 'Unknown', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Beach', 'Season', ',', 'Rome', 'in', 'Silver']\n",
            "Tokenized sentence: ['Lamb', 'Of', 'God']\n",
            "Tokenized sentence: ['Derek', 'Christopher', '&', 'Vaxx']\n",
            "Tokenized sentence: ['Bhai', 'Gurmail', 'Singh', 'Ji', '/', 'D', 'P']\n",
            "Tokenized sentence: ['Janir', 'Sanchez', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['孫儀', '/', '劉家昌']\n",
            "Tokenized sentence: ['Duy', 'Khánh']\n",
            "Tokenized sentence: ['INFERNALES', 'RECORD']\n",
            "Tokenized sentence: ['Ali', 'Abdulah']\n",
            "Tokenized sentence: ['Jon', 'Secada', '/', 'Miguel', 'A', '.', 'Morejón']\n",
            "Tokenized sentence: ['Traditional', '/', 'Yvan', 'Cassar', '/', 'Eric', 'Chevalier']\n",
            "Tokenized sentence: ['Jim', 'Croce']\n",
            "Tokenized sentence: ['CA', 'JONK', 'MAN', 'TIM', 'P', '/', 'CA', 'K', 'A', 'N', 'V', 'A', 'N', 'ERWIN', 'H', '/', 'P', 'A', 'GRI', 'DKILLER', '&', 'NOS', 'FERATU']\n",
            "Tokenized sentence: ['Jordan', 'R', 'Todd']\n",
            "Tokenized sentence: ['Soundmind', 'Artists', '&', 'Weastwick', 'Music']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Jackson', ',', 'Andy', '/', 'Jackson', ',', 'Glenroy', '/', 'Jackson', ',', 'Nicola', '/', 'Jackson', ',', 'Trevor']\n",
            "Tokenized sentence: ['Amaal', 'Mallik', '/', 'Badshah', '/', 'Kumaar']\n",
            "Tokenized sentence: ['Ebk', 'Eski']\n",
            "Tokenized sentence: ['Jacopo', 'Ferretti', '/', 'Gaetano', 'Donizetti']\n",
            "Tokenized sentence: ['TAIHE', 'I']\n",
            "Tokenized sentence: ['GlockBoy', 'BoBo']\n",
            "Tokenized sentence: ['Nguy', 'ễn', 'Ngọc', 'Thiện']\n",
            "Tokenized sentence: ['Dave', 'Cobb', '/', 'Tim', 'Hanseroth', '/', 'Brandi', 'Carlile', '/', 'Phil', 'Hanseroth', '/', 'Thomas', 'Benjamin', 'Cooper']\n",
            "Tokenized sentence: ['BOOK', 'D', 'A', 'N', 'ST', 'EVEN', '/', 'GOLDST', 'EIN', 'A', 'NDREW', 'MAXWEL', 'L', '/', 'OVERSTREET', 'CHORD']\n",
            "Tokenized sentence: ['Ángel', 'Aníbal']\n",
            "Tokenized sentence: ['-/Akira', 'The', 'Don']\n",
            "Tokenized sentence: ['Boon', 'wdg', '&', 'marr', 'osama']\n",
            "Tokenized sentence: ['Paul', 'Noriega']\n",
            "Tokenized sentence: ['Bob', 'Young', '/', 'Francis', 'Rossi']\n",
            "Tokenized sentence: ['櫻井', '敦司', '/', '今井', '寿']\n",
            "Tokenized sentence: ['William', 'Byrd']\n",
            "Tokenized sentence: ['Stephen', 'Roslonek']\n",
            "Tokenized sentence: ['Raekwon', 'Bailey']\n",
            "Tokenized sentence: ['Jerry', 'Herman']\n",
            "Tokenized sentence: ['Martin', 'Briley', '/', 'Danny', 'Tate']\n",
            "Tokenized sentence: ['Paramith', 'a', 'Rusady']\n",
            "Tokenized sentence: ['David', 'Sitbon', '/', 'Julien', 'Reichen', '/', 'Florentin', 'Coquelin', '/', 'Christoph', 'Grossniklaus']\n",
            "Tokenized sentence: ['Jaquae', 'Baker']\n",
            "Tokenized sentence: ['ボン', 'ジュ', 'ール', '鈴木']\n",
            "Tokenized sentence: ['Prznt']\n",
            "Tokenized sentence: ['Ganjina', '&', 'Sarban']\n",
            "Tokenized sentence: ['Rowe', '(', 'Collier', ',', 'Lang', ',', 'Munoz', '&', 'Ruelas', ')']\n",
            "Tokenized sentence: ['Олег', 'Попков']\n",
            "Tokenized sentence: ['ZUN', '/', '〜蒼', '〜']\n",
            "Tokenized sentence: ['Dave', 'Audé', '/', 'Crystal', 'Waters', '/', 'Neal', 'Brian', 'Conway']\n",
            "Tokenized sentence: ['Rauf', 'Mirzaev', '/', 'Faik', 'Mirzaev']\n",
            "Tokenized sentence: ['Selwy', 'n', 'Birchwood', '/', 'Selwy', 'n', 'Birchwood', 'Music']\n",
            "Tokenized sentence: ['Oscar', 'Pearce', '/', 'Copyright', 'Control', '/', 'Marlo', 'Hoogstraten']\n",
            "Tokenized sentence: ['Adam', 'Jame', 'ček', '/', 'Aleksandr', 'Chemeze', 'Odu']\n",
            "Tokenized sentence: ['Denver', 'Dale']\n",
            "Tokenized sentence: ['Gary', 'Novak', '/', 'Carl', 'Walker', '/', 'Scott', 'Kinsey', '/', 'Timothy', 'LeFebvre', '/', 'Timothy', 'Lefebvre']\n",
            "Tokenized sentence: ['Nikolay', 'Bespalov']\n",
            "Tokenized sentence: ['Deborah', 'Lurie']\n",
            "Tokenized sentence: ['LOS', 'Y', 'A', 'NIS', 'DE', 'V', 'I', 'L', 'LA', 'DE', 'GPE', '.']\n",
            "Tokenized sentence: ['Claudio', 'Monteverdi']\n",
            "Tokenized sentence: ['wondem', 'u', 'jira']\n",
            "Tokenized sentence: ['Luke', 'Frizon/Tristan', 'Barnes', '/', 'Nelson', 'Barnes']\n",
            "Tokenized sentence: ['YU', 'M', 'IN', '/', 'Franken', '/', 'Heondred']\n",
            "Tokenized sentence: ['Exøtix', ',', 'PS', 'Y', 'K', 'I', 'DD', ',', 'J4', 'M']\n",
            "Tokenized sentence: ['Sameer', '/', 'Nadeem', '-', 'Shravan']\n",
            "Tokenized sentence: ['Thomas', 'F', 'Blake']\n",
            "Tokenized sentence: ['Daniel', 'Vernunft']\n",
            "Tokenized sentence: ['Udana', 'Prageeth', 'Jerome', 'Silva']\n",
            "Tokenized sentence: ['Kaka', 'Barboza']\n",
            "Tokenized sentence: ['Deepin', '(', '디핀', ')', '/Liquor', '(', '리쿼', ')', '/김박첼라', '(', 'Kim', 'Park', 'Challa', ')']\n",
            "Tokenized sentence: ['Hampus', 'Wiberg', ',', 'Joshua', 'Voß', ',', 'Wiberg', '&', 'eedion']\n",
            "Tokenized sentence: ['ZillaKami']\n",
            "Tokenized sentence: ['Mbosso']\n",
            "Tokenized sentence: ['Piero', 'Ciampi', '/', 'Gianfranco', 'Reverberi']\n",
            "Tokenized sentence: ['Ninguno', 'Ninguno']\n",
            "Tokenized sentence: ['Gary', 'Francois', '/', 'Serge', 'Ramaekers', '/', 'Hansbert', 'Vanhove', '/', 'Copyright', 'Control', '/', 'Gery', 'Francois']\n",
            "Tokenized sentence: ['Nguy', 'ễn', 'Minh', 'Anh']\n",
            "Tokenized sentence: ['Larry', 'Kimpel']\n",
            "Tokenized sentence: ['Digital', 'Durian', '&', 'Astro']\n",
            "Tokenized sentence: ['Xena', 'Aouita', '&', 'Haja', 'Hamdaouia']\n",
            "Tokenized sentence: ['Daniel', 'López', 'Rodríguez', '&', 'Dora', 'Postigo']\n",
            "Tokenized sentence: ['JAYLEIN', 'CANTRELL']\n",
            "Tokenized sentence: ['Heather', 'Evonne', 'Markham']\n",
            "Tokenized sentence: ['Jane', 'Taylor', '/', 'Declan', 'Flynn', '/', 'Copyright', 'Control', '/', 'Wolfgang', 'Amadeus', 'Mozart']\n",
            "Tokenized sentence: ['Pat', 'Fish', '/', 'Fire', 'Songs']\n",
            "Tokenized sentence: ['Aleksandar', 'Dimitrijevic']\n",
            "Tokenized sentence: ['Shinichi', 'Mitsui/Yui', 'Ogura']\n",
            "Tokenized sentence: ['M', '.', 'P', 'Yadav', '/', 'J', '.', 'P', 'Tiwari']\n",
            "Tokenized sentence: ['Loïc', 'Leborgne']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Rodriguez', 'Loperena', ',', 'Osvaldo']\n",
            "Tokenized sentence: ['Chris', 'Rea']\n",
            "Tokenized sentence: ['Ahmad', 'Eerfan']\n",
            "Tokenized sentence: ['Los', 'Ninos', 'de', 'Yemalla', 'Music', 'SESAC']\n",
            "Tokenized sentence: ['Michael', 'Fesser']\n",
            "Tokenized sentence: ['Soulja', 'Boy', ',', '\\u200b\\u200b', 'Hoopstar', 'Beats']\n",
            "Tokenized sentence: ['حسين', 'حمزة', '/', 'رفيق', 'حبيقة']\n",
            "Tokenized sentence: ['Eddie', 'James']\n",
            "Tokenized sentence: ['Mario', 'Sandoval']\n",
            "Tokenized sentence: ['Mano', 'Brown', '/', 'Edi', 'Rock']\n",
            "Tokenized sentence: ['Laurent', 'Scimeca', '/', 'Cedric', 'De', 'Pasquale', '/', 'D', '&', 'P', 'Mode', 'Music', '(', 'ASCAP', ')', '/Gervais', 'Music', 'Publishing', '(', 'ASCAP', ')', '/Sample', 'Pack']\n",
            "Tokenized sentence: ['SI', 'MON', 'JOE', '(', 'USA', '1', '-', 'CNT', 'R', 'Y', ')', '/', 'SPEE', 'R', 'MARC', 'D', 'A', 'V', 'ID']\n",
            "Tokenized sentence: ['Adem', 'Ciwan']\n",
            "Tokenized sentence: ['Kid', 'Tune']\n",
            "Tokenized sentence: ['Javier', 'San', 'Roman']\n",
            "Tokenized sentence: ['Larry', 'Gatlin', '/', 'Jerry', 'Kennedy']\n",
            "Tokenized sentence: ['M', '.', 'S.Bairagi', '/', 'Surinder', 'Sehaj']\n",
            "Tokenized sentence: ['Mook', 'Boy']\n",
            "Tokenized sentence: ['Heinrich', 'August', 'Marschner', '/', 'W', '.', 'A', '.', 'Wohlbruck']\n",
            "Tokenized sentence: ['Melvin', 'Schmitz', '/', '18', 'Karat']\n",
            "Tokenized sentence: ['Kieran', 'the', 'Light']\n",
            "Tokenized sentence: ['Joe', 'Reisman', '/', 'Henry', 'Mancini']\n",
            "Tokenized sentence: ['Katerina', 'Westbrook', ',', 'Gold', 'i', 'Lux', '&', 'Kennedy', 'James']\n",
            "Tokenized sentence: ['Peter', 'Mynte/John', 'Rostill']\n",
            "Tokenized sentence: ['Пётр', 'Ильич', 'Чайковский']\n",
            "Tokenized sentence: ['ZydSounds']\n",
            "Tokenized sentence: ['Yaven', 'Mauld', 'in', '&', 'David', 'McDaniel']\n",
            "Tokenized sentence: ['Gesher', '/', 'I', 'Am', 'Grime']\n",
            "Tokenized sentence: ['Charles', 'Nicholas', 'Hayes']\n",
            "Tokenized sentence: ['Masked', 'Wolf', ',', 'IDK', ',', 'KayCyy', ',', 'Sir', 'Nolan', ',', 'Tyron', 'Hapi', ',', 'Feli', 'Ferraro']\n",
            "Tokenized sentence: ['Juan', 'Peralta']\n",
            "Tokenized sentence: ['James', 'Lightfoot', '/', 'Alastair', 'Thorpe', '/', 'Jonathan', 'Barnard', '/', 'David', 'Lightfoot']\n",
            "Tokenized sentence: ['Richard', 'Butler', '/', 'Martin', 'L', '.', 'Gore']\n",
            "Tokenized sentence: ['Hainze', 'Diaz', 'Arroy', 'o', ',', 'Neftal', 'í', 'Alvare', 'z', 'Nuñe', 'z', ',', 'Carlkos', 'Crespo', 'Planas', '&', 'Hector', 'E', '.', 'Birriel', 'Caraballo']\n",
            "Tokenized sentence: ['James', 'Hetfield', '/', 'Lars', 'Ulrich']\n",
            "Tokenized sentence: ['Deshaun', 'Leonard']\n",
            "Tokenized sentence: ['Ger', 'm', 'án', 'Montero']\n",
            "Tokenized sentence: ['Crown', 'Uzama']\n",
            "Tokenized sentence: ['Kevin', 'Kade']\n",
            "Tokenized sentence: ['Uri', 'Porter', '-', 'Wise', '/', 'Gabrielius', 'Boyle']\n",
            "Tokenized sentence: ['Tyler', 'Glenn', '/', 'Justin', 'Meldal-Johnsen']\n",
            "Tokenized sentence: ['Rhe', 'ma', 'Loseke']\n",
            "Tokenized sentence: ['Damien', 'Delat', 'tre']\n",
            "Tokenized sentence: ['aimyon', '/', 'Mo', 'to', 'Kawabe']\n",
            "Tokenized sentence: ['Devin', 'Vann']\n",
            "Tokenized sentence: ['Douglas', 'Cezar']\n",
            "Tokenized sentence: ['Ryan', 'Justin', 'Baker']\n",
            "Tokenized sentence: ['Stefan', 'Andersson', '/', 'Petrus']\n",
            "Tokenized sentence: ['Oliver', 'Ponsford', ',', 'Chris', 'Wilkie', '/', 'Chris', 'Wilkie', '/', 'Oliver', 'Ponsford', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Fuzzorama', 'Records', '/', 'Daniel', 'Dorninger', ',', 'Hans', 'Peter', 'Leitner', ',', 'Michael', 'Hirschmugl', ',', 'Bernhard', 'Weigl', ',', 'Bernhard', 'Sorger']\n",
            "Tokenized sentence: ['Juan', 'Llanos', '&', 'Jorge', 'Kiny', 'Murillo']\n",
            "Tokenized sentence: ['Arturo', 'Váz', 'quez', '/', 'Váz', 'quez', 'Arturo']\n",
            "Tokenized sentence: ['Umber', 'to', 'Sulpasso', 'jr', '.']\n",
            "Tokenized sentence: ['Wande', 'Isola', ',', 'Deandre', 'Hunter', ',', 'Xavier', 'Baird', '&', 'Clark', 'Lacossade']\n",
            "Tokenized sentence: ['Quico']\n",
            "Tokenized sentence: ['Greg', 'Ackell', ',', 'Motohiro', 'Yasue', ',', 'Paula', 'Kelley', ',', 'Peter', 'Koeplin', ',', 'Steve', 'Zimmerman']\n",
            "Tokenized sentence: ['Copyright', 'Control', '/', 'Dr', '.', 'Djaendar', 'J', 'Lumban', 'Gaol']\n",
            "Tokenized sentence: ['Keisuke', 'Kuwata']\n",
            "Tokenized sentence: ['Norman', 'Gimbel', '/', 'Toots', 'Thielemans']\n",
            "Tokenized sentence: ['Tee', 'Grizzley', ',', 'Skilla', 'Baby', ',', 'Wayne', '616', ',', 'Pablo616']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Monti', ',', 'Daniel']\n",
            "Tokenized sentence: ['YG', ',', 'Slim', '400']\n",
            "Tokenized sentence: ['Tradicional']\n",
            "Tokenized sentence: ['Roz', 'z', 'Williams']\n",
            "Tokenized sentence: ['Ethereal']\n",
            "Tokenized sentence: ['Rosendale']\n",
            "Tokenized sentence: ['Rareblin']\n",
            "Tokenized sentence: ['Bill', 'Withers', '/', 'Lee', 'Scratch', 'Perry']\n",
            "Tokenized sentence: ['Francisco', 'Renter', 'ía']\n",
            "Tokenized sentence: ['BEN', 'Z', 'O']\n",
            "Tokenized sentence: ['Dennis', 'Steven', 'Jagard']\n",
            "Tokenized sentence: ['Apashe', '&', 'LeKtriQue', '/', 'Riptide', 'Music']\n",
            "Tokenized sentence: ['Hoàng', 'Trang']\n",
            "Tokenized sentence: ['Martez', 'Harrison']\n",
            "Tokenized sentence: ['Casper', 'Michael', 'Frisk', '/', 'Adrian', 'Valentino', 'Bellan']\n",
            "Tokenized sentence: ['Jeff', 'Sheridan', '/', 'Kendall', 'Schmidt', '/', 'The', 'Mix', '2', 'Broadcast', '/', 'Dick', 'Tracy', '/', 'Soundworks']\n",
            "Tokenized sentence: ['Christopher', 'Proctor']\n",
            "Tokenized sentence: ['Moro', '/', 'Skizo', 'Beats']\n",
            "Tokenized sentence: ['Kerwin', 'Du', 'Bois']\n",
            "Tokenized sentence: ['Jory', 'Boy', ',', 'Anuel', 'A', 'A']\n",
            "Tokenized sentence: ['Mc', 'Cafferty']\n",
            "Tokenized sentence: ['Joey', 'Tempest']\n",
            "Tokenized sentence: ['Kaisson', 'Delroy', 'Brothers']\n",
            "Tokenized sentence: ['eyes', ',', 'wings', 'and', 'many', 'other', 'things']\n",
            "Tokenized sentence: ['Erols', '-', 'Enroc-Music', '/', 'Juan', 'Maldonado', 'Fernande', 'z']\n",
            "Tokenized sentence: ['Hugo', 'Velá', 'z', 'quez']\n",
            "Tokenized sentence: ['Gene', 'Autry/Harriett', 'Melka', '/', 'Oakley', 'Haldeman']\n",
            "Tokenized sentence: ['Стас', 'Михайлов']\n",
            "Tokenized sentence: ['Mark', 'Blissenden', '/', 'Andrew', 'Burdall']\n",
            "Tokenized sentence: ['Tsu', 'Surf', ',', 'Jordyn']\n",
            "Tokenized sentence: ['Frankie', 'Ogilvie']\n",
            "Tokenized sentence: ['Gulzar', '/', 'Rahul', 'Dev', 'Burman']\n",
            "Tokenized sentence: ['Astor', 'Piazzolla']\n",
            "Tokenized sentence: ['Tom', 'Waits']\n",
            "Tokenized sentence: ['Francisco', 'Duran', '&', 'Mauricio', 'Dur', 'án']\n",
            "Tokenized sentence: ['Andreias', 'Mihai', 'Liviu']\n",
            "Tokenized sentence: ['Wheezy', ',', 'Yak', 'Gotti', ',', 'Young', 'Thug']\n",
            "Tokenized sentence: ['Arturo', 'Safin', '&', 'Газави', 'Сэмуэль']\n",
            "Tokenized sentence: ['Tom', 'Hodge', '/', 'Fran', 'z', 'Kirmann']\n",
            "Tokenized sentence: ['Gabriele', 'Azzalin']\n",
            "Tokenized sentence: ['H.', 'Biggs', '/', 'J', '.', 'Thomas']\n",
            "Tokenized sentence: ['Angel', 'Chairez', '&', 'Jesus', 'Martinez']\n",
            "Tokenized sentence: ['Jann', 'Michael', 'Engel/Murat', 'Parlak', '/', 'Jeff', 'Aug', '/', 'Niko', 'Lai', '/', 'Anne', 'Clark']\n",
            "Tokenized sentence: ['Jorge', 'Macias', 'Góme', 'z']\n",
            "Tokenized sentence: ['Brahim', 'Biskri']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Benjamin', ',', 'Emanuel']\n",
            "Tokenized sentence: ['Marvin', 'P.', 'Dalton', '/', 'Bob', 'Jones', 'And', 'Nick', 'Bruno']\n",
            "Tokenized sentence: ['Paul', 'Simon']\n",
            "Tokenized sentence: ['CC']\n",
            "Tokenized sentence: ['Cash', 'Cash', '/', 'Ilsey', 'Juber', '/', 'Jenn', 'Decilveo', '/', 'Alexander', 'Makhlouf', '/', 'Jean', 'Paul', 'Makhlouf', '/', 'Samuel', 'Warren', 'Frisch']\n",
            "Tokenized sentence: ['Sam', 'Reed', '/', 'Tokyo', 'Prose', '/', 'Copyright', 'Control', '(', 'PRO', ')', '/S', 'Reed', '/', 'S', 'Stokes', '/', 'J', 'Mcbride']\n",
            "Tokenized sentence: ['Tony', 'Hiller', '/', 'Colin', 'Michael', 'Frechter']\n",
            "Tokenized sentence: ['Michael', 'Ramey', 'Combs']\n",
            "Tokenized sentence: ['Juan', 'Gabriel']\n",
            "Tokenized sentence: ['CA', 'LISZT', 'FRANZ']\n",
            "Tokenized sentence: ['Madonna', '/', 'Patrick', 'Leonard', '/', 'Bruce', 'Gaitsch']\n",
            "Tokenized sentence: ['Carson', 'Patrick', 'Linzie']\n",
            "Tokenized sentence: ['Talibando']\n",
            "Tokenized sentence: ['Edgar', 'Silva']\n",
            "Tokenized sentence: ['[', 'traditional', ']', '/Sir', 'Henry', 'Walford', 'Davies']\n",
            "Tokenized sentence: ['FATTA', 'L']\n",
            "Tokenized sentence: ['Lasse', 'Andersson', '/', 'Pernilla', 'Wahlgren']\n",
            "Tokenized sentence: ['Real', 'Red', '/', 'Christopher', 'Rodgers']\n",
            "Tokenized sentence: ['Kevione', 'Austin', '/', 'Brian', 'Chan', 'Jones', '/', 'Kiz']\n",
            "Tokenized sentence: ['CA', \"O'N\", 'E', 'A', 'L', 'SEA', 'N', '/', 'CA', 'RASSE', 'A', 'NDREW', '/', 'P', 'A', 'BUTANE', '&', 'SOMEONE', 'ELSE']\n",
            "Tokenized sentence: ['Retro', 'Future', ',', 'K', 'CAMP']\n",
            "Tokenized sentence: ['Pap', 'Chanel']\n",
            "Tokenized sentence: ['Hans', 'Hee']\n",
            "Tokenized sentence: ['Popular']\n",
            "Tokenized sentence: ['Shinya', 'Kiyozuka/Ludwig', 'van', 'Beethoven']\n",
            "Tokenized sentence: ['Wayne', 'Wallace', '/', 'Jes', 'ús', 'Díaz/Elisabeth', 'Fuentes', '/', 'Guillermo', 'Céspedes', '/', 'Bobbi', 'Cespedes']\n",
            "Tokenized sentence: ['Babyfxce', 'E']\n",
            "Tokenized sentence: ['Mats', 'Persson', '/', 'Per', 'Gessle']\n",
            "Tokenized sentence: ['U', '-', 'Twenty']\n",
            "Tokenized sentence: ['Joel', 'Harrison', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Alberdis', 'Collado', '/', 'Copyright', 'Control', '/', 'Francisco', 'Miguel', 'Hidalgo', '/', 'Cesar', 'Guarionex', 'Marmolejos', 'Dilone']\n",
            "Tokenized sentence: ['Will', 'Wiesenfeld']\n",
            "Tokenized sentence: ['Arnoux', 'Zaramody']\n",
            "Tokenized sentence: ['Baligh', 'Hamdi', '&', 'Magdi', 'Nagib']\n",
            "Tokenized sentence: ['ALGUE', 'R', 'O', 'ALGUE', 'R', 'O', 'AUGUS', 'TO', '/', 'DONA', 'GG', 'IO', 'GIU', 'SEPPE', '/', 'K', 'APS', 'SCHOEN', 'FIEL', 'D', 'ARTU', 'R', 'O']\n",
            "Tokenized sentence: ['Juan', 'Pablo', 'Canales', 'Sánchez']\n",
            "Tokenized sentence: ['Jeremy', 'Soule/Emil', 'Pagliarulo', '/', 'Judith', 'de', 'los', 'Santos']\n",
            "Tokenized sentence: ['Simon', 'Ondrejka']\n",
            "Tokenized sentence: ['Mikolai', 'Stroinski', ',', 'Marcin', 'Przyby', 'łowicz']\n",
            "Tokenized sentence: ['LiSA', '/', '草野', '華', '余子']\n",
            "Tokenized sentence: ['범이', '낭이', '(', 'BEOM', 'x', 'N', 'A', 'NG', ')', ',', '신사', '동호', '랭이', '(', 'Shinsadong', 'Tiger', ')']\n",
            "Tokenized sentence: ['Zachary', 'Haleblian', ',', 'Simon', 'Broberg', '&', 'Mel', 'Wood']\n",
            "Tokenized sentence: ['A', 'BROEKHUYSE', 'ADRIA', 'N', 'J', '/', 'A', 'KENN', 'E', 'D', 'Y', 'N', 'E', 'EV', '/', 'C', 'SMIR', 'NO', 'V', 'EVGEN', 'Y', '/', 'P', 'A', 'AUROS', 'ON', 'IC', '&', 'N', 'E', 'EV', 'KENN', 'E', 'D', 'Y']\n",
            "Tokenized sentence: ['Stone', 'II', '&', 'FAM', 'ENT']\n",
            "Tokenized sentence: ['Dennison', 'De', 'Lima', 'Gomes']\n",
            "Tokenized sentence: ['Jahlil', 'Terrance', 'Lawrence']\n",
            "Tokenized sentence: ['Dener', 'Ceide']\n",
            "Tokenized sentence: ['BBG', 'YaYa']\n",
            "Tokenized sentence: ['Alvin', 'Lee']\n",
            "Tokenized sentence: ['Emilian', 'o', 'Bruguer', 'a', '&', 'Fran', 'z', 'Loddo']\n",
            "Tokenized sentence: ['Gianni', 'Togni', '/', 'Guido', 'Morra/Ricardo', 'Montaner']\n",
            "Tokenized sentence: ['Ben', 'Weis', 'man', '/', 'Sammy', 'Gallop']\n",
            "Tokenized sentence: ['井ノ上', '和', '宏/Tsukas', 'a']\n",
            "Tokenized sentence: ['Claude', 'Debussy', '/', 'Erik', 'Satie']\n",
            "Tokenized sentence: ['Hector', 'A', 'Leyva']\n",
            "Tokenized sentence: ['BIA', ',', 'Victoria', 'Mon', 'ét']\n",
            "Tokenized sentence: ['\\u200bskylarallen', ',', 'Rivilin']\n",
            "Tokenized sentence: ['Alan', 'Jay', 'Lerner', '/', 'Frederick', 'Loewe']\n",
            "Tokenized sentence: ['Playboylos']\n",
            "Tokenized sentence: ['Luke', 'Dick', ',', 'Josh', 'Kear']\n",
            "Tokenized sentence: ['Preston', 'Summerville', ',', 'Chris', 'Harris', ',', 'Josh', 'Knight', '&', 'Jese', 'Galvan']\n",
            "Tokenized sentence: ['Roderick', 'Tate', '/', 'Kenneth', 'Daniels', '/', 'Shekinah', 'Daniels', '/', 'Big', 'Headed', 'Bros', 'LLC', '/', 'GoodVibes', 'Productions']\n",
            "Tokenized sentence: ['Korletey', 'Mimpey']\n",
            "Tokenized sentence: ['MJ', 'Kuok']\n",
            "Tokenized sentence: ['Ballpoint', '/', 'David', 'Halsey']\n",
            "Tokenized sentence: ['WILLOW', '/', 'James', 'Chul', 'Rim']\n",
            "Tokenized sentence: ['Justin', 'Moore', '&', 'Amodium', 'AUS']\n",
            "Tokenized sentence: ['Anthony', 'White']\n",
            "Tokenized sentence: ['Vandell', 'Andrew', '&', 'Romana', 'De', 'Meneges']\n",
            "Tokenized sentence: ['Antoine', 'Thomas', ',', 'Rakeem', 'Smith']\n",
            "Tokenized sentence: ['10kcashin', '/', 'Vanie', 'Brim']\n",
            "Tokenized sentence: ['Giuseppe', 'Verdi', '/', 'Salvatore', 'Cammarano']\n",
            "Tokenized sentence: ['Osunlade']\n",
            "Tokenized sentence: ['Michelle', 'Hord', '/', 'Zach', 'Hord', '/', 'Casi', 'Joy', 'Lankford', '/', 'Bryan', 'Lankford']\n",
            "Tokenized sentence: ['高橋', '雅則']\n",
            "Tokenized sentence: ['Afroto', '-', 'عفروتو']\n",
            "Tokenized sentence: ['Chris', 'Brann', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Moure', 'Olguin', '&', 'Porras', 'Nunez']\n",
            "Tokenized sentence: ['Ill', 'Will', '(', 'Battle', 'Rapper', ')', ',', 'Serius', 'Jones']\n",
            "Tokenized sentence: ['Karl-Ewert', '/', 'Jules', 'Sylvain']\n",
            "Tokenized sentence: ['SIMM', 'IE', 'SIMS']\n",
            "Tokenized sentence: ['Javier', 'Rocha']\n",
            "Tokenized sentence: ['Endo', 'Anaconda', '/', 'Balts', 'Nill']\n",
            "Tokenized sentence: ['HAYWOOD', 'D', 'A', 'V', 'ID', 'WESLEY', '/', 'KEA', 'R', 'JOSHUA', 'PE', 'TER', '/', 'KELLEY', 'CHARLES', 'BURGE', 'SS', '/', 'SCOTT', 'HILLARY', 'D', 'A', 'W', 'N']\n",
            "Tokenized sentence: ['Brandon', 'Hart']\n",
            "Tokenized sentence: ['Timothy', 'Alan', 'Kasparek']\n",
            "Tokenized sentence: ['Daniel', 'Kim', '(', '김다', '니엘', ')']\n",
            "Tokenized sentence: ['DP', '/', 'JIR', 'I', 'Z', 'IMA', '/', 'JOS', 'EF', 'SKVOREC', 'K', 'Y', '/', 'obo', 'OSA']\n",
            "Tokenized sentence: ['Mr', 'NurBayan']\n",
            "Tokenized sentence: ['Oliver', 'Tree', 'Nickell', '/', 'David', 'Pramik', '/', 'Marshmello']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/johnson', ',', 'Wesley']\n",
            "Tokenized sentence: ['Georges', 'Brassens', '/', 'Julio', 'Ardiles', 'Gray']\n",
            "Tokenized sentence: ['Thad', 'Jones']\n",
            "Tokenized sentence: ['Мухамедьяров', 'Данис', 'Римович', '&', 'Даянов', 'Наркис', 'Наилович']\n",
            "Tokenized sentence: ['CG', '5']\n",
            "Tokenized sentence: ['Irkenc', 'Hy', 'ka']\n",
            "Tokenized sentence: ['Fred', 'Fisher']\n",
            "Tokenized sentence: ['Geminian', 'o', 'Giacomelli', '/', 'Zeno', '&', 'Lalli']\n",
            "Tokenized sentence: ['Tomo', 'Katsurada', '/', 'Kotsuguy', '/', 'Go', 'Kurosawa']\n",
            "Tokenized sentence: ['Sapho']\n",
            "Tokenized sentence: ['Tristan', 'Salvati/Yohann', 'Malory']\n",
            "Tokenized sentence: ['Nadeem', '-', 'Shravan', '/', 'Sameer']\n",
            "Tokenized sentence: ['Philippe', 'Saisse']\n",
            "Tokenized sentence: ['Lang', 'Van', 'Inc', '.']\n",
            "Tokenized sentence: ['matthew', 'acree', '/', 'toheryl', 'drewitt']\n",
            "Tokenized sentence: ['Alick', 'Macheso']\n",
            "Tokenized sentence: ['Still', 'Nas']\n",
            "Tokenized sentence: ['Johann', 'Strauss', ',', 'Jr']\n",
            "Tokenized sentence: ['Tyrone', 'Israel', 'Lopez', 'Perez', '/', 'Bobby', 'J', 'Frausto']\n",
            "Tokenized sentence: ['Shay', 'Alon', '/', 'Christopher', 'Delarue']\n",
            "Tokenized sentence: ['Nour', 'Eldin', 'Mohamed']\n",
            "Tokenized sentence: ['MARCHAND', 'A', 'LA', 'N', 'PHILIP', '(', 'DR', ')']\n",
            "Tokenized sentence: ['써니', '(', 'Sunnie', ')']\n",
            "Tokenized sentence: ['Koud', 'Pouss']\n",
            "Tokenized sentence: ['Anjaan', '/', 'Rahul', 'Dev', 'Burman']\n",
            "Tokenized sentence: ['Натан', 'Вячеславович', 'Моргунов', '/', 'Светлана', 'Владимировна', 'Моргунова']\n",
            "Tokenized sentence: ['Jalen', 'Brown']\n",
            "Tokenized sentence: ['David', 'Webster', '/', 'Brendan', 'Webster', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['YUC', \"'\", 'e']\n",
            "Tokenized sentence: ['Sony', '/', 'CMRRA', '/', 'Kim', 'Deal', '/', 'Mark', 'Freegard', '/', 'Harry', 'Fox', 'Agency', '/', 'ATV', 'Music', 'Publishing', '/', 'ATV', 'Music', 'Publishing', 'Ltd', '/', 'Period', 'Music', ',', 'admin', 'by', 'Sony']\n",
            "Tokenized sentence: ['Freshy', 'Kanal', ',', 'Jughead', 'Jay', ',', 'Evan', ',', 'Creep', 'Switch', ',', 'Mat4yo']\n",
            "Tokenized sentence: ['Alexis', 'y', 'Fido']\n",
            "Tokenized sentence: ['Aunty', 'Donna']\n",
            "Tokenized sentence: ['Mateo', 'Piracés', '-', 'Ugarte/Luê', 'Soares']\n",
            "Tokenized sentence: ['唐壁光', '&', '徐淑華']\n",
            "Tokenized sentence: ['Awich', '/', 'Ke', 'Yano', '$', ',', 'Chaki', 'Zulu']\n",
            "Tokenized sentence: ['L', 'I', 'F', 'E', '/', 'L', 'I', 'N', 'E', ',', 'Leo', 'Napier']\n",
            "Tokenized sentence: ['Bouchra', 'Elm', '/', 'Ahmed', 'Houssainy', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Angel', 'Gon', 'zález']\n",
            "Tokenized sentence: ['Manuel', 'Pic', 'ón', '/', 'Pablo', 'Neruda']\n",
            "Tokenized sentence: ['Future', 'Cut', '/', 'Lily', 'Allen', '/', 'Clement', 'Dodd', '/', 'Darren', 'Lewis', '/', 'Jackie', 'Mittoo', '/', 'Iyiola', 'Babalola']\n",
            "Tokenized sentence: ['Howard', 'Baer']\n",
            "Tokenized sentence: ['SuperFitness', '/', 'Stefano', 'Folegatti', '/', 'Planeta', 'Mix', 'Records', 'S.L./Copyright', 'Control']\n",
            "Tokenized sentence: ['Baljeet', 'Singh', 'Chahal']\n",
            "Tokenized sentence: ['David', 'Mitchell']\n",
            "Tokenized sentence: ['Edward', 'Flint']\n",
            "Tokenized sentence: ['Dehmel/Grothe', '/', 'Baierle', 'Records']\n",
            "Tokenized sentence: ['Chen', 'Yue']\n",
            "Tokenized sentence: ['Willie', 'Banks']\n",
            "Tokenized sentence: ['Angelo', 'Natalie']\n",
            "Tokenized sentence: ['Antoine', 'Dauvergne']\n",
            "Tokenized sentence: ['Domenico', 'Scarlatti']\n",
            "Tokenized sentence: ['G', '.', 'T', '.']\n",
            "Tokenized sentence: ['Magnus', 'William', 'Klainguti']\n",
            "Tokenized sentence: ['Jerry', 'Leiber', '/', 'Mike', 'Stoller/Ben', 'E', '.', 'King']\n",
            "Tokenized sentence: ['Wilhelm', 'Körner']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Markos', ',', 'Elan', 'Galang']\n",
            "Tokenized sentence: ['Don', 'Henley', '/', 'Glenn', 'Frey', '/', 'Jackson', 'Browne/Elliot', 'Scheiner']\n",
            "Tokenized sentence: ['Another', 'Infinity', '/', 'mitsu']\n",
            "Tokenized sentence: ['Saucii', '/', 'Eighty', '8', '/', 'Jai', 'Beats', '/', 'Jaidyn', 'Hullum', '/', 'Jaylon', 'Howard', '/', 'Eric', 'Foley', 'Jr./Rodarius', 'Green']\n",
            "Tokenized sentence: ['Prafulla', 'Kar']\n",
            "Tokenized sentence: ['Lyndn', 'David', 'Gauntlett', '/', 'Jake', 'Damien', 'Ridley']\n",
            "Tokenized sentence: ['AKINA', 'STEPHEN', '(', 'MR', ')']\n",
            "Tokenized sentence: ['Daryl', 'Stuermer', '/', 'Phil', 'Collins']\n",
            "Tokenized sentence: ['Jean', 'Feline', '/', 'Paul', 'Misraki', '/', 'Cornelis', 'Martin', 'Schrama']\n",
            "Tokenized sentence: ['Huy', 'ền', 'Thanh', '/', 'Huỳnh', 'Anh']\n",
            "Tokenized sentence: ['Michel', 'Mauleart', 'Monton', '/', 'Oswald', 'Durand', '/', 'Pepe', 'Bayard']\n",
            "Tokenized sentence: ['Semaj', 'Nelson']\n",
            "Tokenized sentence: ['Peter', 'Blegvad']\n",
            "Tokenized sentence: ['Jonny', 'Greenwood', '/', 'Philip', 'Selway', '/', 'Thom', 'Yorke', '/', 'Ed', 'O', '’', 'Brien', '/', 'Colin', 'Greenwood']\n",
            "Tokenized sentence: ['Twana', 'Nzar']\n",
            "Tokenized sentence: ['CFY', 'Music', '/', 'Niko', 'Wenner', '/', 'Diorite', 'Music']\n",
            "Tokenized sentence: ['Klak', 'Hits', '/', 'Oscar', 'Armando', 'Diaz', 'De', 'Leon', '/', 'Javier', 'Gonzalez', 'El', 'Tamarindo']\n",
            "Tokenized sentence: ['DUC', 'TR', 'I']\n",
            "Tokenized sentence: ['Ryan', 'Perry', 'Gilliland', ',', 'Tyler', 'Bank', ',', 'Tamara', 'McGuckin', '&', 'Elysse', 'Yulo']\n",
            "Tokenized sentence: ['MIKE', '(', 'USA', ')', ',', 'Sideshow', ',', 'The', 'Alchemist']\n",
            "Tokenized sentence: ['Michael', 'Curro']\n",
            "Tokenized sentence: ['Anton', 'Muravyev']\n",
            "Tokenized sentence: ['DILLON', 'LEONA', 'RD', 'WIN', 'STON']\n",
            "Tokenized sentence: ['Peter', 'Dale', '/', 'Reginald', 'King']\n",
            "Tokenized sentence: ['传统', '音乐', '&', '李富兴']\n",
            "Tokenized sentence: ['Trúc', 'Phương']\n",
            "Tokenized sentence: ['William', 'Daniels', '&', 'David', 'Brewster']\n",
            "Tokenized sentence: ['Z', 'A', 'N', '/', '三', '宅', '一', '徳']\n",
            "Tokenized sentence: ['rubiros', 'a', 'matos']\n",
            "Tokenized sentence: ['RR', '/', 'Peđa', 'Medenica']\n",
            "Tokenized sentence: ['Wolfgang', 'Amadeus', 'Mozart', '/', 'Lorenzo', 'Da', 'Ponte']\n",
            "Tokenized sentence: ['Clément', 'Rhétorie']\n",
            "Tokenized sentence: ['Khalid', '/', 'FINN', 'EAS/Billie', 'Eilish', '/', 'Alejandro', 'Cázares']\n",
            "Tokenized sentence: ['Z', 'The', 'Savage', '/', 'Chris', 'Duval', '/', 'Get', 'Em', 'Louie', '/', 'Zues', 'Negrete/Luis', 'F', '.', 'Cofre', '/', 'Noah', 'Malik', 'Lee']\n",
            "Tokenized sentence: ['Y', 'Vũ', '/', 'Hoài', 'An', '/', 'Lê', 'Giang', '/', 'Lê', 'Duy', 'ên', '/', 'Thanh', 'Sơn', '/', 'Khiết', 'Duy', '/', 'Ngân', 'Giang', '/', 'Song', 'Ngọc', '/', 'Đình', 'Văn', '/', 'Hà', 'Phư', 'ơng', '/', 'Khánh', 'Băng/Lam', 'Phư', 'ơng', '/', 'Nhật', 'Ngân', '/', 'Tiến', 'Luân', '/', 'Lư', 'Nhất', 'Vũ', '/', 'Trần', 'Trịnh', '/', 'Hoàng', 'Thi', 'Thơ', '/', 'Vũ', 'Quốc', 'Việt', '/', 'Trịnh', 'Công', 'Sơn', '/', 'Võ', 'Đông', 'Điền', '/', 'Nguy', 'ễn', 'Ngọc', 'Thiện']\n",
            "Tokenized sentence: ['Massimo', 'Russo/Dibenedet', 'to', 'Edizioni', '/', 'Francesco', 'Dibenedet', 'to']\n",
            "Tokenized sentence: ['Space', 'Ambient']\n",
            "Tokenized sentence: ['Nacio', 'Herb', 'Brown', '/', 'Arthur', 'Freed']\n",
            "Tokenized sentence: ['Devontae', 'Loper', ',', 'Dedrick', 'Wright', '&', 'Christian', 'Landry']\n",
            "Tokenized sentence: ['Young', 'Chop', ',', 'Lil', 'Bibby']\n",
            "Tokenized sentence: ['Kanda', 'Bongo', 'Man']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Richburg', ',', 'Terence']\n",
            "Tokenized sentence: ['Styx', '/', 'James', 'Oliver', 'Trummy', 'Young']\n",
            "Tokenized sentence: ['Lindy', '-', 'Fay', 'Hella', '/', 'Einar', 'Selvik', '/', 'Linda', 'Fey', 'Hella']\n",
            "Tokenized sentence: ['Edmund', 'Clement', '/', 'Thomas', 'Brückner', '/', 'Robert', 'Garg', '/', 'Andrea', 'Martin', '/', 'Ivan', 'Matias', '/', 'Eniac']\n",
            "Tokenized sentence: ['Trịnh', 'Công', 'Sơn']\n",
            "Tokenized sentence: ['Flau', '’', 'jae']\n",
            "Tokenized sentence: ['Salomon', 'Hermann', 'Mosenthal/Anton', 'Rubinstein']\n",
            "Tokenized sentence: ['Lil', 'Durk', ',', 'King', 'Louie', ',', 'Ronnie']\n",
            "Tokenized sentence: ['Stu', 'Mackenzie/Michael', 'Cavanagh', '/', 'Joey', 'Walker']\n",
            "Tokenized sentence: ['Eddie', 'Matos']\n",
            "Tokenized sentence: ['Dan', 'Williams', '&', 'Ryan', 'Hoover']\n",
            "Tokenized sentence: ['Lana', 'Del', 'Rey', '/', 'Daniel', 'Law', 'Heath']\n",
            "Tokenized sentence: ['Клинских', 'Ю', '.']\n",
            "Tokenized sentence: ['CA', 'BALDING', 'NICHOLAS', 'MATTHEW', '/', 'CA', 'BRACKINS', 'BOBBY', 'CLIFTON', 'III', '/', 'CA', 'FELTON', 'JEREMY', 'P', '/', 'CA', 'GILLUM', 'GERALD', '/', 'CA', 'REDWINE', 'JON', '/', 'PA', 'FEAT', '.', 'G', '-', 'EAZY', '&', 'JEREMIH', 'BOBBY', 'BRACKINS']\n",
            "Tokenized sentence: ['Duy', 'ên', 'Anh', '&', 'Phạm', 'Duy']\n",
            "Tokenized sentence: ['Alexandra', 'Harwood']\n",
            "Tokenized sentence: ['Ludwig', 'van', 'Beethoven']\n",
            "Tokenized sentence: ['U', 'N', 'K', 'NO', 'W', 'N', 'WR', 'I', 'TER', '/', '羅莎', '莎']\n",
            "Tokenized sentence: ['BALLI', 'DOM', 'I', 'NIC']\n",
            "Tokenized sentence: ['Hugo', 'Zarco']\n",
            "Tokenized sentence: ['Discount']\n",
            "Tokenized sentence: [\"D'zine\"]\n",
            "Tokenized sentence: ['Flogging', 'Molly']\n",
            "Tokenized sentence: ['John', 'Toso']\n",
            "Tokenized sentence: ['Los', 'Wawanco']\n",
            "Tokenized sentence: ['Murray', 'Clarke']\n",
            "Tokenized sentence: ['Thomas', 'Newman']\n",
            "Tokenized sentence: ['Carl', 'Perkins']\n",
            "Tokenized sentence: ['Young', 'Thug']\n",
            "Tokenized sentence: ['Vivacious', 'Fierce/Brandon', 'Morales']\n",
            "Tokenized sentence: ['Henry', 'Jackman', '/', 'Dominic', 'Lewis']\n",
            "Tokenized sentence: ['Robert', 'Hunter', '/', 'Mickey', 'Hart', '/', 'Phil', 'Lesh', '/', 'Bob', 'Weir', '/', 'Ron', 'McKernan']\n",
            "Tokenized sentence: ['jorge', 'zuñiga']\n",
            "Tokenized sentence: ['XR', 'S', '/', 'DJ', 'Marky']\n",
            "Tokenized sentence: ['Ali', 'Sethe']\n",
            "Tokenized sentence: ['Tina', 'Noelle', ',', 'Tristan', '``', 'Plaquey', 'Chan', \"''\", 'Rice', ',', 'Donavelo', 'donavelo', '&', 'Devon', 'Nico']\n",
            "Tokenized sentence: ['MATTHEW', 'ROB', 'ER', 'TFRENETTE', '/', 'M', 'I', 'KEREN', 'O', '/', 'PAUL', 'WARRENDEA', 'N']\n",
            "Tokenized sentence: ['Naeleck', '&', 'Sarah', 'Rebecca']\n",
            "Tokenized sentence: ['GS', 'Music']\n",
            "Tokenized sentence: ['Daniel', 'Rustage']\n",
            "Tokenized sentence: ['Wrecia', 'Holloway', '/', 'James', 'McDougal']\n",
            "Tokenized sentence: ['Quelly', 'Woo', ',', 'Kyle', 'Richh', ',', 'Meenman']\n",
            "Tokenized sentence: ['Tim', 'Kiefer']\n",
            "Tokenized sentence: ['Ger', 'a', 'MX']\n",
            "Tokenized sentence: ['Ross', 'Stagg']\n",
            "Tokenized sentence: ['Daniel', 'Robinson', ',', 'Jr', '.']\n",
            "Tokenized sentence: ['Bandai', 'Namco', 'Music', 'Live', 'Inc', './', '株式会社', 'バン', 'ダイ', 'ナム', 'コミ', 'ュー', 'ジッ', 'クラ', 'イブ']\n",
            "Tokenized sentence: ['BAEZ', 'GABRIEL', '/', 'COLON', 'RUBEN', 'DOM', 'I', 'NGO', '(', 'MR', ')']\n",
            "Tokenized sentence: ['Dayane', 'Camargo', ',', 'Lara', 'Menezes', ',', 'Rafael', 'Quadros', ',', 'Vinni', 'Miranda', ',', 'Waléria', 'Leão']\n",
            "Tokenized sentence: ['CA', 'DJ', 'BUD', 'DHA', '/', 'CA', 'HUNTE', 'ANGEL', 'A', 'ANN', '/', 'CA', 'KEN', 'Z', 'O', 'EDDY', '/', 'CA', 'PERALTA', 'CARLO', '/', 'CA', 'PHILLIP', 'A', 'NDREW', '/', 'P', 'A', 'MA', 'FFIO', '&', 'ANGEL', 'A', 'HUNTE', 'KULCHA', 'FEAT']\n",
            "Tokenized sentence: ['サイ', 'トウ', 'ヨシ', 'ヒロ']\n",
            "Tokenized sentence: ['Benjamin', 'I', 'srael']\n",
            "Tokenized sentence: ['Oje', 'Ollivierre', '/', 'Anna', '-', 'Sharé', 'Blake', '/', 'Jason', 'Arthur', 'Farmer', '/', 'Jason', 'J', '-', 'Vibe', 'Farmer']\n",
            "Tokenized sentence: ['Christian', 'Colberg']\n",
            "Tokenized sentence: ['Geddy', 'Lee', '/', 'Neil', 'Peart', '/', 'Alex', 'Lifeson']\n",
            "Tokenized sentence: ['Tony', 'Lopez', '/', 'Ricardo', 'Delgado/Alber', 'to', 'Javier', 'Amado/Rober', 'to', 'Gañ', 'án', 'Ojea', '/', 'Francisco', 'Javier', 'Nav', 'ío', '/', 'Jos', 'é', 'Miguel', 'Red', 'in', 'Red', 'in', '/', 'Julio', 'César', 'Sánchez', 'Suárez']\n",
            "Tokenized sentence: ['Earl', 'Falconer', '/', 'Brian', 'Travers', '/', 'Ali', 'Campbell', '/', 'Robin', 'Campbell', '/', 'Jim', 'Brown', '/', 'Terence', 'Wilson', '/', 'Michael', 'Virtue', '/', 'Norman', 'Hassan']\n",
            "Tokenized sentence: ['Катаев', 'Ислам', 'Идрисович']\n",
            "Tokenized sentence: ['Fr', '.', 'Nathanael', 'Guirguis']\n",
            "Tokenized sentence: ['Daljit', 'Faridkoti', '/', 'Shinda', 'Brar']\n",
            "Tokenized sentence: ['Фартушный', 'Егор', 'Дмитриевич']\n",
            "Tokenized sentence: ['Bill', 'Withers']\n",
            "Tokenized sentence: ['Zane', 'Alexander', '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Enrico', 'Cacace']\n",
            "Tokenized sentence: ['Colloquial', 'Sound', 'Recordings']\n",
            "Tokenized sentence: ['Kamanaleo', 'A', 'Edwards']\n",
            "Tokenized sentence: ['Joseph', 'Tavana']\n",
            "Tokenized sentence: ['Link', 'Wray', '/', 'Austin', 'de', 'Lone', '/', 'Eggs', 'Over', 'Easy']\n",
            "Tokenized sentence: ['David', 'Arkenstone']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Roberts', ',', 'Terrell']\n",
            "Tokenized sentence: ['Haruki', 'Neru', '/', 'Yoshiak', 'i', 'Watanuki/', 'Masahiro', 'Mizoguchi']\n",
            "Tokenized sentence: ['Emmanuel', 'Okon']\n",
            "Tokenized sentence: ['Jaq', 'uavis', 'Damajanique', 'Graham']\n",
            "Tokenized sentence: ['Fabio', 'Concato']\n",
            "Tokenized sentence: ['Lakim', '&', 'M', '.', 'Mills']\n",
            "Tokenized sentence: ['Andre', 'Wagner', '/', 'René', 'Schönrock', '/', 'Kai', 'Bondzio', '/', 'Andreas', 'Friebe']\n",
            "Tokenized sentence: ['Tonia', 'Reeh']\n",
            "Tokenized sentence: ['Harry', 'Thumann', '/', 'Hermann', 'Weindorf']\n",
            "Tokenized sentence: ['Riverworn']\n",
            "Tokenized sentence: ['Jerome', 'Fagnet', ',', 'Samuel', 'Guibout', '&', 'Thomas', 'Bernardini']\n",
            "Tokenized sentence: ['Rafael', 'I', 'gnacio']\n",
            "Tokenized sentence: ['Mário', 'Sabo/Hrivnak', 'Peter', '/', 'martin', 'kosovan', '/', 'Peter', 'Hrivň', 'ák', '/', 'Juraj', 'Štefánik']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/OROZCO', ',', 'JOSE', '-', 'LUIS']\n",
            "Tokenized sentence: ['Shirish', 'Kunder']\n",
            "Tokenized sentence: ['B.', 'Woods', '/', 'Colin', 'Wolfe', '/', 'Gerald', 'Cooper', '/', 'G', '.', 'Clinton', ',', 'Jr', '.']\n",
            "Tokenized sentence: ['A', 'L', 'YY', 'T', 'IN', 'E', 'N', 'ERJA', 'ANNELI/C', 'L', 'YY', 'T', 'IN', 'E', 'N', 'ERJA', 'ANNELI/PA', 'ERJA', 'L', 'YY', 'T', 'IN', 'E', 'N', '&', 'HEI', 'KK', 'I', 'SILVEN', 'NO', 'IN', 'E', 'N']\n",
            "Tokenized sentence: ['anaise', 'areille', 'laurejoie']\n",
            "Tokenized sentence: ['Emma', 'Back']\n",
            "Tokenized sentence: ['Luc', 'Plamondon', '/', 'Ric', 'cardo', 'Cocciante']\n",
            "Tokenized sentence: ['Jean', 'Carlos', 'Centeno']\n",
            "Tokenized sentence: ['Philip', 'Buckley']\n",
            "Tokenized sentence: ['Adam', 'Schlesinger', '/', 'Chris', 'Colling', 'wood']\n",
            "Tokenized sentence: ['Kent', 'Fingal/Lars', 'Wiggman']\n",
            "Tokenized sentence: ['Lars', 'Ulrich', '/', 'James', 'Hetfield', '/', 'Cliff', 'Burton', '/', 'Kirk', 'Hammett']\n",
            "Tokenized sentence: ['Ito', 'Rapadas']\n",
            "Tokenized sentence: ['Mckenzie', 'Jakovie', 'Haynes']\n",
            "Tokenized sentence: ['Michael', 'Thomas', 'Kinsella']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Viola', ',', 'Lisa', 'Santos']\n",
            "Tokenized sentence: ['Seymour', 'Solomon', '/', 'Gershon', 'Kingsley', '/', 'Jean', '-', 'Jacques', 'Perrey']\n",
            "Tokenized sentence: ['Son', '(', 'Choi', ',', 'Hash', ',', 'Kim', ',', 'Myun', 'do', '&', 'Vista', ')']\n",
            "Tokenized sentence: ['Christos', 'Panos', '/', 'Maxwell', 'Obraham', 'Whitehorn']\n",
            "Tokenized sentence: ['William', 'Tasker']\n",
            "Tokenized sentence: ['Jeffrey', 'Bowman', '/', 'Josh', 'Williams', '/', 'Maggie', 'Eckford']\n",
            "Tokenized sentence: ['Yuki', 'Kishida', '/', 'Kentaro', 'Sonoda']\n",
            "Tokenized sentence: ['Klubasic', '/', 'Soulbridge', '/', 'Antonio', 'Luigi', 'Frassanito']\n",
            "Tokenized sentence: ['Samuel', 'Figueroa']\n",
            "Tokenized sentence: ['Rewire/Master', 'Legrand/Blenfre', 'Almonte', '/', 'Frank', 'La', 'Melodia', '/', 'Valentina', 'Patarroy', 'o', 'Cantero']\n",
            "Tokenized sentence: ['Andy', 'Rogelio', 'Cuevas', 'Cavazos']\n",
            "Tokenized sentence: ['Archie', 'Roach']\n",
            "Tokenized sentence: ['Tommy', 'Wener', '/', 'Buck', 'Owens']\n",
            "Tokenized sentence: ['Deidre', 'Brown', '/', 'Yamma', 'Brown', '/', 'Deanna', 'Brown']\n",
            "Tokenized sentence: ['Bright', 'Music', 'Ltd']\n",
            "Tokenized sentence: ['Ondrej', 'Souk', 'up', '/', 'Janus', 'z', 'Onufrowicz']\n",
            "Tokenized sentence: ['Derechos', 'de', 'Autor', 'Reservados']\n",
            "Tokenized sentence: ['Marko', 'Popov']\n",
            "Tokenized sentence: ['Tommy', 'Boyce', '/', 'Bobby', 'Hart']\n",
            "Tokenized sentence: ['Tay', 'c', '/', 'Nyadiko']\n",
            "Tokenized sentence: ['Muzaliwali', '/', 'Emmy']\n",
            "Tokenized sentence: ['BARNETT', 'I', 'A', 'N', 'R', 'Y', 'A', 'N', 'MR']\n",
            "Tokenized sentence: ['Munshi', 'Juwel', '&', 'Tazul', 'I', 'slam']\n",
            "Tokenized sentence: ['Глюк', '’', 'oZa']\n",
            "Tokenized sentence: ['James', 'Riscinti']\n",
            "Tokenized sentence: ['Лиханов', 'Дмитрий', 'Андреевич']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Thompson', ',', 'Adrian', 'Tyrell']\n",
            "Tokenized sentence: ['Summer', 'Walker', ',', 'Khalid', 'Robinson', ',', 'Jamil', 'Chammas', ',', 'Simon', 'Rosen', ',', 'Denis', 'Kosiak', ',', 'Khirye', 'Tyler', '&', 'Kaely', 'n', 'Behr']\n",
            "Tokenized sentence: ['Marcel', 'Delannoy']\n",
            "Tokenized sentence: ['Devi', 'Sri', 'Prasad']\n",
            "Tokenized sentence: ['Juan', 'Pablo', 'Canales', 'Sánchez', '&', 'Alexis', 'Peña']\n",
            "Tokenized sentence: ['Freeze', '/', 'Ampichino', '/', 'Young', 'Bossy']\n",
            "Tokenized sentence: ['CA', 'SUCKLEY', 'JOR', 'D', 'A', 'N', 'THOMA', 'S/CA', 'WALKER', 'JOHN', '/', 'P', 'A', 'JOR', 'D', 'A', 'N', 'SUCKLEY', '&', 'KUT', 'SKI']\n",
            "Tokenized sentence: ['Nathan', 'Thomas', ',', 'Carissa', 'Alvarado', ',', 'Michael', 'Alvarado']\n",
            "Tokenized sentence: ['L.A.', 'Reid', '/', 'Kenny', 'Edmonds', '/', 'Kenneth', 'Edmonds', '/', 'Antonio', 'L.A', 'Reid']\n",
            "Tokenized sentence: ['Zay', 'Bang']\n",
            "Tokenized sentence: ['Dee', 'j', 'Beatty']\n",
            "Tokenized sentence: ['Valery', 'Shibitov']\n",
            "Tokenized sentence: ['Irving', 'Caesar', '/', 'Vincent', 'Youmans']\n",
            "Tokenized sentence: ['The', 'Uninvited']\n",
            "Tokenized sentence: ['Armen', 'Grigorjan']\n",
            "Tokenized sentence: ['Ulf', 'Engström', '/', 'Viktor', 'Strand', '/', 'Gabriel', 'Alares', '/', 'Andreas', 'Olsson', '/', 'Camilla', 'Läckberg']\n",
            "Tokenized sentence: ['Mark', 'Farner']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Nguyen', ',', 'Steven', 'Gia', 'Phu']\n",
            "Tokenized sentence: ['Vexi', 'Salmi']\n",
            "Tokenized sentence: ['Percy', 'Jones']\n",
            "Tokenized sentence: ['Subaru', 'Shibutani']\n",
            "Tokenized sentence: ['Ghostemane']\n",
            "Tokenized sentence: ['Dan', 'Wilson']\n",
            "Tokenized sentence: ['Euringer', '/', 'James', 'Galus']\n",
            "Tokenized sentence: ['Kevin', 'Donohue', '/', 'Nathan', 'Aurora', '/', 'Joshua', 'Fairman', '/', 'Christopher', 'Anderson']\n",
            "Tokenized sentence: ['Gö', 'khan', 'Mand', 'ır']\n",
            "Tokenized sentence: ['Amir', 'Teima', ',', 'Khaled', 'Ezz']\n",
            "Tokenized sentence: ['ZERO', '(', 'YVES', '&', 'ADAMS', ')', '/Ava1anche', '/', 'Andy', 'Love']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Tran', ',', 'David']\n",
            "Tokenized sentence: ['Tim', '&', 'Marquetta', 'Wright', '/', 'Anthony', 'Gunter']\n",
            "Tokenized sentence: ['Alfred', 'Lion', '/', 'Jon', 'Hendricks', '/', 'Herbert', 'Hancock']\n",
            "Tokenized sentence: ['John', 'Mateos', 'Ong']\n",
            "Tokenized sentence: ['Julian', '/', 'Label', 'Control']\n",
            "Tokenized sentence: ['Francisco', 'Javier', 'San', 'z', '&', 'Domingo', 'Sampedro']\n",
            "Tokenized sentence: ['วั', 'งส', 'ัน', 'ต์', '/', 'เอ', 'ื้', 'อ', 'สุ', 'นท', 'รส', 'นา', 'น', '/', 'สม', 'ชา', 'ย', 'กฤ', 'ษณ', 'ะเ', 'ศร', 'ณี']\n",
            "Tokenized sentence: ['Kyle', 'Harris', 'Johnson']\n",
            "Tokenized sentence: ['Harry', 'Warren', '/', 'Mort', 'Dixon', '/', 'Joe', 'Young']\n",
            "Tokenized sentence: ['Jessica', 'Daniel', 'Abbott']\n",
            "Tokenized sentence: ['Dominik', 'Smith']\n",
            "Tokenized sentence: ['JOSEPH', 'MICALLEF', '(', 'ARRAN', 'GER', ')']\n",
            "Tokenized sentence: ['J.', 'Carlsson']\n",
            "Tokenized sentence: ['Gerald', 'Leroy', 'Chauncey', 'Page']\n",
            "Tokenized sentence: ['Bimbim']\n",
            "Tokenized sentence: ['Holger', 'Gutwald']\n",
            "Tokenized sentence: ['Johnny', 'Mercer', '/', 'Joseph', 'Kos', 'ma', '/', 'Jacques', 'Prévert']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Polk', ',', 'Brandon']\n",
            "Tokenized sentence: ['Nick', 'Beggs', '/', 'Steve', 'Askew', '/', 'Stuart', 'Neale', '/', 'Jeremy', 'Strode']\n",
            "Tokenized sentence: ['Henry', 'M', 'Ysabel/Joel', 'Ysabel']\n",
            "Tokenized sentence: ['Vlad', 'Nedelea']\n",
            "Tokenized sentence: ['Kevin', 'Hart']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Mccartney', ',', 'Brandon', 'Christopher']\n",
            "Tokenized sentence: ['Jabari', 'Stevens']\n",
            "Tokenized sentence: ['Simen', 'M', 'Eriksrud', '/', 'Kent', 'Sundberg/Cato', 'Sundberg']\n",
            "Tokenized sentence: ['Willie', '“', 'The', 'Lion', '”', 'Smith']\n",
            "Tokenized sentence: ['The', 'BK', 'Sound', '/', '上江洌清作', '/', 'Dennis', 'Emanuel', 'Brown', '/', 'Yvonne', 'Marjorie', 'Brown']\n",
            "Tokenized sentence: ['LAUBSCHER', 'R', 'Y', 'A', 'N']\n",
            "Tokenized sentence: ['Dzũng', 'Chinh']\n",
            "Tokenized sentence: ['Jhordy', 'Gilber', 'to', 'Jauregui', 'Gon', 'zález']\n",
            "Tokenized sentence: ['Danilo', 'Ranger']\n",
            "Tokenized sentence: ['Kal', 'Banx', '/', 'Austin', 'Brown', '/', 'Ivan', 'Jackson', '/', 'Solána', 'Rowe', '/', 'Zach', 'Witness', '/', 'Jaylah', 'Hickmon', '/', 'Oh', 'Gosh', 'Leotus']\n",
            "Tokenized sentence: ['Hassan', 'Barrow']\n",
            "Tokenized sentence: ['Tama/Yaso', '8', '/', 'Masashi', '/', 'Cozy', 'Kubo']\n",
            "Tokenized sentence: ['DROELOE']\n",
            "Tokenized sentence: ['Nikos', 'Gatsos', ',', 'Giorgos', 'Hadjinasios', '&', 'Jose', 'Maria', 'Puron']\n",
            "Tokenized sentence: ['Edot', 'Babyy', ',', 'KinoDa1', ',', 'Ivprod']\n",
            "Tokenized sentence: ['Giovanni', 'Simone', 'Mayr']\n",
            "Tokenized sentence: ['wtfeuro/Yuki', 'Beats', '/', 'jupiterstwin', '/', 'Dhariyan', 'Allan']\n",
            "Tokenized sentence: ['John', 'Hornsby', '/', 'Wayne', 'Pooley']\n",
            "Tokenized sentence: ['MARSHALL', 'PHILIP', 'PETER', '(', 'MR', ')']\n",
            "Tokenized sentence: ['Singga']\n",
            "Tokenized sentence: ['Farzad', 'Farzin/Yaha', 'Kashani', '/', 'Anooshirvan', 'Taghavi']\n",
            "Tokenized sentence: ['Johannes', 'Wamberg', '/', 'Coco', 'Karshøj']\n",
            "Tokenized sentence: ['Kobe', 'Borowet', 'z', '&', 'Makenna', 'Kuzy', 'k']\n",
            "Tokenized sentence: ['Bhupen', 'Hazarika', '/', 'Hemanga', 'Biswas']\n",
            "Tokenized sentence: ['Ike', 'Turner']\n",
            "Tokenized sentence: ['Angela', 'Leiva']\n",
            "Tokenized sentence: ['David', 'Crowder', '/', 'Brent', 'Milligan']\n",
            "Tokenized sentence: ['BIGGS', 'KELLY', '/', 'CLARKE', 'OWEN', '/', 'COPYRIGHT', 'CONTROL', '/', 'TAYLOR', 'LINDSEY', 'JOHN']\n",
            "Tokenized sentence: ['Darryl', 'Granberry']\n",
            "Tokenized sentence: ['Dee', 'Snider']\n",
            "Tokenized sentence: ['Sr', '.', 'Trầm', 'Hương', ',', 'FMSR']\n",
            "Tokenized sentence: ['Nikhil', '-', 'Vinay']\n",
            "Tokenized sentence: ['Goo', 'Goo', \"Gaga's\", '/', 'Copyright', 'Control']\n",
            "Tokenized sentence: ['Ace', 'Hood']\n",
            "Tokenized sentence: ['Jack', 'Fowler', '/', 'Kellin', 'Quinn', '/', 'Andrew', 'Marcus', 'Baylis', '/', 'Nicholas', 'Anthony', 'Martin']\n",
            "Tokenized sentence: ['Feio', '/', 'Cheope/Xororó', '/', 'F', '.', 'Baldoni', '/', 'Gino', 'De', 'Stefani', '/', 'Guiseppe', 'Carella/vers', '.', 'Cláudio', 'Rabello']\n",
            "Tokenized sentence: ['Joseph', 'Costello', '/', 'Scott', 'Shepherd', '/', 'Simon', 'Allison', '-', 'Booth', '/', 'Simon', 'Hargreaves']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Smith', ',', 'Leah']\n",
            "Tokenized sentence: ['V', '.', 'Acosta', ',', 'M', '.', 'Harrouy', 'i', ',', 'R', '.', 'Cruz', ',', 'A', '.', 'Fernánde', 'z']\n",
            "Tokenized sentence: ['Fran', 'z', 'Schubert', '/', 'Richard', 'Strauss']\n",
            "Tokenized sentence: ['Maxwell', 'Anderson', '/', 'Kurt', 'Weill']\n",
            "Tokenized sentence: ['Massimo', 'Gabut', 'ti', '/', 'Bebe', 'Rexha', '/', 'Phil', 'Plested', '/', 'Maurizio', 'Lobina', '/', 'David', 'Guet', 'ta', '/', 'Kamille', '/', 'Gianfranco', 'Randone']\n",
            "Tokenized sentence: ['Westside', 'Gunn', ',', 'Estee', 'Nack', ',', 'Crucial', 'The', 'Guillotine']\n",
            "Tokenized sentence: ['Hengky', 'Muhamad', 'Fajar', 'Sampurna']\n",
            "Tokenized sentence: ['Kyveon', 'Lamar', 'Harris']\n",
            "Tokenized sentence: ['Mack', 'Jamieson', '/', 'Myles', 'Shane', 'Pringle', 'Brown', '/', 'Joe', 'Chiari', '/', 'Darren', 'Boachie', '/', 'David', 'Jordan', 'Larkins', 'II', '/', 'Whitney', 'McClain', '/', 'Obi', 'Ebele', '/', 'Uche', 'Ebele']\n",
            "Tokenized sentence: ['Sona', 'Jho']\n",
            "Tokenized sentence: ['Carlos', 'Chávez']\n",
            "Tokenized sentence: ['Carl', 'Stalling', '/', 'Walt', 'Disney']\n",
            "Tokenized sentence: ['ALBERTO', 'AGUI', 'LERA', 'VALADEZ']\n",
            "Tokenized sentence: ['GR', 'EGOR', 'IO', 'CARMONA', 'CARMONA', '/', 'SARAY', 'BARRUL', 'AMA', 'DOR/obo', 'SGAE']\n",
            "Tokenized sentence: ['Gregory', 'I', 'saacs']\n",
            "Tokenized sentence: ['Andrew', 'Lockington']\n",
            "Tokenized sentence: ['Hirst/Moginie', '/', 'K', 'eith', 'Walker']\n",
            "Tokenized sentence: ['Victoria', 'Orenze']\n",
            "Tokenized sentence: ['Itsuka']\n",
            "Tokenized sentence: ['Buckwheat', 'Zydeco']\n",
            "Tokenized sentence: ['Raymond', 'AR', '/', 'Ipunk']\n",
            "Tokenized sentence: ['Zurab', 'Rezoevich', 'Shavdiya/Artem', 'Anatol', \"'evich\", 'Ursul/Evgenii', 'Evgen', \"'evich\", 'Dugin', '/', 'Daniil', 'Aleksandrovich', 'Bumagin']\n",
            "Tokenized sentence: ['CA', 'CRANNY', 'SIMON', '/', 'CA', 'SARGENT', 'CHRISTOPHER', 'RICHARD', '/', 'PA', 'SY', '&', 'UNKNOWN']\n",
            "Tokenized sentence: ['C', 'GJEDREM', 'TORE', '/', 'CA', 'HAAVIK', 'PETTER', '/', 'CA', 'OMAR', 'NASRA', 'ALI', '/', 'PA', 'OST', '&', 'KJEX']\n",
            "Tokenized sentence: ['Miles', 'Davis']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/George', ',', 'Wegens']\n",
            "Tokenized sentence: ['Jon', 'Björk']\n",
            "Tokenized sentence: ['Илья', 'Банин']\n",
            "Tokenized sentence: ['Виктор', 'Цой']\n",
            "Tokenized sentence: ['Lionel', 'Wendling']\n",
            "Tokenized sentence: ['E', '.', 'Lindert', '/', 'Antonio', 'Vivald', 'i']\n",
            "Tokenized sentence: ['Jenn', 'Carter', ',', 'Kyle', 'Richh']\n",
            "Tokenized sentence: ['Anónimo']\n",
            "Tokenized sentence: ['Eric', 'Raymond', 'Oros', '&', 'Doug', 'Cruz']\n",
            "Tokenized sentence: ['Weezy']\n",
            "Tokenized sentence: ['B.', 'Mann', '/', 'S', '.', 'Jones', '/', 'N', '.', 'Sugarman', '/', 'T', '.', 'Brenneck', '/', 'H', '.', 'Steinweiss', '/', 'Bosco', 'Mann', ',', 'BMI', '/', 'Boscosound', 'Music', '(', 'BMI', ')', '/Treatsound', 'Music', '(', 'BMI', ')', '/Extraordinaire', 'Music', '(', 'BMI', ')']\n",
            "Tokenized sentence: ['黃瑋', '中']\n",
            "Tokenized sentence: ['Rib', ':', 'y', '(', 'uhki', ')']\n",
            "Tokenized sentence: ['Vijay', 'Chauhan', '/', 'Arya', 'Sharma']\n",
            "Tokenized sentence: ['Dreevis', '/', 'Marlo', 'Aka', 'Lil', 'Cuz']\n",
            "Tokenized sentence: ['TURRUBIATE', 'REGALADO', 'DOMINGO']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Moore', ',', 'Devarius']\n",
            "Tokenized sentence: ['王佳瑜', '&', '湯哲']\n",
            "Tokenized sentence: ['Don', 'Scroggs', ',', 'Mark', 'Ringweslki', ',', 'John', 'Watkins', '&', 'Jeff', 'Pearles']\n",
            "Tokenized sentence: ['Jon', 'Lundin', '/', 'Scotty', 'Sire', '/', 'Bruce', 'Wiegner']\n",
            "Tokenized sentence: ['R.', 'Barlow', '/', 'A', '.', 'Chirino', '/', 'G', '.', 'Estefan', '/', 'Tony', 'Moran', '/', 'Randall', 'Barlow', '/', 'E', '.', 'Estefan', ',', 'Jr./Emilio', 'Estefan', ',', 'Jr', '.']\n",
            "Tokenized sentence: ['Piers', 'Kalinowsky', '&', 'Skypierr']\n",
            "Tokenized sentence: ['Birk', 'Storm', '/', 'Malthe', 'Madsen', '/', 'Malthe', 'Rostrup', '/', 'August', 'Møller', 'Fogh', '/', 'Andreas', 'Bendix', 'Wilson']\n",
            "Tokenized sentence: ['Rodrigo', 'y', 'Gabriela']\n",
            "Tokenized sentence: ['Eximo', 'Blue']\n",
            "Tokenized sentence: ['Thunderstorms', '/', 'Lifted', 'Outwards', 'Records']\n",
            "Tokenized sentence: ['Andris', 'Freidenfelds', '/', 'Normunds', 'Jakušonoks', '/', 'Ģirts', 'Lūsis']\n",
            "Tokenized sentence: ['Bae', 'Lin', '/', 'Ares', 'Wu']\n",
            "Tokenized sentence: ['Evan', 'Westfall', ',', 'Taylor', 'Meier']\n",
            "Tokenized sentence: ['Adrian', 'Galvin', '/', 'AdrianGalvinSongs', '/', 'Michael', 'Maramag', '(', 'Blackbirdblackbird', ')', '/St', 'Music', 'Llc', 'adminstered', 'by', 'Songtrust']\n",
            "Tokenized sentence: ['Quenten', 'Cortez', 'Taylor']\n",
            "Tokenized sentence: ['Ken', 'Alberti/Alicia', 'Stamkos', '/', 'Nicolas', 'Chapelan']\n",
            "Tokenized sentence: ['John', 'Debney']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Castillo', 'pinzon', ',', 'Iran']\n",
            "Tokenized sentence: ['Cavetown']\n",
            "Tokenized sentence: ['Izzyhavinn']\n",
            "Tokenized sentence: ['Rose-Marie', 'Stråhle']\n",
            "Tokenized sentence: ['Johnny', 'Ortiz']\n",
            "Tokenized sentence: ['Derrick', 'Dixon']\n",
            "Tokenized sentence: ['M', 'I', 'KE', '(', 'VNM', ')']\n",
            "Tokenized sentence: ['Kai', 'Wright']\n",
            "Tokenized sentence: ['Rick', 'Rubin', '/', 'Scott', 'Yancey', 'Avett', '/', 'Timothy', 'Seth', 'Avett', '/', 'Robert', 'William', 'Jr.', 'Crawford']\n",
            "Tokenized sentence: ['Peter', 'Kvint', '/', 'Andreas', 'Johnson']\n",
            "Tokenized sentence: ['Sessa', 'Paolo', '/', 'Music', 'Market']\n",
            "Tokenized sentence: ['Michael', 'Stein', '/', 'Kyle', 'Dixon']\n",
            "Tokenized sentence: ['Ufuk', 'Beydemir']\n",
            "Tokenized sentence: ['Víctor', 'Merino', '/', 'Odeon', 'del', 'Perú', '/', 'Juan', 'Gonzalo', 'Rose']\n",
            "Tokenized sentence: ['Kiran', 'Ahluwalia', '/', 'Rez', 'Abbasi']\n",
            "Tokenized sentence: ['Marco', 'di', 'Mauro']\n",
            "Tokenized sentence: ['James', 'Covell']\n",
            "Tokenized sentence: ['Travis', 'Griffiths']\n",
            "Tokenized sentence: ['Vadim', 'Peare', '/', 'Ó', 'liver', 'Gallego', 'Sarmiento']\n",
            "Tokenized sentence: ['李達濤']\n",
            "Tokenized sentence: ['Jeff', 'Lynne']\n",
            "Tokenized sentence: ['Jeremy', 'Ford']\n",
            "Tokenized sentence: ['Boss', 'Rampuri', '/', 'Shishir', 'Pandey', '/', 'Zee', 'Music', 'Company']\n",
            "Tokenized sentence: ['Micah', 'Wilshire']\n",
            "Tokenized sentence: ['Giulian', 'o', 'Boursier']\n",
            "Tokenized sentence: ['Arya', 'Sharma', '&', 'Mukesh', 'Mishra']\n",
            "Tokenized sentence: ['Bruce', 'Robison', '/', 'El', 'Saddle', 'Music', '/', 'These', 'Are', 'Pulse', 'Songs', '/', 'American', 'Songs', 'Beta', '/', 'The', 'San', 'Benito', 'Kid', 'Publishing', '(', 'Concord', ')']\n",
            "Tokenized sentence: ['Librado', 'Perez', 'Vega']\n",
            "Tokenized sentence: ['Rella', 'Gz', 'x', 'Wawa', '2', 'Sneakyy', 'x', 'Miyaa', 'V']\n",
            "Tokenized sentence: ['Augus', 'to', 'Constancio', 'Coello', '/', 'Carlos', 'Härtling']\n",
            "Tokenized sentence: ['Jan', 'Lutge', 'baucks', '/', 'C', 'pook', '-', 'E']\n",
            "Tokenized sentence: ['Dee', 'Watkins', ',', 'Yung', 'Hydro', 'Beat', 'z', ',', 'True', 'Colors']\n",
            "Tokenized sentence: ['玲夏']\n",
            "Tokenized sentence: ['James', 'Blood', 'Ulmer']\n",
            "Tokenized sentence: ['Mr.', 'Franks', '/', 'Tayla', 'Parx', '/', 'Oliver', 'Frid', '/', 'Tommy', 'Brown', '/', 'Ariana', 'Grande', '/', 'Steven', 'Franks', '/', 'Oliver', \"'\", 'Junior', \"'\", 'Frid']\n",
            "Tokenized sentence: ['Tom', 'Odell']\n",
            "Tokenized sentence: ['Tim', 'Hecker']\n",
            "Tokenized sentence: ['Chief', 'Keef']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Armour', ',', 'John']\n",
            "Tokenized sentence: ['Drego', ',', 'Beno']\n",
            "Tokenized sentence: ['UnlimitedMusic']\n",
            "Tokenized sentence: ['Kenji', 'Kawai', '/', 'Ran', 'ma', 'Teki', 'Kagekidan', 'Bungeibu']\n",
            "Tokenized sentence: ['Latrina', 'Martin']\n",
            "Tokenized sentence: ['Ben', 'Watkins', '/', 'Steve', 'Stevens']\n",
            "Tokenized sentence: ['Ли', 'тв', 'ин', 'ен', 'ко', 'Па', 'ве', 'л', 'Ни', 'ко', 'ла', 'ев', 'ич']\n",
            "Tokenized sentence: ['Matthew', 'John', 'Torres']\n",
            "Tokenized sentence: ['Shampoo']\n",
            "Tokenized sentence: ['Olivier', 'Dax', '/', 'Simon', 'Delacroix']\n",
            "Tokenized sentence: ['Jean', '-', 'Michel', 'Defaye']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Ervay', ',', 'Jamie', 'J', '.']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Henson', ',', 'Samuel']\n",
            "Tokenized sentence: ['David', 'Oberger', '/', 'Peter', 'Buchhas', '/', 'Bernhard', 'Fries', '/', 'Florentin', 'Pinter', '/', 'Abdul', 'Rah', 'man', 'Mohamed', 'Bahaudeen']\n",
            "Tokenized sentence: ['Terry', 'Date', '/', 'Jordan', 'Fish', '/', 'Oliver', 'Sykes', '/', 'Lee', 'David', 'Malia']\n",
            "Tokenized sentence: ['Edward', 'Elgar']\n",
            "Tokenized sentence: ['James', 'Elbert', 'Spraggins', 'II']\n",
            "Tokenized sentence: ['Rodrigo', 'Septién', 'Rodríguez', ',', 'Álvaro', 'Pascual', 'Santamera']\n",
            "Tokenized sentence: ['John', '“', 'Virgo', '”', 'Garrett', 'III/Benito', 'Benites', '/', 'Antoinette', 'Colandero']\n",
            "Tokenized sentence: ['Preston', 'Crump', '/', 'Antwan', 'Patton', '/', 'Jeffery', 'Bowden', '/', 'Blake', 'German', '/', 'Patrick', 'Brown', '/', 'Tony', 'Macaulay']\n",
            "Tokenized sentence: ['Matthew', 'Ellard', '/', 'Between', 'The', 'Buried', 'And', 'Me']\n",
            "Tokenized sentence: ['张玮玮']\n",
            "Tokenized sentence: ['Robert', 'John', '“', 'Mutt', '”', 'Lange', '/', 'Phil', 'Collen', '/', 'Joe', 'Elliot', 't', '/', 'Steve', 'Clark']\n",
            "Tokenized sentence: ['R.', 'Johnson']\n",
            "Tokenized sentence: ['Aaron', 'Albert', 'Srdoc']\n",
            "Tokenized sentence: ['Commissioner', 'Gordon', '/', 'Chris', 'Kir', 'by', '/', 'Matt', 'Andersen']\n",
            "Tokenized sentence: ['Marty', 'Wilde', '/', 'Ricky', 'Wilde']\n",
            "Tokenized sentence: ['682', 'Bxby']\n",
            "Tokenized sentence: ['Adrián', 'Marcel', 'Torres', 'Ruí', 'z', 'Díaz', '&', 'Arnaldo', 'Grego', 'Torres', 'Ruí', 'z', 'Díaz']\n",
            "Tokenized sentence: ['Edgar', 'Froese/Tangerine', 'Dream', '/', 'Christopher', 'Franke/Johannes', 'Schmoelling']\n",
            "Tokenized sentence: ['Sconosciuto', 'Corale']\n",
            "Tokenized sentence: ['UK', 'N', 'W', 'N', '/', 'Humza', 'Zac', 'Akhtar', '/', 'Hugh', 'Rogers', '/', 'Amanak', 'i', 'Holan', 'i']\n",
            "Tokenized sentence: ['Jonathan', 'Michael', 'Fleming']\n",
            "Tokenized sentence: ['Giulian', 'o', 'Sacchet', 'to', '/', 'Giordano', 'Trivellato', '/', 'Tobacco', 'Music', 'Edition', '(', 'Gem', 'a', ')']\n",
            "Tokenized sentence: ['Delila', 'Paz/Mike', 'Frade', '/', 'Edgey', 'Pires', '/', 'Jo', 'ão', 'Brand', 'ão']\n",
            "Tokenized sentence: ['Roy', 'Cordell', 'Johnson']\n",
            "Tokenized sentence: ['Henry', 'Staroste', '/', 'Peter', 'Szigeti']\n",
            "Tokenized sentence: ['Dr', 'Mafia', 'Beats', '&', 'Saadallah', 'HALIM']\n",
            "Tokenized sentence: ['Alain', 'Neffe/Nadine', 'Bal']\n",
            "Tokenized sentence: ['Ueta', 'Jr', 'Muasika', ',', 'James', 'Muasika', '&', 'Richard', 'Baker']\n",
            "Tokenized sentence: ['R', '.', 'M', '.', 'Atria']\n",
            "Tokenized sentence: ['Luiza', '(', 'JPN', ')']\n",
            "Tokenized sentence: ['Get', 'ty', '/', 'Srav3', 'R', '/', 'D', 'J', 'Noriken']\n",
            "Tokenized sentence: ['Aguado', 'Gimenez', ',', 'Alfonso/Macias', 'Pintado', ',', 'Jose', 'Luis', '/', 'Ovia', '#', 'o', 'Linana', ',', 'Lino', '/', 'Pastor', 'Martinez', ',', 'Julio', '/', 'Universal', 'Music', '-', 'MGB', 'Songs']\n",
            "Tokenized sentence: ['Copyright', 'Control', '/', 'Teenage', 'Mutants', '&', 'Purple', 'Disco', 'Machine']\n",
            "Tokenized sentence: ['鳴風', '/', '佐久間', '貴生']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Busser', ',', 'Buddy', 'J']\n",
            "Tokenized sentence: ['Shawn', 'Thomas']\n",
            "Tokenized sentence: ['Screaming', 'Trees']\n",
            "Tokenized sentence: ['Gerald', 'Haddon', '/', 'Gerald', 'HaddonITammi', 'HaddonIVaShawn', 'Mitchell', '/', 'Precious', 'Baby', 'PublishingI', 'T', 'Bella', 'MusicIWalkway', 'Music']\n",
            "Tokenized sentence: ['Medina', 'Music', '/', 'Lars', 'Støvland']\n",
            "Tokenized sentence: ['Gabriel', 'Wilson']\n",
            "Tokenized sentence: ['Tedashii', ',', 'Carvello']\n",
            "Tokenized sentence: ['Mille', 'Petrozza/Rober', 'to', 'Fioretti', '/', 'Jürgen', 'Reil']\n",
            "Tokenized sentence: ['Shantanu', 'Avinashi']\n",
            "Tokenized sentence: ['Billie', 'Ray', 'Fingers', '/', 'Bruce', 'Fingers', '/', 'Jasmine', 'Denise', 'Calalang', 'Santos']\n",
            "Tokenized sentence: ['Carlo', 'Donida', '/', 'Jerry', 'Leiber', '/', 'Mike', 'Stoller', '/', 'Giulio', 'Rapetti']\n",
            "Tokenized sentence: ['MD', 'Saddam']\n",
            "Tokenized sentence: ['Adam', 'Kapit', '/', 'Doug', 'Rockwell', '/', 'ちゃんみな', '/', 'Tova', 'Litvin']\n",
            "Tokenized sentence: ['Noah', 'James', 'Gose', '&', 'Chloe', 'Olivia', 'Gose']\n",
            "Tokenized sentence: ['Gael', 'Linn', '/', 'David', 'Hines', '/', 'Traditional', '/', 'Rod', 'Stradling', '/', 'Danny', 'Stradling', '/', 'Jonathan', 'Philip', 'Moore', '/', 'Barnaby', 'Jack', 'Stradling', '/', 'Alton', 'Livingston', 'Ricketts', '/', 'Thomas', 'Charles', 'Greenhalgh', '/', 'Cooking', 'Vinyl', 'Publishing']\n",
            "Tokenized sentence: ['Ian', 'Hardie']\n",
            "Tokenized sentence: ['DJ', 'MHD', 'IND', '/', 'Nitin', 'Raikwar', '/', 'DJ', 'Harshit', 'Shah']\n",
            "Tokenized sentence: ['Dennis', 'Brown']\n",
            "Tokenized sentence: ['Warner', 'Chappell', '/', 'Inzima', 'Publishing', '2003', 'Ltd']\n",
            "Tokenized sentence: ['Bishop', 'Owen', 'Dwyer']\n",
            "Tokenized sentence: ['9lokk']\n",
            "Tokenized sentence: ['Loquillo', '/', 'Jaime', 'Stinus']\n",
            "Tokenized sentence: ['Klaus', 'Jankuhn', '/', 'WestBam', '/', 'DJ', 'Dick']\n",
            "Tokenized sentence: ['Lil', 'Peep']\n",
            "Tokenized sentence: ['Cookin', '’', 'Soul']\n",
            "Tokenized sentence: ['P.', 'Spector', '/', 'J', 'Leiber']\n",
            "Tokenized sentence: ['<', 'Unknown', '>', '/Mcleary', ',', 'Thomas']\n",
            "Tokenized sentence: ['XR', 'S', '/', 'DJ', 'Marky']\n",
            "Tokenized sentence: ['Teresa', 'De', 'Sio']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LangDetectException",
          "evalue": "No features in text.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLangDetectException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-386-f3fb4e9096bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbleu_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'BLEU score = {bleu_value*100:.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-385-1e81b73efa2a>\u001b[0m in \u001b[0;36mcalculate_bleu\u001b[0;34m(data, src_field, trg_field, model, device, max_len)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpred_trg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#cut off <eos> token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-384-8010e440fc16>\u001b[0m in \u001b[0;36mnormalize_sentence\u001b[0;34m(sentence, src_field, trg_field, model, device, max_len)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Detect the language of the input sentence (now it's a string)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Tokenize based on language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-384-8010e440fc16>\u001b[0m in \u001b[0;36mdetect_language\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Function to detect language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlangdetect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langdetect/detector_factory.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langdetect/detector.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mwhich\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhighest\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         '''\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langdetect/detector.py\u001b[0m in \u001b[0;36mget_probabilities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangprob\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langdetect/detector.py\u001b[0m in \u001b[0;36m_detect_block\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLangDetectException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCantDetectError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'No features in text.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanglist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLangDetectException\u001b[0m: No features in text."
          ]
        }
      ]
    }
  ]
}